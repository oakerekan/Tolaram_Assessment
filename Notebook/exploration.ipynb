{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e540342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For modeling (optional, if needed later)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Date and time handling\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7142ec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Excel file: data\\Project Assessment Data.xlsx\n",
      "‚úÖ Successfully loaded 14 sheets\n",
      "\n",
      "================================================================================\n",
      "üìä SHEET PREVIEW AND VARIABLE CREATION\n",
      "================================================================================\n",
      "\n",
      "üìÑ Sheet: 'plant description'\n",
      "   Shape: (13 rows, 2 columns)\n",
      "   Columns: ['Plant Code', 'Name']\n",
      "   Preview (first 3 rows):\n",
      "Plant Code   Name\n",
      "      A110 Plant1\n",
      "      A111 Plant2\n",
      "      A112 Plant3\n",
      "   ‚úÖ Created variable: df_plant_description\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'afko'\n",
      "   Shape: (200 rows, 183 columns)\n",
      "   Columns: ['MANDT', 'AUFNR 2 Order Number', 'GLTRP', 'GSTRP', 'FTRMS'] ... ['QPGT.LTEXTV', 'QPGT.INAKTIV']\n",
      "   Preview (first 3 rows):\n",
      " MANDT  AUFNR 2 Order Number    GLTRP    GSTRP  ...  QPGT.SPRACHE  QPGT.KURZTEXT  QPGT.LTEXTV  QPGT.INAKTIV\n",
      "   600             340011104 20241005 20241005  ...           NaN            NaN          NaN           NaN\n",
      "   600             340011105 20241005 20241005  ...           NaN            NaN          NaN           NaN\n",
      "   600             340011106 20241005 20241005  ...           NaN            NaN          NaN           NaN\n",
      "   ‚úÖ Created variable: df_afko\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'afpo'\n",
      "   Shape: (1,000 rows, 100 columns)\n",
      "   Columns: ['MANDT', 'AUFNR', 'POSNR', 'PSOBS', 'QUNUM'] ... ['MILL_OC_RUMNG', 'MILL_OC_SORT']\n",
      "   Preview (first 3 rows):\n",
      " MANDT    AUFNR  POSNR PSOBS  ...  FSH_SALLOC_QTY  MILL_OC_AUFNR_U  MILL_OC_RUMNG  MILL_OC_SORT\n",
      "   600 10000550      1     E  ...               0              NaN              0             0\n",
      "   600 10001074      1     E  ...               0              NaN              0             0\n",
      "   600 10001071      1     E  ...               0              NaN              0             0\n",
      "   ‚úÖ Created variable: df_afpo\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'aufm'\n",
      "   Shape: (1,000 rows, 46 columns)\n",
      "   Columns: ['MANDT', 'MBLNR', 'MJAHR', 'ZEILE', 'BLDAT'] ... ['/CWM/ERFME', 'WTY_IND']\n",
      "   Preview (first 3 rows):\n",
      " MANDT      MBLNR  MJAHR  ZEILE  ...  /CWM/MEINS  /CWM/ERFMG  /CWM/ERFME  WTY_IND\n",
      "   600 5000713450   2024      1  ...         NaN           0         NaN      NaN\n",
      "   600 5000713436   2024      1  ...         NaN           0         NaN      NaN\n",
      "   600 5000713437   2024      1  ...         NaN           0         NaN      NaN\n",
      "   ‚úÖ Created variable: df_aufm\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'aufk'\n",
      "   Shape: (1,000 rows, 125 columns)\n",
      "   Columns: ['MANDT', 'AUFNR', 'AUART', 'AUTYP', 'REFNR'] ... ['Z_TRAILER_NO_EXIT', 'ZZODO_METER_EXIT']\n",
      "   Preview (first 3 rows):\n",
      " MANDT     AUFNR AUART  AUTYP  ...  Z_TRAILER_NO Z_TRUCK_NO_EXIT  Z_TRAILER_NO_EXIT ZZODO_METER_EXIT\n",
      "   600 220002709  ZIS1     10  ...           NaN             NaN                NaN                0\n",
      "   600 210000458  ZIF1     10  ...           NaN             NaN                NaN                0\n",
      "   600 210000459  ZIF1     10  ...           NaN             NaN                NaN                0\n",
      "   ‚úÖ Created variable: df_aufk\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'qmel'\n",
      "   Shape: (200 rows, 165 columns)\n",
      "   Columns: ['MANDT', 'QMNUM', 'QMART', 'QMTXT', 'ARTPR'] ... ['SHN_ORIGIN', 'UII']\n",
      "   Preview (first 3 rows):\n",
      " MANDT    QMNUM QMART QMTXT  ... SHN_FUNCT_LOC  SHN_EQUIPMENT SHN_ORIGIN  UII\n",
      "   600 10003076    Z1   NaN  ...           NaN            NaN          0  NaN\n",
      "   600 10003906    Z1   NaN  ...           NaN            NaN          0  NaN\n",
      "   600 10003909    Z1   NaN  ...           NaN            NaN          0  NaN\n",
      "   ‚úÖ Created variable: df_qmel\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'qmfe'\n",
      "   Shape: (1,000 rows, 90 columns)\n",
      "   Columns: ['MANDT', 'QMNUM', 'FENUM', 'CATEGORY', 'ERNAM'] ... ['PROD_MATNR', 'PROD_SERNR']\n",
      "   Preview (first 3 rows):\n",
      " MANDT    QMNUM  FENUM  CATEGORY  ... LGNUM  VLTYP  PROD_MATNR  PROD_SERNR\n",
      "   600 10004225      1       NaN  ...   NaN    NaN         NaN         NaN\n",
      "   600 10004226      1       NaN  ...   NaN    NaN         NaN         NaN\n",
      "   600 10004253      1       NaN  ...   NaN    NaN         NaN         NaN\n",
      "   ‚úÖ Created variable: df_qmfe\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'qmih'\n",
      "   Shape: (1,000 rows, 60 columns)\n",
      "   Columns: ['MANDT', 'QMNUM', 'IWERK', 'ILOAN', 'ILOAI'] ... ['PAMS_PROID', 'PAMS_KOKRS']\n",
      "   Preview (first 3 rows):\n",
      " MANDT    QMNUM IWERK  ILOAN  ... PAMS_KOSTL PAMS_AUFNR  PAMS_PROID  PAMS_KOKRS\n",
      "   600 10000024  A113   1692  ...        NaN        NaN           0         NaN\n",
      "   600 10000092  A113   1810  ...        NaN        NaN           0         NaN\n",
      "   600 10000043  A113   1639  ...        NaN        NaN           0         NaN\n",
      "   ‚úÖ Created variable: df_qmih\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'qmur'\n",
      "   Shape: (1,000 rows, 32 columns)\n",
      "   Columns: ['MANDT', 'QMNUM', 'FENUM', 'URNUM', 'ERNAM'] ... ['CHANGEDDATETIME', 'INVOLVPERC']\n",
      "   Preview (first 3 rows):\n",
      " MANDT    QMNUM  FENUM  URNUM  ... DUMMY_QMUR_INCL_EEW_PS  ROOTCAUSE CHANGEDDATETIME  INVOLVPERC\n",
      "   600 10000004      1      1  ...                    NaN        NaN    2.022080e+13           0\n",
      "   600 10000095      1      1  ...                    NaN        NaN    2.022080e+13           0\n",
      "   600 10000317      1      1  ...                    NaN        NaN    2.022080e+13           0\n",
      "   ‚úÖ Created variable: df_qmur\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'qpcd'\n",
      "   Shape: (1,000 rows, 16 columns)\n",
      "   Columns: ['MANDT', 'KATALOGART', 'CODEGRUPPE', 'CODE', 'VERSION'] ... ['FOLGEAKTI', 'CODE4ACTIONBOX_REL']\n",
      "   Preview (first 3 rows):\n",
      " MANDT KATALOGART CODEGRUPPE CODE  ...  VERWENDUNG  GELOESCHT  FOLGEAKTI CODE4ACTIONBOX_REL\n",
      "     0          1      COLOR    1  ...         NaN        NaN        NaN                NaN\n",
      "     0          1      COLOR   10  ...         NaN        NaN        NaN                NaN\n",
      "     0          1      COLOR   11  ...         NaN        NaN        NaN                NaN\n",
      "   ‚úÖ Created variable: df_qpcd\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'qpct'\n",
      "   Shape: (1,000 rows, 11 columns)\n",
      "   Columns: ['MANDT', 'KATALOGART', 'CODEGRUPPE', 'CODE', 'SPRACHE'] ... ['INAKTIV', 'GELOESCHT']\n",
      "   Preview (first 3 rows):\n",
      " MANDT KATALOGART CODEGRUPPE CODE  ... KURZTEXT  LTEXTV  INAKTIV GELOESCHT\n",
      "     0          1      COLOR    1  ...      Rot     NaN      NaN       NaN\n",
      "     0          1      COLOR    1  ...      Red     NaN      NaN       NaN\n",
      "     0          1      COLOR   10  ...    Braun     NaN      NaN       NaN\n",
      "   ‚úÖ Created variable: df_qpct\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'crhd_v1'\n",
      "   Shape: (525 rows, 7 columns)\n",
      "   Columns: ['MANDT', 'OBJTY', 'OBJID', 'SPRAS', 'ARBPL', 'WERKS', 'KTEXT']\n",
      "   Preview (first 3 rows):\n",
      " MANDT OBJTY    OBJID SPRAS  ARBPL WERKS      KTEXT\n",
      "   600     A 10000000     E NSPXM1  A113 Preventive\n",
      "   600     A 10000001     E NSPXM2  A113 Corrective\n",
      "   600     A 10000002     E NSPXM3  A113    Utility\n",
      "   ‚úÖ Created variable: df_crhd_v1\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'jest'\n",
      "   Shape: (1,000 rows, 6 columns)\n",
      "   Columns: ['MANDT', 'OBJNR', 'STAT', 'INACT', 'CHGNR', '_DATAAGING']\n",
      "   Preview (first 3 rows):\n",
      " MANDT          OBJNR  STAT  INACT  CHGNR  _DATAAGING\n",
      "   600 QM000010000006 I0072    NaN      1           0\n",
      "   600 QM000010001496 I0072    NaN      1           0\n",
      "   600 QM000010002074 I0072    NaN      1           0\n",
      "   ‚úÖ Created variable: df_jest\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Sheet: 'qpgt'\n",
      "   Shape: (651 rows, 7 columns)\n",
      "   Columns: ['MANDANT', 'KATALOGART', 'CODEGRUPPE', 'SPRACHE', 'KURZTEXT', 'LTEXTV', 'INAKTIV']\n",
      "   Preview (first 3 rows):\n",
      " MANDANT KATALOGART CODEGRUPPE SPRACHE     KURZTEXT  LTEXTV  INAKTIV\n",
      "       0          1      COLOR       D        Farbe     NaN      NaN\n",
      "       0          1      COLOR       E        Color     NaN      NaN\n",
      "       0          2   $$OSS001       D Zu senden: &     NaN      NaN\n",
      "   ‚úÖ Created variable: df_qpgt\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìã SUMMARY:\n",
      "   ‚Ä¢ Total sheets loaded: 14\n",
      "   ‚Ä¢ Total rows across all sheets: 10,589\n",
      "   ‚Ä¢ Created variables: ['df_plant_description', 'df_afko', 'df_afpo', 'df_aufm', 'df_aufk', 'df_qmel', 'df_qmfe', 'df_qmih', 'df_qmur', 'df_qpcd', 'df_qpct', 'df_crhd_v1', 'df_jest', 'df_qpgt']\n",
      "\n",
      "================================================================================\n",
      "üìà DETAILED SHEET SUMMARY\n",
      "================================================================================\n",
      "       Sheet Name  Rows  Columns  Memory Usage (KB)  Has Missing Values  Numeric Columns  Text Columns  Date Columns\n",
      "             aufm  1000       46             707.16                True               38             8             0\n",
      "             afpo  1000      100            1292.06                True               87            13             0\n",
      "             aufk  1000      125            1475.34                True              114            11             0\n",
      "             qmur  1000       32             518.64                True               24             8             0\n",
      "             qmih  1000       60             730.88                True               52             8             0\n",
      "             qmfe  1000       90            1043.10                True               83             7             0\n",
      "             qpct  1000       11             326.14                True                5             6             0\n",
      "             jest  1000        6             145.64                True                4             2             0\n",
      "             qpcd  1000       16             368.98                True                9             7             0\n",
      "             qpgt   651        7             155.98                True                3             4             0\n",
      "          crhd_v1   525        7             152.06               False                2             5             0\n",
      "             afko   200      183             471.13                True              158            25             0\n",
      "             qmel   200      165             371.75                True              152            13             0\n",
      "plant description    13        2               1.50               False                0             2             0\n",
      "\n",
      "üéØ Available DataFrame variables: ['df_plant_description', 'df_afko', 'df_afpo', 'df_aufm', 'df_aufk', 'df_qmel', 'df_qmfe', 'df_qmih', 'df_qmur', 'df_qpcd', 'df_qpct', 'df_crhd_v1', 'df_jest', 'df_qpgt']\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Excel Data Loader with Error Handling and Logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def load_excel_sheets(excel_path, preview_rows=2, create_globals=True):\n",
    "    \"\"\"\n",
    "    Load all sheets from an Excel file with enhanced error handling and logging.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    excel_path : str\n",
    "        Path to the Excel file\n",
    "    preview_rows : int\n",
    "        Number of rows to preview for each sheet\n",
    "    create_globals : bool\n",
    "        Whether to create global DataFrame variables\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with lowercase sheet names as keys and DataFrames as values\n",
    "    \"\"\"\n",
    "    \n",
    "    df_lowercase = {}\n",
    "    created_variables = []\n",
    "    \n",
    "    # Convert to Path object for better path handling\n",
    "    file_path = Path(excel_path)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not file_path.exists():\n",
    "        print(f\"‚ùå Error: File not found at: {excel_path}\")\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        return df_lowercase\n",
    "    \n",
    "    try:\n",
    "        print(f\"üìÇ Loading Excel file: {excel_path}\")\n",
    "        \n",
    "        # Read all sheets\n",
    "        df_dict = pd.read_excel(excel_path, sheet_name=None)\n",
    "        \n",
    "        # Convert sheet names to lowercase and clean them\n",
    "        df_lowercase = {}\n",
    "        for sheet_name, df in df_dict.items():\n",
    "            clean_sheet_name = sheet_name.lower().strip()\n",
    "            df_lowercase[clean_sheet_name] = df\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded {len(df_lowercase)} sheets\")\n",
    "        \n",
    "        # Preview and create variables\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìä SHEET PREVIEW AND VARIABLE CREATION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for sheet_key, df_value in df_lowercase.items():\n",
    "            print(f\"\\nüìÑ Sheet: '{sheet_key}'\")\n",
    "            print(f\"   Shape: ({df_value.shape[0]:,} rows, {df_value.shape[1]:,} columns)\")\n",
    "            \n",
    "            # Show column names\n",
    "            if len(df_value.columns) <= 10:\n",
    "                print(f\"   Columns: {list(df_value.columns)}\")\n",
    "            else:\n",
    "                print(f\"   Columns: {list(df_value.columns[:5])} ... {list(df_value.columns[-2:])}\")\n",
    "            \n",
    "            # Preview data\n",
    "            if not df_value.empty:\n",
    "                print(f\"   Preview (first {preview_rows} rows):\")\n",
    "                print(df_value.head(preview_rows).to_string(index=False, max_cols=8))\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è  Sheet is empty\")\n",
    "            \n",
    "            # Create global variable if requested\n",
    "            if create_globals:\n",
    "                variable_name = create_safe_variable_name(sheet_key)\n",
    "                globals()[variable_name] = df_value\n",
    "                created_variables.append(variable_name)\n",
    "                print(f\"   ‚úÖ Created variable: {variable_name}\")\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\nüìã SUMMARY:\")\n",
    "        print(f\"   ‚Ä¢ Total sheets loaded: {len(df_lowercase)}\")\n",
    "        print(f\"   ‚Ä¢ Total rows across all sheets: {sum(df.shape[0] for df in df_lowercase.values()):,}\")\n",
    "        print(f\"   ‚Ä¢ Created variables: {created_variables}\")\n",
    "        \n",
    "        return df_lowercase\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Excel file: {str(e)}\")\n",
    "        return df_lowercase\n",
    "\n",
    "def create_safe_variable_name(sheet_name):\n",
    "    \"\"\"\n",
    "    Create a safe Python variable name from a sheet name.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sheet_name : str\n",
    "        Original sheet name\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Safe variable name\n",
    "    \"\"\"\n",
    "    # Replace spaces and special characters with underscores\n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9_]', '_', sheet_name)\n",
    "    \n",
    "    # Remove consecutive underscores\n",
    "    safe_name = re.sub(r'_+', '_', safe_name)\n",
    "    \n",
    "    # Remove leading/trailing underscores\n",
    "    safe_name = safe_name.strip('_')\n",
    "    \n",
    "    # Ensure it starts with a letter or underscore (not a number)\n",
    "    if safe_name and safe_name[0].isdigit():\n",
    "        safe_name = 'sheet_' + safe_name\n",
    "    \n",
    "    # Add df_ prefix\n",
    "    return f'df_{safe_name}' if safe_name else 'df_unnamed_sheet'\n",
    "\n",
    "def get_sheet_info(df_dict):\n",
    "    \"\"\"\n",
    "    Get summary information about all loaded sheets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_dict : dict\n",
    "        Dictionary of DataFrames\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Summary information\n",
    "    \"\"\"\n",
    "    if not df_dict:\n",
    "        print(\"No sheets to summarize.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    summary_data = []\n",
    "    for sheet_name, df in df_dict.items():\n",
    "        info = {\n",
    "            'Sheet Name': sheet_name,\n",
    "            'Rows': df.shape[0],\n",
    "            'Columns': df.shape[1],\n",
    "            'Memory Usage (KB)': round(df.memory_usage(deep=True).sum() / 1024, 2),\n",
    "            'Has Missing Values': df.isnull().any().any(),\n",
    "            'Numeric Columns': df.select_dtypes(include=['number']).shape[1],\n",
    "            'Text Columns': df.select_dtypes(include=['object']).shape[1],\n",
    "            'Date Columns': df.select_dtypes(include=['datetime']).shape[1]\n",
    "        }\n",
    "        summary_data.append(info)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    return summary_df.sort_values('Rows', ascending=False)\n",
    "\n",
    "# Usage Example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the Excel path\n",
    "    excel_path = r'data\\Project Assessment Data.xlsx'\n",
    "    \n",
    "    # Load the sheets\n",
    "    df_lowercase = load_excel_sheets(excel_path, preview_rows=3, create_globals=True)\n",
    "    \n",
    "    # Get summary information\n",
    "    if df_lowercase:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"üìà DETAILED SHEET SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        summary = get_sheet_info(df_lowercase)\n",
    "        print(summary.to_string(index=False))\n",
    "        \n",
    "        # Show available DataFrames in globals\n",
    "        df_variables = [var for var in globals() if var.startswith('df_') and isinstance(globals()[var], pd.DataFrame)]\n",
    "        print(f\"\\nüéØ Available DataFrame variables: {df_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1feec4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting QMEL Data Cleaning...\n",
      "üßπ STARTING QMEL DATA CLEANING PIPELINE\n",
      "============================================================\n",
      "Initial dataset shape: (200, 165)\n",
      "\n",
      "üìä 1. INITIAL DATA ASSESSMENT\n",
      "Shape: (200, 165)\n",
      "Total missing values: 19,833\n",
      "Memory usage: 0.36 MB\n",
      "\n",
      "üîç 2. MISSING VALUES ANALYSIS\n",
      "                        Missing_Count  Missing_Percentage\n",
      "AENAM                             200               100.0\n",
      "AUFNR                             200               100.0\n",
      "WAERS                             200               100.0\n",
      "VERID                             200               100.0\n",
      "SA_AUFNR                          200               100.0\n",
      "RM_WERKS                          200               100.0\n",
      "RM_MATNR                          200               100.0\n",
      "QMCOD                             200               100.0\n",
      "AUSWIRK                           200               100.0\n",
      "VKORG                             200               100.0\n",
      "MATNR                             200               100.0\n",
      "REVLV                             200               100.0\n",
      "MATKL                             200               100.0\n",
      "PRDHA                             200               100.0\n",
      "KZKRI                             200               100.0\n",
      "KZDKZ                             200               100.0\n",
      "KUNUM                             200               100.0\n",
      "MAKNZ                             200               100.0\n",
      "LIFNUM                            200               100.0\n",
      "VBELN                             200               100.0\n",
      "BUNAME                            200               100.0\n",
      "SPART                             200               100.0\n",
      "BSTNK                             200               100.0\n",
      "LGORTCHARG                        200               100.0\n",
      "LICHN                             200               100.0\n",
      "TEILEV                            200               100.0\n",
      "VTWEG                             200               100.0\n",
      "ADRNR                             200               100.0\n",
      "MAWERK                            200               100.0\n",
      "QMKAT                             200               100.0\n",
      "QMGRP                             200               100.0\n",
      "HERSTELLER                        200               100.0\n",
      "EMATNR                            200               100.0\n",
      "CHARG                             200               100.0\n",
      "EBELN                             200               100.0\n",
      "FERTAUFNR                         200               100.0\n",
      "LGORTVORG                         200               100.0\n",
      "BKGRP                             200               100.0\n",
      "EKORG                             200               100.0\n",
      "/SAPSMOSS/STATUS                  200               100.0\n",
      "/SAPSMOSS/SYSID                   200               100.0\n",
      "ZZAUFNR6                          200               100.0\n",
      "ZZAUFNR7                          200               100.0\n",
      "ZZAUFNR8                          200               100.0\n",
      "MBLNR                             200               100.0\n",
      "LS_KDAUF                          200               100.0\n",
      "LS_VBELN                          200               100.0\n",
      "MGEIN                             200               100.0\n",
      "FEART                             200               100.0\n",
      "QWRNUM                            200               100.0\n",
      "COAUFNR                           200               100.0\n",
      "REFNUM                            200               100.0\n",
      "KDMAT                             200               100.0\n",
      "SERIALNR                          200               100.0\n",
      "IDNLF                             200               100.0\n",
      "PROFIL_TYP                        200               100.0\n",
      "TSEGFL                            200               100.0\n",
      "KZLOESCH                          200               100.0\n",
      "CVP_XBLCK                         200               100.0\n",
      "DEVICEID                          200               100.0\n",
      "VKBUR                             200               100.0\n",
      "VKGRP                             200               100.0\n",
      "AUTKZ                             200               100.0\n",
      "FUNKTION                          200               100.0\n",
      "ZZAUFNR1                          200               100.0\n",
      "TSEGTP                            200               100.0\n",
      "TZONID                            200               100.0\n",
      "ZZAUFNR2                          200               100.0\n",
      "ZZAUFNR3                          200               100.0\n",
      "ZZAUFNR5                          200               100.0\n",
      "ZZAUFNR4                          200               100.0\n",
      "/ISDFPS/USERMODE                  200               100.0\n",
      "/ISDFPS/OBJNR                     200               100.0\n",
      "/SAPSMOSS/MANDT                   200               100.0\n",
      "OPPONENT                          200               100.0\n",
      "KALVAR                            200               100.0\n",
      "ZZAUFNR9                          200               100.0\n",
      "ZZAUFNR10                         200               100.0\n",
      "/SAPSMOSS/INSTN                   200               100.0\n",
      "/SAPSMOSS/OSSYS                   200               100.0\n",
      "/SAPSMOSS/DBSYS                   200               100.0\n",
      "/SAPSMOSS/REL                     200               100.0\n",
      "/SAPSMOSS/COMP                    200               100.0\n",
      "/SAPSMOSS/FRONT                   200               100.0\n",
      "/SAPSMOSS/SYSTYP                  200               100.0\n",
      "/SAPSMOSS/ADDID                   200               100.0\n",
      "/SAPSMOSS/ADDREL                  200               100.0\n",
      "SHN_OBJTY                         200               100.0\n",
      "SHN_FUNCT_LOC                     200               100.0\n",
      "LOGSYSTEM                         200               100.0\n",
      "/ISDFPS/MEQUI                     200               100.0\n",
      "SHN_EQUIPMENT                     200               100.0\n",
      "OBJNR_REAL                        200               100.0\n",
      "OBJNR_STAT                        200               100.0\n",
      "DUMMY_QMEL_INCL_EEW_PS            200               100.0\n",
      "UII                               200               100.0\n",
      "INDTX                             192                96.0\n",
      "PRIOK                             176                88.0\n",
      "FEKNZ                             151                75.5\n",
      "QMNAM                              32                16.0\n",
      "CROBJTY                            29                14.5\n",
      "ARBPLWERK                          29                14.5\n",
      "QMTXT                              24                12.0\n",
      "\n",
      "üìÖ 3. DATE COLUMN CONVERSION\n",
      "   üìÖ 'ERDAT': int64 ‚Üí datetime64[ns]\n",
      "      üìä Valid dates: 200, Missing: 0\n",
      "   üìÖ 'AEDAT': int64 ‚Üí datetime64[ns]\n",
      "      ‚ö†Ô∏è  200 values couldn't be converted\n",
      "      üìä Valid dates: 0, Missing: 200\n",
      "   ‚ö†Ô∏è  Column 'PSTER' (Period From) not found\n",
      "   ‚ö†Ô∏è  Column 'PETRI' (Period To) not found\n",
      "   üìÖ 'BEZDT': int64 ‚Üí datetime64[ns]\n",
      "      üìä Valid dates: 200, Missing: 0\n",
      "   üìÖ 'QMDAT': int64 ‚Üí datetime64[ns]\n",
      "      üìä Valid dates: 200, Missing: 0\n",
      "   üìÖ 'LTRMN': int64 ‚Üí datetime64[ns]\n",
      "      ‚ö†Ô∏è  200 values couldn't be converted\n",
      "      üìä Valid dates: 0, Missing: 200\n",
      "\n",
      "üîß 4. DATA TYPE OPTIMIZATION\n",
      "\n",
      "üìù 5. TEXT DATA CLEANING\n",
      "   üìù Cleaned 'BZMNG': 200 empty values converted to NaN\n",
      "\n",
      "üè∑Ô∏è  6. CATEGORICAL DATA ANALYSIS\n",
      "   üìä 'QMART': 1 unique values\n",
      "      Values: {'Z1': 200}\n",
      "   üìä 'PRIOK': 3 unique values\n",
      "      Values: {nan: 176, 1.0: 14, 2.0: 6, 3.0: 4}\n",
      "   üìä 'QMGRP': 0 unique values\n",
      "      Values: {nan: 200}\n",
      "   üìä 'QMCOD': 0 unique values\n",
      "      Values: {nan: 200}\n",
      "   üìä 'MAWERK': 0 unique values\n",
      "      Values: {nan: 200}\n",
      "\n",
      "üóëÔ∏è  7. DUPLICATE REMOVAL\n",
      "   ‚úÖ No duplicate rows found\n",
      "\n",
      "üìà 8. CLEANING SUMMARY\n",
      "========================================\n",
      "Rows: 200 ‚Üí 200 (+0)\n",
      "Missing values: 19,833 ‚Üí 20,433\n",
      "Date columns converted: 5\n",
      "Memory usage: 0.38 MB\n",
      "Duplicates removed: 0\n",
      "\n",
      "============================================================\n",
      "üìã QMEL DATA QUALITY REPORT\n",
      "============================================================\n",
      "üìä Total Notifications: 200\n",
      "üìÖ Date Range: 2023-03-09 to 2023-09-18\n",
      "   Span: 193 days\n",
      "üè∑Ô∏è  Top Notification Types:\n",
      "   Z1: 200\n",
      "‚úÖ Data Completeness: 38.08%\n",
      "\n",
      "üéØ Cleaned DataFrame 'df_qmel_clean' is ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "def clean_qmel_data(df_qmel, verbose=True):\n",
    "    \"\"\"\n",
    "    Comprehensive cleaning pipeline for QMEL (Quality Notifications) data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_qmel : pd.DataFrame\n",
    "        Raw QMEL DataFrame\n",
    "    verbose : bool\n",
    "        Whether to print detailed progress information\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Cleaned QMEL DataFrame\n",
    "    dict : Cleaning summary statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    if df_qmel is None or df_qmel.empty:\n",
    "        print(\"‚ùå DataFrame is None or empty. Cannot proceed with cleaning.\")\n",
    "        return df_qmel, {}\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_clean = df_qmel.copy()\n",
    "    initial_shape = df_clean.shape\n",
    "    cleaning_stats = {\n",
    "        'initial_rows': initial_shape[0],\n",
    "        'initial_columns': initial_shape[1],\n",
    "        'date_columns_converted': 0,\n",
    "        'missing_values_before': df_clean.isnull().sum().sum(),\n",
    "        'duplicates_removed': 0\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"üßπ STARTING QMEL DATA CLEANING PIPELINE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Initial dataset shape: {initial_shape}\")\n",
    "    \n",
    "    # 1. Initial Data Assessment\n",
    "    if verbose:\n",
    "        print(f\"\\nüìä 1. INITIAL DATA ASSESSMENT\")\n",
    "        print(f\"Shape: {df_clean.shape}\")\n",
    "        print(f\"Total missing values: {df_clean.isnull().sum().sum():,}\")\n",
    "        print(f\"Memory usage: {df_clean.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # 2. Handle Missing Values Analysis\n",
    "    if verbose:\n",
    "        print(f\"\\nüîç 2. MISSING VALUES ANALYSIS\")\n",
    "        missing_analysis = analyze_missing_values(df_clean)\n",
    "        if not missing_analysis.empty:\n",
    "            print(missing_analysis.to_string())\n",
    "        else:\n",
    "            print(\"‚úÖ No missing values found!\")\n",
    "    \n",
    "    # 3. Date Column Conversion\n",
    "    if verbose:\n",
    "        print(f\"\\nüìÖ 3. DATE COLUMN CONVERSION\")\n",
    "    \n",
    "    # SAP date columns in QMEL\n",
    "    date_columns = {\n",
    "        'ERDAT': 'Creation Date',\n",
    "        'AEDAT': 'Changed Date', \n",
    "        'PSTER': 'Period From',\n",
    "        'PETRI': 'Period To',\n",
    "        'BEZDT': 'Reference Date',\n",
    "        'QMDAT': 'Notification Date',\n",
    "        'LTRMN': 'Delivery Date'\n",
    "    }\n",
    "    \n",
    "    date_conversion_results = {}\n",
    "    \n",
    "    for col, description in date_columns.items():\n",
    "        if col in df_clean.columns:\n",
    "            result = convert_sap_date_column(df_clean, col, verbose=verbose)\n",
    "            date_conversion_results[col] = result\n",
    "            if result['converted']:\n",
    "                cleaning_stats['date_columns_converted'] += 1\n",
    "        elif verbose:\n",
    "            print(f\"   ‚ö†Ô∏è  Column '{col}' ({description}) not found\")\n",
    "    \n",
    "    # 4. Data Type Optimization\n",
    "    if verbose:\n",
    "        print(f\"\\nüîß 4. DATA TYPE OPTIMIZATION\")\n",
    "    \n",
    "    # Convert numeric columns that might be stored as strings\n",
    "    numeric_candidates = ['PRIOK', 'QMNUM', 'MATNR', 'MENGE', 'MEINS']\n",
    "    for col in numeric_candidates:\n",
    "        if col in df_clean.columns:\n",
    "            original_dtype = df_clean[col].dtype\n",
    "            if df_clean[col].dtype == 'object':\n",
    "                try:\n",
    "                    # Try to convert to numeric\n",
    "                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "                    if verbose:\n",
    "                        print(f\"   ‚úÖ Converted '{col}' from {original_dtype} to {df_clean[col].dtype}\")\n",
    "                except:\n",
    "                    if verbose:\n",
    "                        print(f\"   ‚ùå Failed to convert '{col}' to numeric\")\n",
    "    \n",
    "    # 5. Text Data Cleaning\n",
    "    if verbose:\n",
    "        print(f\"\\nüìù 5. TEXT DATA CLEANING\")\n",
    "    \n",
    "    text_columns = ['QMTXT', 'KURZTEXT', 'LTXTM', 'BZMNG']\n",
    "    for col in text_columns:\n",
    "        if col in df_clean.columns:\n",
    "            original_nulls = df_clean[col].isnull().sum()\n",
    "            # Strip whitespace and convert empty strings to NaN\n",
    "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "            df_clean[col] = df_clean[col].replace(['', 'nan', 'None', '0'], pd.NaT)\n",
    "            new_nulls = df_clean[col].isnull().sum()\n",
    "            if verbose and new_nulls != original_nulls:\n",
    "                print(f\"   üìù Cleaned '{col}': {new_nulls - original_nulls} empty values converted to NaN\")\n",
    "    \n",
    "    # 6. Categorical Data Analysis\n",
    "    if verbose:\n",
    "        print(f\"\\nüè∑Ô∏è  6. CATEGORICAL DATA ANALYSIS\")\n",
    "    \n",
    "    categorical_columns = ['QMART', 'PRIOK', 'QMGRP', 'QMCOD', 'MAWERK', 'LIFNR']\n",
    "    for col in categorical_columns:\n",
    "        if col in df_clean.columns:\n",
    "            unique_count = df_clean[col].nunique()\n",
    "            if verbose:\n",
    "                print(f\"   üìä '{col}': {unique_count} unique values\")\n",
    "                if unique_count <= 20:  # Show value counts for small categorical vars\n",
    "                    print(f\"      Values: {df_clean[col].value_counts(dropna=False).head().to_dict()}\")\n",
    "    \n",
    "    # 7. Remove Duplicates\n",
    "    if verbose:\n",
    "        print(f\"\\nüóëÔ∏è  7. DUPLICATE REMOVAL\")\n",
    "    \n",
    "    initial_rows = len(df_clean)\n",
    "    # Check for complete duplicates\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df_clean)\n",
    "    cleaning_stats['duplicates_removed'] = duplicates_removed\n",
    "    \n",
    "    if verbose:\n",
    "        if duplicates_removed > 0:\n",
    "            print(f\"   üóëÔ∏è  Removed {duplicates_removed} duplicate rows\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ No duplicate rows found\")\n",
    "    \n",
    "    # 8. Final Statistics\n",
    "    cleaning_stats.update({\n",
    "        'final_rows': len(df_clean),\n",
    "        'final_columns': len(df_clean.columns),\n",
    "        'missing_values_after': df_clean.isnull().sum().sum(),\n",
    "        'memory_mb': df_clean.memory_usage(deep=True).sum() / (1024**2)\n",
    "    })\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüìà 8. CLEANING SUMMARY\")\n",
    "        print(f\"=\" * 40)\n",
    "        print(f\"Rows: {cleaning_stats['initial_rows']:,} ‚Üí {cleaning_stats['final_rows']:,} \"\n",
    "              f\"({cleaning_stats['final_rows'] - cleaning_stats['initial_rows']:+,})\")\n",
    "        print(f\"Missing values: {cleaning_stats['missing_values_before']:,} ‚Üí {cleaning_stats['missing_values_after']:,}\")\n",
    "        print(f\"Date columns converted: {cleaning_stats['date_columns_converted']}\")\n",
    "        print(f\"Memory usage: {cleaning_stats['memory_mb']:.2f} MB\")\n",
    "        print(f\"Duplicates removed: {cleaning_stats['duplicates_removed']:,}\")\n",
    "    \n",
    "    return df_clean, cleaning_stats\n",
    "\n",
    "def convert_sap_date_column(df, column_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Convert SAP date column from YYYYMMDD format to datetime.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the column\n",
    "    column_name : str\n",
    "        Name of the column to convert\n",
    "    verbose : bool\n",
    "        Whether to print conversion details\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Conversion results and statistics\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        return {'converted': False, 'reason': 'Column not found'}\n",
    "    \n",
    "    original_dtype = df[column_name].dtype\n",
    "    original_nulls = df[column_name].isnull().sum()\n",
    "    \n",
    "    # Convert to string and handle SAP null representations\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    \n",
    "    # Replace SAP null representations\n",
    "    sap_nulls = ['0', '0.0', '00000000', 'nan', 'None', '']\n",
    "    df[column_name] = df[column_name].replace(sap_nulls, pd.NaT)\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df[column_name] = pd.to_datetime(df[column_name], format='%Y%m%d', errors='coerce')\n",
    "    \n",
    "    new_nulls = df[column_name].isnull().sum()\n",
    "    conversion_failures = new_nulls - original_nulls\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   üìÖ '{column_name}': {original_dtype} ‚Üí {df[column_name].dtype}\")\n",
    "        if conversion_failures > 0:\n",
    "            print(f\"      ‚ö†Ô∏è  {conversion_failures} values couldn't be converted\")\n",
    "        print(f\"      üìä Valid dates: {len(df) - new_nulls:,}, Missing: {new_nulls:,}\")\n",
    "    \n",
    "    return {\n",
    "        'converted': True,\n",
    "        'original_dtype': str(original_dtype),\n",
    "        'new_dtype': str(df[column_name].dtype),\n",
    "        'conversion_failures': conversion_failures,\n",
    "        'total_nulls': new_nulls\n",
    "    }\n",
    "\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"\n",
    "    Analyze missing values in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to analyze\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Missing value analysis\n",
    "    \"\"\"\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if missing_data.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    missing_percent = (missing_data / len(df) * 100).round(2)\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_data,\n",
    "        'Missing_Percentage': missing_percent\n",
    "    })\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "def get_qmel_quality_report(df_qmel_clean):\n",
    "    \"\"\"\n",
    "    Generate a data quality report for cleaned QMEL data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_qmel_clean : pd.DataFrame\n",
    "        Cleaned QMEL DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Quality report\n",
    "    \"\"\"\n",
    "    if df_qmel_clean.empty:\n",
    "        return {'error': 'DataFrame is empty'}\n",
    "    \n",
    "    report = {\n",
    "        'total_notifications': len(df_qmel_clean),\n",
    "        'date_range': {},\n",
    "        'notification_types': {},\n",
    "        'priority_distribution': {},\n",
    "        'data_completeness': {}\n",
    "    }\n",
    "    \n",
    "    # Date range analysis\n",
    "    if 'ERDAT' in df_qmel_clean.columns:\n",
    "        valid_dates = df_qmel_clean['ERDAT'].dropna()\n",
    "        if not valid_dates.empty:\n",
    "            report['date_range'] = {\n",
    "                'earliest': valid_dates.min(),\n",
    "                'latest': valid_dates.max(),\n",
    "                'span_days': (valid_dates.max() - valid_dates.min()).days\n",
    "            }\n",
    "    \n",
    "    # Notification types\n",
    "    if 'QMART' in df_qmel_clean.columns:\n",
    "        report['notification_types'] = df_qmel_clean['QMART'].value_counts().head(10).to_dict()\n",
    "    \n",
    "    # Priority distribution\n",
    "    if 'PRIOK' in df_qmel_clean.columns:\n",
    "        report['priority_distribution'] = df_qmel_clean['PRIOK'].value_counts().to_dict()\n",
    "    \n",
    "    # Data completeness\n",
    "    total_cells = df_qmel_clean.shape[0] * df_qmel_clean.shape[1]\n",
    "    missing_cells = df_qmel_clean.isnull().sum().sum()\n",
    "    report['data_completeness'] = {\n",
    "        'completeness_percentage': round((1 - missing_cells/total_cells) * 100, 2),\n",
    "        'missing_cells': missing_cells,\n",
    "        'total_cells': total_cells\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if df_qmel exists and clean it\n",
    "    if 'df_qmel' in globals() and not df_qmel.empty:\n",
    "        print(\"üöÄ Starting QMEL Data Cleaning...\")\n",
    "        \n",
    "        # Clean the data\n",
    "        df_qmel_clean, stats = clean_qmel_data(df_qmel, verbose=True)\n",
    "        \n",
    "        # Generate quality report\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìã QMEL DATA QUALITY REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        quality_report = get_qmel_quality_report(df_qmel_clean)\n",
    "        \n",
    "        print(f\"üìä Total Notifications: {quality_report.get('total_notifications', 'N/A'):,}\")\n",
    "        \n",
    "        if 'date_range' in quality_report and quality_report['date_range']:\n",
    "            dr = quality_report['date_range']\n",
    "            print(f\"üìÖ Date Range: {dr['earliest'].strftime('%Y-%m-%d')} to {dr['latest'].strftime('%Y-%m-%d')}\")\n",
    "            print(f\"   Span: {dr['span_days']:,} days\")\n",
    "        \n",
    "        if 'notification_types' in quality_report and quality_report['notification_types']:\n",
    "            print(f\"üè∑Ô∏è  Top Notification Types:\")\n",
    "            for ntype, count in list(quality_report['notification_types'].items())[:5]:\n",
    "                print(f\"   {ntype}: {count:,}\")\n",
    "        \n",
    "        if 'data_completeness' in quality_report:\n",
    "            dc = quality_report['data_completeness']\n",
    "            print(f\"‚úÖ Data Completeness: {dc['completeness_percentage']}%\")\n",
    "        \n",
    "        print(f\"\\nüéØ Cleaned DataFrame 'df_qmel_clean' is ready for analysis!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå df_qmel not found or is empty. Please load the data first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c956df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SAP QUALITY MANAGEMENT COMPREHENSIVE INTEGRATION\n",
      "================================================================================\n",
      "\n",
      "To use this script with your data:\n",
      "\n",
      "1. Load your SAP dataframes:\n",
      "   df_aufk = pd.read_csv('your_aufk_file.csv')\n",
      "   df_afko = pd.read_csv('your_afko_file.csv')\n",
      "   # ... load all other SAP tables\n",
      "\n",
      "2. Run the comprehensive integration:\n",
      "   comprehensive_df, summary_stats, quality_details = create_comprehensive_sap_view(\n",
      "       df_aufk=df_aufk, df_afko=df_afko, df_afpo=df_afpo, df_aufm=df_aufm,\n",
      "       df_qmel=df_qmel, df_qmfe=df_qmfe, df_qmur=df_qmur, df_qmih=df_qmih,\n",
      "       df_qpcd=df_qpcd, df_qpct=df_qpct, df_qpgt=df_qpgt,\n",
      "       df_crhd_v1=df_crhd_v1, df_jest=df_jest\n",
      "   )\n",
      "\n",
      "3. Analyze trends:\n",
      "   trends = analyze_quality_trends(comprehensive_df, summary_stats)\n",
      "\n",
      "4. Export results:\n",
      "   excel_file, csv_file, report_file = export_comprehensive_results(\n",
      "       comprehensive_df, summary_stats, quality_details\n",
      "   )\n",
      "\n",
      "5. Explore your data:\n",
      "   print(comprehensive_df.columns.tolist())\n",
      "   print(summary_stats)\n",
      "   comprehensive_df.head()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_plant_data():\n",
    "    \"\"\"\n",
    "    Create the plant description dataframe from your actual plant data\n",
    "    \"\"\"\n",
    "    plant_data = {\n",
    "        'Plant_Code': ['A110', 'A111', 'A112', 'A113', 'A114', 'A210', 'A211', \n",
    "                      'A310', 'A410', 'A510', 'A610', 'A710', 'A810'],\n",
    "        'Plant_Name': ['Plant1', 'Plant2', 'Plant3', 'Plant4', 'Plant5', 'Plant6', \n",
    "                      'Plant7', 'Plant8', 'Plant9', 'Plant10', 'Plant11', 'Plant12', 'Plant13']\n",
    "    }\n",
    "    \n",
    "    df_plant_description = pd.DataFrame(plant_data)\n",
    "    return df_plant_description\n",
    "\n",
    "def enhance_codes_with_descriptions(df_quality, code_group_col, code_col, \n",
    "                                  df_qpcd, df_qpct, df_qpgt, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Add code descriptions to quality tables\n",
    "    \n",
    "    Parameters:\n",
    "    - df_quality: Quality table (QMEL, QMFE, or QMUR)\n",
    "    - code_group_col: Column name for code group (e.g., 'QMGRP', 'FEGRP', 'URGRP')\n",
    "    - code_col: Column name for code (e.g., 'QMCOD', 'FECOD', 'URCOD')\n",
    "    - df_qpcd, df_qpct, df_qpgt: Code definition tables\n",
    "    - prefix: Prefix for new columns\n",
    "    \"\"\"\n",
    "    \n",
    "    enhanced_df = df_quality.copy()\n",
    "    \n",
    "    if df_qpcd is not None and not df_qpcd.empty and df_qpct is not None and not df_qpct.empty:\n",
    "        try:\n",
    "            # Join with code definitions\n",
    "            enhanced_df = enhanced_df.merge(\n",
    "                df_qpcd[['MANDT', 'KATALOGART', 'CODEGRUPPE', 'CODE']],\n",
    "                left_on=['MANDT', code_group_col, code_col],\n",
    "                right_on=['MANDT', 'CODEGRUPPE', 'CODE'],\n",
    "                how='left',\n",
    "                suffixes=('', f'_{prefix}DEF')\n",
    "            )\n",
    "            \n",
    "            # Join with code texts (taking English or first available language)\n",
    "            qpct_english = df_qpct[df_qpct['SPRACHE'] == 'EN'] if 'SPRACHE' in df_qpct.columns else df_qpct\n",
    "            if qpct_english.empty and 'SPRACHE' in df_qpct.columns:\n",
    "                qpct_english = df_qpct.groupby(['MANDT', 'KATALOGART', 'CODEGRUPPE', 'CODE']).first().reset_index()\n",
    "            \n",
    "            enhanced_df = enhanced_df.merge(\n",
    "                qpct_english[['MANDT', 'KATALOGART', 'CODEGRUPPE', 'CODE', 'KURZTEXT']],\n",
    "                on=['MANDT', 'KATALOGART', 'CODEGRUPPE', 'CODE'],\n",
    "                how='left',\n",
    "                suffixes=('', f'_{prefix}TEXT')\n",
    "            )\n",
    "            \n",
    "            # Add group descriptions from QPGT if available\n",
    "            if df_qpgt is not None and not df_qpgt.empty:\n",
    "                qpgt_english = df_qpgt[df_qpgt['SPRACHE'] == 'EN'] if 'SPRACHE' in df_qpgt.columns else df_qpgt\n",
    "                if qpgt_english.empty and 'SPRACHE' in df_qpgt.columns:\n",
    "                    qpgt_english = df_qpgt.groupby(['MANDANT', 'KATALOGART', 'CODEGRUPPE']).first().reset_index()\n",
    "                \n",
    "                enhanced_df = enhanced_df.merge(\n",
    "                    qpgt_english[['MANDANT', 'KATALOGART', 'CODEGRUPPE', 'KURZTEXT']].rename(columns={'MANDANT': 'MANDT'}),\n",
    "                    left_on=['MANDT', 'KATALOGART', 'CODEGRUPPE'],\n",
    "                    right_on=['MANDT', 'KATALOGART', 'CODEGRUPPE'],\n",
    "                    how='left',\n",
    "                    suffixes=('', f'_{prefix}GRP')\n",
    "                )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Code enhancement failed for {prefix}: {e}\")\n",
    "    \n",
    "    return enhanced_df\n",
    "\n",
    "def create_comprehensive_sap_view(df_aufk, df_afko, df_afpo, df_aufm, df_qmel, df_qmfe, \n",
    "                                df_qmur, df_qmih, df_qpcd, df_qpct, df_qpgt, df_crhd_v1, df_jest,\n",
    "                                df_plant_description=None):\n",
    "    \"\"\"\n",
    "    Creates a comprehensive SAP quality management view using your exact table structures\n",
    "    \n",
    "    Parameters:\n",
    "    - All your SAP dataframes as loaded\n",
    "    - df_plant_description: Optional, will create from your plant data if not provided\n",
    "    \n",
    "    Returns:\n",
    "    - comprehensive_df: Main integrated dataset\n",
    "    - summary_stats: Comprehensive summary statistics\n",
    "    - quality_details: Detailed quality analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ SAP COMPREHENSIVE QUALITY MANAGEMENT INTEGRATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Prepare plant data if not provided\n",
    "    if df_plant_description is None:\n",
    "        df_plant_description = prepare_plant_data()\n",
    "        print(\"‚úì Created plant description from your data\")\n",
    "    \n",
    "    # Validate required data\n",
    "    if df_aufk is None or df_aufk.empty:\n",
    "        raise ValueError(\"AUFK (Order Master) data is required!\")\n",
    "    \n",
    "    print(f\"üìä Input Data Summary:\")\n",
    "    print(f\"   ‚Ä¢ AUFK (Orders): {len(df_aufk):,} records\")\n",
    "    print(f\"   ‚Ä¢ AFKO (Headers): {len(df_afko):,} records\" if df_afko is not None and not df_afko.empty else \"   ‚Ä¢ AFKO: Not available\")\n",
    "    print(f\"   ‚Ä¢ AFPO (Items): {len(df_afpo):,} records\" if df_afpo is not None and not df_afpo.empty else \"   ‚Ä¢ AFPO: Not available\")\n",
    "    print(f\"   ‚Ä¢ QMEL (Quality): {len(df_qmel):,} records\" if df_qmel is not None and not df_qmel.empty else \"   ‚Ä¢ QMEL: Not available\")\n",
    "    print(f\"   ‚Ä¢ Plants: {len(df_plant_description):,} plants\")\n",
    "    \n",
    "    # STEP 1: Build Production Order Base\n",
    "    print(f\"\\nüè≠ STEP 1: Building Production Order Foundation...\")\n",
    "    base_df = df_aufk.copy()\n",
    "    \n",
    "    # Add plant information early - primary source AUFK.WERKS\n",
    "    if 'WERKS' in base_df.columns:\n",
    "        base_df = base_df.merge(\n",
    "            df_plant_description,\n",
    "            left_on='WERKS',\n",
    "            right_on='Plant_Code',\n",
    "            how='left',\n",
    "            suffixes=('', '_PLANT')\n",
    "        )\n",
    "        print(f\"   ‚úì Plant information added via AUFK.WERKS: {len(base_df):,} orders\")\n",
    "    \n",
    "    # Join with AFKO (Order Header)\n",
    "    if df_afko is not None and not df_afko.empty:\n",
    "        join_cols = ['MANDT', 'AUFNR'] if 'MANDT' in df_afko.columns else ['AUFNR']\n",
    "        base_df = base_df.merge(df_afko, on=join_cols, how='left', suffixes=('', '_AFKO'))\n",
    "        print(f\"   ‚úì Order headers joined: {len(base_df):,} orders\")\n",
    "    \n",
    "    # STEP 2: Add Order Items Summary (AFPO)\n",
    "    print(f\"\\nüì¶ STEP 2: Processing Order Items...\")\n",
    "    \n",
    "    if df_afpo is not None and not df_afpo.empty:\n",
    "        try:\n",
    "            # Create order items summary\n",
    "            group_cols = ['MANDT', 'AUFNR'] if 'MANDT' in df_afpo.columns else ['AUFNR']\n",
    "            \n",
    "            # Build aggregation dictionary based on available columns\n",
    "            agg_dict = {}\n",
    "            if 'MATNR' in df_afpo.columns:\n",
    "                agg_dict['MATNR'] = lambda x: ', '.join(x.unique()[:5])  # Top 5 materials\n",
    "            if 'POSNR' in df_afpo.columns:\n",
    "                agg_dict['POSNR'] = 'count'\n",
    "            if 'CHARG' in df_afpo.columns:\n",
    "                agg_dict['CHARG'] = lambda x: ', '.join(x.dropna().unique()[:3])  # Top 3 batches\n",
    "            if 'PWERK' in df_afpo.columns:\n",
    "                agg_dict['PWERK'] = lambda x: x.iloc[0] if len(x) > 0 else None  # Take first plant\n",
    "            if 'PSMNG' in df_afpo.columns:\n",
    "                agg_dict['PSMNG'] = 'sum'  # Total planned quantity\n",
    "            if 'WEMNG' in df_afpo.columns:\n",
    "                agg_dict['WEMNG'] = 'sum'  # Total received quantity\n",
    "            \n",
    "            if agg_dict:\n",
    "                items_summary = df_afpo.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "                \n",
    "                # Rename columns for clarity\n",
    "                rename_dict = {}\n",
    "                if 'MATNR' in agg_dict:\n",
    "                    rename_dict['MATNR'] = 'ORDER_MATERIALS'\n",
    "                if 'POSNR' in agg_dict:\n",
    "                    rename_dict['POSNR'] = 'ORDER_ITEM_COUNT'\n",
    "                if 'CHARG' in agg_dict:\n",
    "                    rename_dict['CHARG'] = 'ORDER_BATCHES'\n",
    "                if 'PWERK' in agg_dict:\n",
    "                    rename_dict['PWERK'] = 'ITEM_PLANT'\n",
    "                if 'PSMNG' in agg_dict:\n",
    "                    rename_dict['PSMNG'] = 'TOTAL_PLANNED_QTY'\n",
    "                if 'WEMNG' in agg_dict:\n",
    "                    rename_dict['WEMNG'] = 'TOTAL_RECEIVED_QTY'\n",
    "                \n",
    "                items_summary = items_summary.rename(columns=rename_dict)\n",
    "                \n",
    "                # Join back to base\n",
    "                base_df = base_df.merge(items_summary, on=group_cols, how='left')\n",
    "                print(f\"   ‚úì Order items summary added: {len(base_df):,} orders\")\n",
    "                \n",
    "                # Add secondary plant info if AUFK doesn't have plant\n",
    "                if 'Plant_Name' not in base_df.columns and 'ITEM_PLANT' in base_df.columns:\n",
    "                    base_df = base_df.merge(\n",
    "                        df_plant_description,\n",
    "                        left_on='ITEM_PLANT',\n",
    "                        right_on='Plant_Code',\n",
    "                        how='left',\n",
    "                        suffixes=('', '_ITEM')\n",
    "                    )\n",
    "                    print(f\"   ‚úì Secondary plant info added via AFPO.PWERK\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Order items processing failed: {e}\")\n",
    "    \n",
    "    # STEP 3: Add Goods Movements Summary (AUFM)\n",
    "    print(f\"\\nüìä STEP 3: Processing Goods Movements...\")\n",
    "    \n",
    "    if df_aufm is not None and not df_aufm.empty and 'AUFNR' in df_aufm.columns:\n",
    "        try:\n",
    "            group_cols = ['MANDT', 'AUFNR'] if 'MANDT' in df_aufm.columns else ['AUFNR']\n",
    "            \n",
    "            # Summarize goods movements\n",
    "            aufm_agg = {\n",
    "                'MBLNR': 'count',  # Number of documents\n",
    "                'BWART': lambda x: ', '.join(x.unique()[:5]),  # Movement types\n",
    "                'MENGE': 'sum',  # Total quantity\n",
    "                'DMBTR': 'sum'  # Total amount\n",
    "            }\n",
    "            \n",
    "            # Only include columns that exist\n",
    "            aufm_agg = {k: v for k, v in aufm_agg.items() if k in df_aufm.columns}\n",
    "            \n",
    "            if aufm_agg:\n",
    "                movements_summary = df_aufm.groupby(group_cols).agg(aufm_agg).reset_index()\n",
    "                \n",
    "                rename_dict = {\n",
    "                    'MBLNR': 'GOODS_MOVEMENT_COUNT',\n",
    "                    'BWART': 'MOVEMENT_TYPES',\n",
    "                    'MENGE': 'TOTAL_MOVEMENT_QTY',\n",
    "                    'DMBTR': 'TOTAL_MOVEMENT_VALUE'\n",
    "                }\n",
    "                \n",
    "                movements_summary = movements_summary.rename(columns={k: v for k, v in rename_dict.items() if k in movements_summary.columns})\n",
    "                \n",
    "                base_df = base_df.merge(movements_summary, on=group_cols, how='left')\n",
    "                print(f\"   ‚úì Goods movements summary added: {len(base_df):,} orders\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Goods movements processing failed: {e}\")\n",
    "    \n",
    "    # STEP 4: Add Quality Notifications (QMEL)\n",
    "    print(f\"\\nüîç STEP 4: Processing Quality Notifications...\")\n",
    "    \n",
    "    if df_qmel is not None and not df_qmel.empty:\n",
    "        try:\n",
    "            # Enhance QMEL with code descriptions\n",
    "            qmel_enhanced = enhance_codes_with_descriptions(\n",
    "                df_qmel, 'QMGRP', 'QMCOD', df_qpcd, df_qpct, df_qpgt, 'QM'\n",
    "            )\n",
    "            \n",
    "            # Find all possible linking columns to AUFNR\n",
    "            aufnr_cols = ['AUFNR'] + [col for col in qmel_enhanced.columns if 'ZZAUFNR' in col]\n",
    "            \n",
    "            quality_notifications = []\n",
    "            \n",
    "            for aufnr_col in aufnr_cols:\n",
    "                if aufnr_col in qmel_enhanced.columns:\n",
    "                    # Create subset with non-null values for this AUFNR column\n",
    "                    qmel_subset = qmel_enhanced[qmel_enhanced[aufnr_col].notna()].copy()\n",
    "                    if not qmel_subset.empty:\n",
    "                        qmel_subset = qmel_subset.rename(columns={aufnr_col: 'AUFNR_LINK'})\n",
    "                        quality_notifications.append(qmel_subset)\n",
    "            \n",
    "            if quality_notifications:\n",
    "                # Combine all quality notifications\n",
    "                all_quality = pd.concat(quality_notifications, ignore_index=True)\n",
    "                \n",
    "                # Group by order and create summary\n",
    "                group_cols = ['MANDT', 'AUFNR_LINK'] if 'MANDT' in all_quality.columns else ['AUFNR_LINK']\n",
    "                \n",
    "                quality_agg = {\n",
    "                    'QMNUM': 'count',\n",
    "                    'QMART': lambda x: ', '.join(x.unique()),\n",
    "                    'PRIOK': 'mean'  # Average priority\n",
    "                }\n",
    "                \n",
    "                # Add description columns if available\n",
    "                if 'KURZTEXT' in all_quality.columns:\n",
    "                    quality_agg['KURZTEXT'] = lambda x: ' | '.join(x.dropna().unique()[:3])\n",
    "                \n",
    "                quality_summary = all_quality.groupby(group_cols).agg(quality_agg).reset_index()\n",
    "                \n",
    "                rename_dict = {\n",
    "                    'AUFNR_LINK': 'AUFNR',\n",
    "                    'QMNUM': 'QUALITY_NOTIF_COUNT',\n",
    "                    'QMART': 'QUALITY_NOTIF_TYPES',\n",
    "                    'PRIOK': 'AVG_QUALITY_PRIORITY'\n",
    "                }\n",
    "                \n",
    "                if 'KURZTEXT' in quality_agg:\n",
    "                    rename_dict['KURZTEXT'] = 'QUALITY_ISSUES_DESC'\n",
    "                \n",
    "                quality_summary = quality_summary.rename(columns=rename_dict)\n",
    "                \n",
    "                # Join to base\n",
    "                join_cols = ['MANDT', 'AUFNR'] if 'MANDT' in quality_summary.columns else ['AUFNR']\n",
    "                base_df = base_df.merge(quality_summary, on=join_cols, how='left')\n",
    "                print(f\"   ‚úì Quality notifications added: {len(base_df):,} orders\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Quality notifications processing failed: {e}\")\n",
    "    \n",
    "    # STEP 5: Add Quality Defects (QMFE)\n",
    "    print(f\"\\n‚ö†Ô∏è  STEP 5: Processing Quality Defects...\")\n",
    "    \n",
    "    if df_qmfe is not None and not df_qmfe.empty and df_qmel is not None and not df_qmel.empty:\n",
    "        try:\n",
    "            # Enhance QMFE with code descriptions\n",
    "            qmfe_enhanced = enhance_codes_with_descriptions(\n",
    "                df_qmfe, 'FEGRP', 'FECOD', df_qpcd, df_qpct, df_qpgt, 'DEFECT'\n",
    "            )\n",
    "            \n",
    "            # Link defects to orders through QMEL\n",
    "            qmel_aufnr_link = df_qmel[['MANDT', 'QMNUM', 'AUFNR']].dropna()\n",
    "            \n",
    "            defects_with_orders = qmfe_enhanced.merge(\n",
    "                qmel_aufnr_link,\n",
    "                on=['MANDT', 'QMNUM'] if 'MANDT' in qmel_aufnr_link.columns else ['QMNUM'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            if not defects_with_orders.empty and 'AUFNR' in defects_with_orders.columns:\n",
    "                # Summarize defects per order\n",
    "                group_cols = ['MANDT', 'AUFNR'] if 'MANDT' in defects_with_orders.columns else ['AUFNR']\n",
    "                \n",
    "                defect_agg = {\n",
    "                    'FENUM': 'count',\n",
    "                    'FECOD': lambda x: ', '.join(x.unique()[:5])\n",
    "                }\n",
    "                \n",
    "                if 'ANZFEHLER' in defects_with_orders.columns:\n",
    "                    defect_agg['ANZFEHLER'] = 'sum'\n",
    "                \n",
    "                if 'KURZTEXT' in defects_with_orders.columns:\n",
    "                    defect_agg['KURZTEXT'] = lambda x: ' | '.join(x.dropna().unique()[:3])\n",
    "                \n",
    "                defects_summary = defects_with_orders.groupby(group_cols).agg(defect_agg).reset_index()\n",
    "                \n",
    "                rename_dict = {\n",
    "                    'FENUM': 'DEFECT_COUNT',\n",
    "                    'FECOD': 'DEFECT_CODES',\n",
    "                    'ANZFEHLER': 'TOTAL_DEFECT_QUANTITY'\n",
    "                }\n",
    "                \n",
    "                if 'KURZTEXT' in defect_agg:\n",
    "                    rename_dict['KURZTEXT'] = 'DEFECT_DESCRIPTIONS'\n",
    "                \n",
    "                defects_summary = defects_summary.rename(columns=rename_dict)\n",
    "                \n",
    "                base_df = base_df.merge(defects_summary, on=group_cols, how='left')\n",
    "                print(f\"   ‚úì Quality defects added: {len(base_df):,} orders\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Quality defects processing failed: {e}\")\n",
    "    \n",
    "    # STEP 6: Add Root Causes (QMUR)\n",
    "    print(f\"\\nüéØ STEP 6: Processing Root Causes...\")\n",
    "    \n",
    "    if df_qmur is not None and not df_qmur.empty and df_qmel is not None and not df_qmel.empty:\n",
    "        try:\n",
    "            # Enhance QMUR with code descriptions\n",
    "            qmur_enhanced = enhance_codes_with_descriptions(\n",
    "                df_qmur, 'URGRP', 'URCOD', df_qpcd, df_qpct, df_qpgt, 'CAUSE'\n",
    "            )\n",
    "            \n",
    "            # Link causes to orders through QMEL\n",
    "            qmel_aufnr_link = df_qmel[['MANDT', 'QMNUM', 'AUFNR']].dropna()\n",
    "            \n",
    "            causes_with_orders = qmur_enhanced.merge(\n",
    "                qmel_aufnr_link,\n",
    "                on=['MANDT', 'QMNUM'] if 'MANDT' in qmel_aufnr_link.columns else ['QMNUM'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            if not causes_with_orders.empty and 'AUFNR' in causes_with_orders.columns:\n",
    "                group_cols = ['MANDT', 'AUFNR'] if 'MANDT' in causes_with_orders.columns else ['AUFNR']\n",
    "                \n",
    "                cause_agg = {\n",
    "                    'URNUM': 'count',\n",
    "                    'URCOD': lambda x: ', '.join(x.unique()[:5])\n",
    "                }\n",
    "                \n",
    "                if 'ROOTCAUSE' in causes_with_orders.columns:\n",
    "                    cause_agg['ROOTCAUSE'] = 'sum'\n",
    "                \n",
    "                if 'KURZTEXT' in causes_with_orders.columns:\n",
    "                    cause_agg['KURZTEXT'] = lambda x: ' | '.join(x.dropna().unique()[:3])\n",
    "                \n",
    "                causes_summary = causes_with_orders.groupby(group_cols).agg(cause_agg).reset_index()\n",
    "                \n",
    "                rename_dict = {\n",
    "                    'URNUM': 'CAUSE_COUNT',\n",
    "                    'URCOD': 'CAUSE_CODES',\n",
    "                    'ROOTCAUSE': 'ROOT_CAUSE_COUNT'\n",
    "                }\n",
    "                \n",
    "                if 'KURZTEXT' in cause_agg:\n",
    "                    rename_dict['KURZTEXT'] = 'CAUSE_DESCRIPTIONS'\n",
    "                \n",
    "                causes_summary = causes_summary.rename(columns=rename_dict)\n",
    "                \n",
    "                base_df = base_df.merge(causes_summary, on=group_cols, how='left')\n",
    "                print(f\"   ‚úì Root causes added: {len(base_df):,} orders\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Root causes processing failed: {e}\")\n",
    "    \n",
    "    # STEP 7: Add Work Center Information\n",
    "    print(f\"\\n‚öôÔ∏è  STEP 7: Adding Work Center Information...\")\n",
    "    \n",
    "    if df_crhd_v1 is not None and not df_crhd_v1.empty:\n",
    "        try:\n",
    "            # Link work centers through AFKO.ARBPL_OBJID or direct ARBPL matches\n",
    "            if 'ARBPL_OBJID' in base_df.columns and 'OBJID' in df_crhd_v1.columns:\n",
    "                base_df = base_df.merge(\n",
    "                    df_crhd_v1[['OBJID', 'ARBPL', 'WERKS', 'KTEXT']],\n",
    "                    left_on='ARBPL_OBJID',\n",
    "                    right_on='OBJID',\n",
    "                    how='left',\n",
    "                    suffixes=('', '_WC')\n",
    "                )\n",
    "                print(f\"   ‚úì Work centers linked via OBJID: {len(base_df):,} orders\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Work center linking failed: {e}\")\n",
    "    \n",
    "    # STEP 8: Add Status Information\n",
    "    print(f\"\\nüìä STEP 8: Adding Status Information...\")\n",
    "    \n",
    "    if df_jest is not None and not df_jest.empty and 'OBJNR' in base_df.columns:\n",
    "        try:\n",
    "            # Get active statuses for orders\n",
    "            active_statuses = df_jest[df_jest['INACT'] == 0] if 'INACT' in df_jest.columns else df_jest\n",
    "            \n",
    "            status_summary = active_statuses.groupby('OBJNR').agg({\n",
    "                'STAT': lambda x: ', '.join(x.unique())\n",
    "            }).reset_index().rename(columns={'STAT': 'ORDER_STATUSES'})\n",
    "            \n",
    "            base_df = base_df.merge(status_summary, on='OBJNR', how='left')\n",
    "            print(f\"   ‚úì Status information added: {len(base_df):,} orders\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Status processing failed: {e}\")\n",
    "    \n",
    "    # STEP 9: Create Derived Fields and KPIs\n",
    "    print(f\"\\nüõ†Ô∏è  STEP 9: Creating KPIs and Derived Fields...\")\n",
    "    \n",
    "    # Fill NaN values for numeric columns\n",
    "    numeric_cols = [\n",
    "        'QUALITY_NOTIF_COUNT', 'DEFECT_COUNT', 'CAUSE_COUNT', 'ROOT_CAUSE_COUNT',\n",
    "        'GOODS_MOVEMENT_COUNT', 'ORDER_ITEM_COUNT', 'TOTAL_PLANNED_QTY', \n",
    "        'TOTAL_RECEIVED_QTY', 'TOTAL_MOVEMENT_QTY', 'AVG_QUALITY_PRIORITY'\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in base_df.columns:\n",
    "            base_df[col] = base_df[col].fillna(0)\n",
    "    \n",
    "    # Create quality indicators\n",
    "    base_df['HAS_QUALITY_ISSUES'] = (base_df.get('QUALITY_NOTIF_COUNT', 0) > 0)\n",
    "    base_df['HAS_DEFECTS'] = (base_df.get('DEFECT_COUNT', 0) > 0)\n",
    "    base_df['HAS_ROOT_CAUSES'] = (base_df.get('ROOT_CAUSE_COUNT', 0) > 0)\n",
    "    base_df['HAS_GOODS_MOVEMENTS'] = (base_df.get('GOODS_MOVEMENT_COUNT', 0) > 0)\n",
    "    \n",
    "    # Calculate quality score (0-100, higher is better)\n",
    "    max_notifications = max(base_df.get('QUALITY_NOTIF_COUNT', pd.Series([0])).max(), 1)\n",
    "    max_defects = max(base_df.get('DEFECT_COUNT', pd.Series([0])).max(), 1)\n",
    "    \n",
    "    base_df['QUALITY_SCORE'] = 100 - (\n",
    "        (base_df.get('QUALITY_NOTIF_COUNT', 0) / max_notifications * 40) +\n",
    "        (base_df.get('DEFECT_COUNT', 0) / max_defects * 35) +\n",
    "        (base_df['HAS_ROOT_CAUSES'].astype(int) * 25)\n",
    "    )\n",
    "    base_df['QUALITY_SCORE'] = base_df['QUALITY_SCORE'].clip(0, 100).round(1)\n",
    "    \n",
    "    # Quality category\n",
    "    base_df['QUALITY_CATEGORY'] = pd.cut(\n",
    "        base_df['QUALITY_SCORE'],\n",
    "        bins=[0, 60, 80, 100],\n",
    "        labels=['Poor', 'Good', 'Excellent'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Production efficiency indicators\n",
    "    if 'TOTAL_PLANNED_QTY' in base_df.columns and 'TOTAL_RECEIVED_QTY' in base_df.columns:\n",
    "        base_df['PRODUCTION_EFFICIENCY'] = (\n",
    "            base_df['TOTAL_RECEIVED_QTY'] / base_df['TOTAL_PLANNED_QTY'].replace(0, np.nan) * 100\n",
    "        ).round(1)\n",
    "        base_df['PRODUCTION_EFFICIENCY'] = base_df['PRODUCTION_EFFICIENCY'].clip(0, 150)  # Cap at 150%\n",
    "    \n",
    "    # Schedule performance (if we have dates)\n",
    "    if 'GSTRP' in base_df.columns and 'GSTRS' in base_df.columns:\n",
    "        try:\n",
    "            base_df['GSTRP'] = pd.to_datetime(base_df['GSTRP'], errors='coerce')\n",
    "            base_df['GSTRS'] = pd.to_datetime(base_df['GSTRS'], errors='coerce')\n",
    "            base_df['SCHEDULE_VARIANCE_DAYS'] = (base_df['GSTRS'] - base_df['GSTRP']).dt.days\n",
    "            base_df['ON_TIME_START'] = (abs(base_df['SCHEDULE_VARIANCE_DAYS']) <= 1).fillna(False)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"   ‚úì KPIs and derived fields created\")\n",
    "    print(f\"   ‚úì Final comprehensive dataset: {len(base_df):,} orders with {len(base_df.columns)} columns\")\n",
    "    \n",
    "    # STEP 10: Generate Comprehensive Summary Statistics\n",
    "    print(f\"\\nüìà STEP 10: Generating Summary Statistics...\")\n",
    "    \n",
    "    summary_stats = {\n",
    "        'dataset_info': {\n",
    "            'total_orders': len(base_df),\n",
    "            'total_columns': len(base_df.columns),\n",
    "            'creation_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        },\n",
    "        'plant_analysis': {},\n",
    "        'quality_analysis': {},\n",
    "        'production_analysis': {},\n",
    "        'order_type_analysis': {}\n",
    "    }\n",
    "    \n",
    "    # Plant analysis\n",
    "    plant_col = None\n",
    "    for col in ['Plant_Name', 'Plant_Code', 'WERKS']:\n",
    "        if col in base_df.columns and base_df[col].notna().sum() > 0:\n",
    "            plant_col = col\n",
    "            break\n",
    "    \n",
    "    if plant_col:\n",
    "        summary_stats['plant_analysis'] = {\n",
    "            'total_plants': base_df[plant_col].nunique(),\n",
    "            'orders_per_plant': base_df[plant_col].value_counts().head(10).to_dict(),\n",
    "            'quality_issues_by_plant': (\n",
    "                base_df.groupby(plant_col)['QUALITY_NOTIF_COUNT'].sum().sort_values(ascending=False).head(10).to_dict()\n",
    "                if 'QUALITY_NOTIF_COUNT' in base_df.columns else {}\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    # Quality analysis\n",
    "    if 'HAS_QUALITY_ISSUES' in base_df.columns:\n",
    "        summary_stats['quality_analysis'] = {\n",
    "            'total_orders_with_issues': base_df['HAS_QUALITY_ISSUES'].sum(),\n",
    "            'quality_issue_rate': (base_df['HAS_QUALITY_ISSUES'].sum() / len(base_df) * 100).round(2),\n",
    "            'total_notifications': base_df.get('QUALITY_NOTIF_COUNT', pd.Series([0])).sum(),\n",
    "            'total_defects': base_df.get('DEFECT_COUNT', pd.Series([0])).sum(),\n",
    "            'total_root_causes': base_df.get('ROOT_CAUSE_COUNT', pd.Series([0])).sum(),\n",
    "            'avg_quality_score': base_df['QUALITY_SCORE'].mean().round(1),\n",
    "            'quality_distribution': base_df['QUALITY_CATEGORY'].value_counts().to_dict()\n",
    "        }\n",
    "    \n",
    "    # Production analysis\n",
    "    summary_stats['production_analysis'] = {\n",
    "        'total_order_items': base_df.get('ORDER_ITEM_COUNT', pd.Series([0])).sum(),\n",
    "        'total_goods_movements': base_df.get('GOODS_MOVEMENT_COUNT', pd.Series([0])).sum(),\n",
    "    }\n",
    "    \n",
    "    if 'PRODUCTION_EFFICIENCY' in base_df.columns:\n",
    "        summary_stats['production_analysis']['avg_production_efficiency'] = base_df['PRODUCTION_EFFICIENCY'].mean().round(1)\n",
    "    \n",
    "    if 'ON_TIME_START' in base_df.columns:\n",
    "        summary_stats['production_analysis']['on_time_start_rate'] = (base_df['ON_TIME_START'].sum() / len(base_df) * 100).round(1)\n",
    "    \n",
    "    # Order type analysis\n",
    "    if 'AUART' in base_df.columns:\n",
    "        summary_stats['order_type_analysis'] = {\n",
    "            'order_types': base_df['AUART'].value_counts().to_dict(),\n",
    "            'quality_issues_by_order_type': (\n",
    "                base_df.groupby('AUART')['QUALITY_NOTIF_COUNT'].sum().sort_values(ascending=False).to_dict()\n",
    "                if 'QUALITY_NOTIF_COUNT' in base_df.columns else {}\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    # STEP 11: Create Quality Details for Deeper Analysis\n",
    "    quality_details = {}\n",
    "    \n",
    "    if df_qmel is not None and not df_qmel.empty:\n",
    "        quality_details['notification_summary'] = {\n",
    "            'total_notifications': len(df_qmel),\n",
    "            'notification_types': df_qmel['QMART'].value_counts().head(10).to_dict() if 'QMART' in df_qmel.columns else {},\n",
    "            'priority_distribution': df_qmel['PRIOK'].value_counts().to_dict() if 'PRIOK' in df_qmel.columns else {}\n",
    "        }\n",
    "    \n",
    "    if df_qmfe is not None and not df_qmfe.empty:\n",
    "        quality_details['defect_summary'] = {\n",
    "            'total_defects': len(df_qmfe),\n",
    "            'defect_codes': df_qmfe['FECOD'].value_counts().head(10).to_dict() if 'FECOD' in df_qmfe.columns else {},\n",
    "            'defect_categories': df_qmfe['FEKAT'].value_counts().to_dict() if 'FEKAT' in df_qmfe.columns else {}\n",
    "        }\n",
    "    \n",
    "    if df_qmur is not None and not df_qmur.empty:\n",
    "        quality_details['cause_summary'] = {\n",
    "            'total_causes': len(df_qmur),\n",
    "            'root_causes': df_qmur['ROOTCAUSE'].sum() if 'ROOTCAUSE' in df_qmur.columns else 0,\n",
    "            'cause_codes': df_qmur['URCOD'].value_counts().head(10).to_dict() if 'URCOD' in df_qmur.columns else {}\n",
    "        }\n",
    "    \n",
    "    print(\"\\n‚úÖ INTEGRATION COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä Final Results Summary:\")\n",
    "    print(f\"   ‚Ä¢ Total Orders: {summary_stats['dataset_info']['total_orders']:,}\")\n",
    "    print(f\"   ‚Ä¢ Total Columns: {summary_stats['dataset_info']['total_columns']}\")\n",
    "    \n",
    "    if summary_stats['plant_analysis']:\n",
    "        print(f\"   ‚Ä¢ Plants Analyzed: {summary_stats['plant_analysis']['total_plants']}\")\n",
    "    \n",
    "    if summary_stats['quality_analysis']:\n",
    "        print(f\"   ‚Ä¢ Orders with Quality Issues: {summary_stats['quality_analysis']['total_orders_with_issues']:,} ({summary_stats['quality_analysis']['quality_issue_rate']:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Average Quality Score: {summary_stats['quality_analysis']['avg_quality_score']:.1f}/100\")\n",
    "        print(f\"   ‚Ä¢ Total Quality Notifications: {summary_stats['quality_analysis']['total_notifications']:,}\")\n",
    "        print(f\"   ‚Ä¢ Total Defects: {summary_stats['quality_analysis']['total_defects']:,}\")\n",
    "    \n",
    "    return base_df, summary_stats, quality_details\n",
    "\n",
    "def analyze_quality_trends(comprehensive_df, summary_stats):\n",
    "    \"\"\"\n",
    "    Perform advanced quality trend analysis\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç ADVANCED QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'ERDAT' not in comprehensive_df.columns:\n",
    "        print(\"   ‚ö†Ô∏è  No creation date available for trend analysis\")\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        # Convert dates\n",
    "        comprehensive_df['ERDAT'] = pd.to_datetime(comprehensive_df['ERDAT'], errors='coerce')\n",
    "        comprehensive_df['YEAR_MONTH'] = comprehensive_df['ERDAT'].dt.to_period('M')\n",
    "        \n",
    "        trends = {}\n",
    "        \n",
    "        # Monthly quality trends\n",
    "        if 'QUALITY_NOTIF_COUNT' in comprehensive_df.columns:\n",
    "            monthly_quality = comprehensive_df.groupby('YEAR_MONTH').agg({\n",
    "                'AUFNR': 'count',\n",
    "                'QUALITY_NOTIF_COUNT': 'sum',\n",
    "                'QUALITY_SCORE': 'mean'\n",
    "            }).round(2)\n",
    "            \n",
    "            trends['monthly_trends'] = monthly_quality.to_dict()\n",
    "            print(f\"   ‚úì Monthly quality trends calculated for {len(monthly_quality)} months\")\n",
    "        \n",
    "        # Plant performance comparison\n",
    "        plant_col = None\n",
    "        for col in ['Plant_Name', 'Plant_Code', 'WERKS']:\n",
    "            if col in comprehensive_df.columns and comprehensive_df[col].notna().sum() > 0:\n",
    "                plant_col = col\n",
    "                break\n",
    "        \n",
    "        if plant_col and 'QUALITY_SCORE' in comprehensive_df.columns:\n",
    "            plant_performance = comprehensive_df.groupby(plant_col).agg({\n",
    "                'AUFNR': 'count',\n",
    "                'QUALITY_SCORE': 'mean',\n",
    "                'QUALITY_NOTIF_COUNT': 'sum'\n",
    "            }).round(2).sort_values('QUALITY_SCORE', ascending=False)\n",
    "            \n",
    "            trends['plant_performance'] = plant_performance.to_dict()\n",
    "            print(f\"   ‚úì Plant performance analysis completed for {len(plant_performance)} plants\")\n",
    "        \n",
    "        return trends\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Trend analysis failed: {e}\")\n",
    "        return {}\n",
    "\n",
    "def export_comprehensive_results(comprehensive_df, summary_stats, quality_details, filename_prefix=\"sap_comprehensive\"):\n",
    "    \"\"\"\n",
    "    Export comprehensive results to multiple formats\n",
    "    \"\"\"\n",
    "    print(f\"\\nüíæ EXPORTING RESULTS...\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    try:\n",
    "        # Excel export with multiple sheets\n",
    "        excel_file = f\"{filename_prefix}_{timestamp}.xlsx\"\n",
    "        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "            # Main comprehensive data\n",
    "            comprehensive_df.to_excel(writer, sheet_name='Comprehensive_Data', index=False)\n",
    "            \n",
    "            # Summary statistics\n",
    "            summary_df = pd.json_normalize(summary_stats, sep='_')\n",
    "            summary_df.to_excel(writer, sheet_name='Summary_Statistics', index=False)\n",
    "            \n",
    "            # Quality analysis\n",
    "            if summary_stats.get('quality_analysis'):\n",
    "                quality_df = pd.DataFrame([summary_stats['quality_analysis']])\n",
    "                quality_df.to_excel(writer, sheet_name='Quality_Analysis', index=False)\n",
    "            \n",
    "            # Plant analysis\n",
    "            if summary_stats.get('plant_analysis', {}).get('orders_per_plant'):\n",
    "                plant_df = pd.DataFrame(list(summary_stats['plant_analysis']['orders_per_plant'].items()), \n",
    "                                      columns=['Plant', 'Order_Count'])\n",
    "                plant_df.to_excel(writer, sheet_name='Plant_Analysis', index=False)\n",
    "            \n",
    "            # Top quality issues\n",
    "            if 'QUALITY_NOTIF_COUNT' in comprehensive_df.columns:\n",
    "                top_issues = comprehensive_df[comprehensive_df['QUALITY_NOTIF_COUNT'] > 0].nlargest(20, 'QUALITY_NOTIF_COUNT')\n",
    "                top_issues.to_excel(writer, sheet_name='Top_Quality_Issues', index=False)\n",
    "        \n",
    "        print(f\"   ‚úÖ Excel file created: {excel_file}\")\n",
    "        \n",
    "        # CSV export for easy analysis\n",
    "        csv_file = f\"{filename_prefix}_{timestamp}.csv\"\n",
    "        comprehensive_df.to_csv(csv_file, index=False)\n",
    "        print(f\"   ‚úÖ CSV file created: {csv_file}\")\n",
    "        \n",
    "        # Summary report\n",
    "        report_file = f\"{filename_prefix}_summary_{timestamp}.txt\"\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(\"SAP COMPREHENSIVE QUALITY MANAGEMENT ANALYSIS REPORT\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            f.write(\"DATASET OVERVIEW:\\n\")\n",
    "            f.write(f\"Total Orders: {summary_stats['dataset_info']['total_orders']:,}\\n\")\n",
    "            f.write(f\"Total Columns: {summary_stats['dataset_info']['total_columns']}\\n\\n\")\n",
    "            \n",
    "            if summary_stats.get('quality_analysis'):\n",
    "                qa = summary_stats['quality_analysis']\n",
    "                f.write(\"QUALITY ANALYSIS:\\n\")\n",
    "                f.write(f\"Orders with Quality Issues: {qa['total_orders_with_issues']:,} ({qa['quality_issue_rate']:.1f}%)\\n\")\n",
    "                f.write(f\"Average Quality Score: {qa['avg_quality_score']:.1f}/100\\n\")\n",
    "                f.write(f\"Total Quality Notifications: {qa['total_notifications']:,}\\n\")\n",
    "                f.write(f\"Total Defects: {qa['total_defects']:,}\\n\")\n",
    "                f.write(f\"Total Root Causes: {qa['total_root_causes']:,}\\n\\n\")\n",
    "            \n",
    "            if summary_stats.get('plant_analysis'):\n",
    "                pa = summary_stats['plant_analysis']\n",
    "                f.write(\"PLANT ANALYSIS:\\n\")\n",
    "                f.write(f\"Total Plants: {pa['total_plants']}\\n\")\n",
    "                f.write(\"Top Plants by Order Volume:\\n\")\n",
    "                for plant, count in list(pa['orders_per_plant'].items())[:5]:\n",
    "                    f.write(f\"  {plant}: {count:,} orders\\n\")\n",
    "        \n",
    "        print(f\"   ‚úÖ Summary report created: {report_file}\")\n",
    "        \n",
    "        return excel_file, csv_file, report_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Export failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Main execution function\n",
    "def main_integration():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate usage of the comprehensive integration\n",
    "    \"\"\"\n",
    "    print(\"üöÄ SAP QUALITY MANAGEMENT COMPREHENSIVE INTEGRATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nTo use this script with your data:\")\n",
    "    print(\"\\n1. Load your SAP dataframes:\")\n",
    "    print(\"   df_aufk = pd.read_csv('your_aufk_file.csv')\")\n",
    "    print(\"   df_afko = pd.read_csv('your_afko_file.csv')\")\n",
    "    print(\"   # ... load all other SAP tables\")\n",
    "    \n",
    "    print(\"\\n2. Run the comprehensive integration:\")\n",
    "    print(\"   comprehensive_df, summary_stats, quality_details = create_comprehensive_sap_view(\")\n",
    "    print(\"       df_aufk=df_aufk, df_afko=df_afko, df_afpo=df_afpo, df_aufm=df_aufm,\")\n",
    "    print(\"       df_qmel=df_qmel, df_qmfe=df_qmfe, df_qmur=df_qmur, df_qmih=df_qmih,\")\n",
    "    print(\"       df_qpcd=df_qpcd, df_qpct=df_qpct, df_qpgt=df_qpgt,\")\n",
    "    print(\"       df_crhd_v1=df_crhd_v1, df_jest=df_jest\")\n",
    "    print(\"   )\")\n",
    "    \n",
    "    print(\"\\n3. Analyze trends:\")\n",
    "    print(\"   trends = analyze_quality_trends(comprehensive_df, summary_stats)\")\n",
    "    \n",
    "    print(\"\\n4. Export results:\")\n",
    "    print(\"   excel_file, csv_file, report_file = export_comprehensive_results(\")\n",
    "    print(\"       comprehensive_df, summary_stats, quality_details\")\n",
    "    print(\"   )\")\n",
    "    \n",
    "    print(\"\\n5. Explore your data:\")\n",
    "    print(\"   print(comprehensive_df.columns.tolist())\")\n",
    "    print(\"   print(summary_stats)\")\n",
    "    print(\"   comprehensive_df.head()\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_integration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4fb9ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SAP COMPREHENSIVE QUALITY MANAGEMENT INTEGRATION\n",
      "================================================================================\n",
      "‚úì Created plant description from your data\n",
      "üìä Input Data Summary:\n",
      "   ‚Ä¢ AUFK (Orders): 3 records\n",
      "   ‚Ä¢ AFKO (Headers): 2 records\n",
      "   ‚Ä¢ AFPO (Items): 3 records\n",
      "   ‚Ä¢ QMEL (Quality): 2 records\n",
      "   ‚Ä¢ Plants: 13 plants\n",
      "\n",
      "üè≠ STEP 1: Building Production Order Foundation...\n",
      "   ‚úì Order headers joined: 3 orders\n",
      "\n",
      "üì¶ STEP 2: Processing Order Items...\n",
      "   ‚úì Order items summary added: 3 orders\n",
      "\n",
      "üìä STEP 3: Processing Goods Movements...\n",
      "   ‚úì Goods movements summary added: 3 orders\n",
      "\n",
      "üîç STEP 4: Processing Quality Notifications...\n",
      "   ‚ö†Ô∏è  Quality notifications processing failed: \"Column(s) ['PRIOK'] do not exist\"\n",
      "\n",
      "‚ö†Ô∏è  STEP 5: Processing Quality Defects...\n",
      "   ‚úì Quality defects added: 3 orders\n",
      "\n",
      "üéØ STEP 6: Processing Root Causes...\n",
      "   ‚úì Root causes added: 3 orders\n",
      "\n",
      "‚öôÔ∏è  STEP 7: Adding Work Center Information...\n",
      "   ‚úì Work centers linked via OBJID: 3 orders\n",
      "\n",
      "üìä STEP 8: Adding Status Information...\n",
      "\n",
      "üõ†Ô∏è  STEP 9: Creating KPIs and Derived Fields...\n",
      "   ‚úì KPIs and derived fields created\n",
      "   ‚úì Final comprehensive dataset: 3 orders with 26 columns\n",
      "\n",
      "üìà STEP 10: Generating Summary Statistics...\n",
      "\n",
      "‚úÖ INTEGRATION COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "üìä Final Results Summary:\n",
      "   ‚Ä¢ Total Orders: 3\n",
      "   ‚Ä¢ Total Columns: 26\n",
      "   ‚Ä¢ Plants Analyzed: 2\n",
      "   ‚Ä¢ Orders with Quality Issues: 0 (0.0%)\n",
      "   ‚Ä¢ Average Quality Score: 80.0/100\n",
      "   ‚Ä¢ Total Quality Notifications: 0\n",
      "   ‚Ä¢ Total Defects: 2.0\n"
     ]
    }
   ],
   "source": [
    "comprehensive_df, summary_stats, quality_details = create_comprehensive_sap_view(\n",
    "       df_aufk=df_aufk, df_afko=df_afko, df_afpo=df_afpo, df_aufm=df_aufm,\n",
    "       df_qmel=df_qmel, df_qmfe=df_qmfe, df_qmur=df_qmur, df_qmih=df_qmih,\n",
    "       df_qpcd=df_qpcd, df_qpct=df_qpct, df_qpgt=df_qpgt,\n",
    "       df_crhd_v1=df_crhd_v1, df_jest=df_jest\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5c68e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ADVANCED QUALITY ANALYSIS\n",
      "============================================================\n",
      "   ‚ö†Ô∏è  No creation date available for trend analysis\n"
     ]
    }
   ],
   "source": [
    "trends = analyze_quality_trends(comprehensive_df, summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb4d1bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö SAP DATA VALIDATION USAGE EXAMPLE\n",
      "============================================================\n",
      "\n",
      "1. BASIC VALIDATION:\n",
      "   # Validate all your tables\n",
      "   results = run_complete_validation_suite(\n",
      "       df_aufk=df_aufk, df_afko=df_afko, df_afpo=df_afpo,\n",
      "       df_qmel=df_qmel, df_qmfe=df_qmfe, # ... other tables\n",
      "   )\n",
      "\n",
      "2. ACCESS RESULTS:\n",
      "   # Get overall quality score\n",
      "   quality_score = results['validation_summary']['overall_quality_score']\n",
      "   \n",
      "   # Get specific table issues\n",
      "   aufk_issues = results['detailed_results']['df_aufk']['issues']\n",
      "   \n",
      "   # Get improvement suggestions\n",
      "   improvements = results['improvement_suggestions']\n",
      "\n",
      "3. IMPLEMENT MONITORING:\n",
      "   # Use monitoring plan for ongoing data quality\n",
      "   monitoring = results['monitoring_plan']\n",
      "   daily_checks = monitoring['daily_checks']\n",
      "\n",
      "4. SCHEDULED VALIDATION:\n",
      "   # Run this weekly/monthly for ongoing monitoring\n",
      "   # Set up automated alerts based on quality scores\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SAPDataValidator:\n",
    "    \"\"\"\n",
    "    Comprehensive data validation and quality checks for SAP data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_results = {}\n",
    "        self.data_quality_score = 0\n",
    "        self.recommendations = []\n",
    "    \n",
    "    def validate_all_tables(self, **tables):\n",
    "        \"\"\"\n",
    "        Validate all SAP tables and generate quality report\n",
    "        \n",
    "        Usage: validate_all_tables(df_aufk=df_aufk, df_afko=df_afko, ...)\n",
    "        \"\"\"\n",
    "        print(\"üîç SAP DATA VALIDATION AND QUALITY ASSESSMENT\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        validation_summary = {\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'tables_validated': 0,\n",
    "            'total_records': 0,\n",
    "            'issues_found': [],\n",
    "            'quality_scores': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Validate each table\n",
    "        for table_name, df in tables.items():\n",
    "            if df is not None and not df.empty:\n",
    "                print(f\"\\nüìã Validating {table_name.upper()}...\")\n",
    "                table_results = self._validate_table(df, table_name)\n",
    "                self.validation_results[table_name] = table_results\n",
    "                validation_summary['tables_validated'] += 1\n",
    "                validation_summary['total_records'] += len(df)\n",
    "                validation_summary['quality_scores'][table_name] = table_results['quality_score']\n",
    "                \n",
    "                if table_results['issues']:\n",
    "                    validation_summary['issues_found'].extend([\n",
    "                        f\"{table_name}: {issue}\" for issue in table_results['issues']\n",
    "                    ])\n",
    "        \n",
    "        # Generate overall assessment\n",
    "        overall_score = np.mean(list(validation_summary['quality_scores'].values())) if validation_summary['quality_scores'] else 0\n",
    "        validation_summary['overall_quality_score'] = round(overall_score, 1)\n",
    "        \n",
    "        # Generate recommendations\n",
    "        validation_summary['recommendations'] = self._generate_recommendations(validation_summary)\n",
    "        \n",
    "        print(f\"\\n‚úÖ VALIDATION COMPLETE\")\n",
    "        print(f\"   üìä Overall Data Quality Score: {validation_summary['overall_quality_score']}/100\")\n",
    "        print(f\"   üìÅ Tables Validated: {validation_summary['tables_validated']}\")\n",
    "        print(f\"   üìà Total Records: {validation_summary['total_records']:,}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Issues Found: {len(validation_summary['issues_found'])}\")\n",
    "        \n",
    "        return validation_summary\n",
    "    \n",
    "    def _validate_table(self, df, table_name):\n",
    "        \"\"\"\n",
    "        Validate individual table\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'table_name': table_name,\n",
    "            'record_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'quality_score': 100,  # Start with perfect score\n",
    "            'issues': [],\n",
    "            'checks_performed': [],\n",
    "            'data_completeness': {},\n",
    "            'data_consistency': {},\n",
    "            'data_accuracy': {}\n",
    "        }\n",
    "        \n",
    "        # Basic structure checks\n",
    "        results['checks_performed'].append('Structure validation')\n",
    "        if len(df) == 0:\n",
    "            results['issues'].append(\"Table is empty\")\n",
    "            results['quality_score'] -= 50\n",
    "        \n",
    "        if len(df.columns) == 0:\n",
    "            results['issues'].append(\"No columns found\")\n",
    "            results['quality_score'] -= 50\n",
    "            return results\n",
    "        \n",
    "        # Data completeness checks\n",
    "        results['checks_performed'].append('Completeness analysis')\n",
    "        completeness = self._check_completeness(df, table_name)\n",
    "        results['data_completeness'] = completeness\n",
    "        \n",
    "        # Penalize for low completeness\n",
    "        avg_completeness = np.mean(list(completeness['column_completeness'].values()))\n",
    "        if avg_completeness < 80:\n",
    "            results['issues'].append(f\"Low data completeness: {avg_completeness:.1f}%\")\n",
    "            results['quality_score'] -= (100 - avg_completeness) * 0.3\n",
    "        \n",
    "        # Data consistency checks\n",
    "        results['checks_performed'].append('Consistency validation')\n",
    "        consistency = self._check_consistency(df, table_name)\n",
    "        results['data_consistency'] = consistency\n",
    "        \n",
    "        if consistency['issues']:\n",
    "            results['issues'].extend(consistency['issues'])\n",
    "            results['quality_score'] -= len(consistency['issues']) * 5\n",
    "        \n",
    "        # Data accuracy checks\n",
    "        results['checks_performed'].append('Accuracy validation')\n",
    "        accuracy = self._check_accuracy(df, table_name)\n",
    "        results['data_accuracy'] = accuracy\n",
    "        \n",
    "        if accuracy['issues']:\n",
    "            results['issues'].extend(accuracy['issues'])\n",
    "            results['quality_score'] -= len(accuracy['issues']) * 3\n",
    "        \n",
    "        # Table-specific validations\n",
    "        table_specific_issues = self._validate_table_specific(df, table_name)\n",
    "        if table_specific_issues:\n",
    "            results['issues'].extend(table_specific_issues)\n",
    "            results['quality_score'] -= len(table_specific_issues) * 2\n",
    "        \n",
    "        # Ensure score doesn't go below 0\n",
    "        results['quality_score'] = max(0, round(results['quality_score'], 1))\n",
    "        \n",
    "        print(f\"   ‚úì {table_name}: Score {results['quality_score']}/100, {len(results['issues'])} issues\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _check_completeness(self, df, table_name):\n",
    "        \"\"\"\n",
    "        Check data completeness\n",
    "        \"\"\"\n",
    "        completeness = {\n",
    "            'total_cells': df.shape[0] * df.shape[1],\n",
    "            'missing_cells': df.isnull().sum().sum(),\n",
    "            'column_completeness': {},\n",
    "            'critical_columns_missing': []\n",
    "        }\n",
    "        \n",
    "        # Calculate completeness per column\n",
    "        for col in df.columns:\n",
    "            missing_pct = (df[col].isnull().sum() / len(df)) * 100\n",
    "            completeness['column_completeness'][col] = round(100 - missing_pct, 1)\n",
    "        \n",
    "        # Check critical columns based on table type\n",
    "        critical_columns = self._get_critical_columns(table_name)\n",
    "        for col in critical_columns:\n",
    "            if col in df.columns:\n",
    "                missing_pct = (df[col].isnull().sum() / len(df)) * 100\n",
    "                if missing_pct > 10:  # More than 10% missing\n",
    "                    completeness['critical_columns_missing'].append(f\"{col} ({missing_pct:.1f}% missing)\")\n",
    "        \n",
    "        completeness['overall_completeness'] = round(\n",
    "            ((completeness['total_cells'] - completeness['missing_cells']) / completeness['total_cells']) * 100, 1\n",
    "        )\n",
    "        \n",
    "        return completeness\n",
    "    \n",
    "    def _check_consistency(self, df, table_name):\n",
    "        \"\"\"\n",
    "        Check data consistency\n",
    "        \"\"\"\n",
    "        consistency = {\n",
    "            'issues': [],\n",
    "            'duplicate_records': 0,\n",
    "            'data_type_issues': [],\n",
    "            'value_range_issues': []\n",
    "        }\n",
    "        \n",
    "        # Check for duplicates based on key columns\n",
    "        key_columns = self._get_key_columns(table_name)\n",
    "        if key_columns:\n",
    "            available_keys = [col for col in key_columns if col in df.columns]\n",
    "            if available_keys:\n",
    "                duplicates = df.duplicated(subset=available_keys).sum()\n",
    "                consistency['duplicate_records'] = duplicates\n",
    "                if duplicates > 0:\n",
    "                    consistency['issues'].append(f\"{duplicates} duplicate records found\")\n",
    "        \n",
    "        # Check data types\n",
    "        for col in df.columns:\n",
    "            if col.upper().endswith('DAT') or 'DATE' in col.upper():\n",
    "                # Should be date-like\n",
    "                try:\n",
    "                    pd.to_datetime(df[col], errors='coerce')\n",
    "                except:\n",
    "                    consistency['data_type_issues'].append(f\"{col} has invalid date format\")\n",
    "            \n",
    "            elif col.upper().endswith('MNG') or col.upper().endswith('QTY'):\n",
    "                # Should be numeric\n",
    "                if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    non_numeric = df[col].apply(lambda x: not str(x).replace('.', '').replace('-', '').isdigit() if pd.notna(x) else False).sum()\n",
    "                    if non_numeric > 0:\n",
    "                        consistency['data_type_issues'].append(f\"{col} has {non_numeric} non-numeric values\")\n",
    "        \n",
    "        # Check value ranges\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if df[col].min() < -999999 or df[col].max() > 999999999:\n",
    "                consistency['value_range_issues'].append(f\"{col} has extreme values\")\n",
    "        \n",
    "        if consistency['data_type_issues']:\n",
    "            consistency['issues'].extend(consistency['data_type_issues'])\n",
    "        \n",
    "        if consistency['value_range_issues']:\n",
    "            consistency['issues'].extend(consistency['value_range_issues'])\n",
    "        \n",
    "        return consistency\n",
    "    \n",
    "    def _check_accuracy(self, df, table_name):\n",
    "        \"\"\"\n",
    "        Check data accuracy\n",
    "        \"\"\"\n",
    "        accuracy = {\n",
    "            'issues': [],\n",
    "            'invalid_codes': [],\n",
    "            'logical_inconsistencies': [],\n",
    "            'reference_integrity_issues': []\n",
    "        }\n",
    "        \n",
    "        # Check for valid plant codes (if applicable)\n",
    "        plant_columns = ['WERKS', 'PWERK', 'Plant_Code']\n",
    "        for col in plant_columns:\n",
    "            if col in df.columns:\n",
    "                # Check if plant codes follow expected pattern\n",
    "                valid_pattern = df[col].str.match(r'^[A-Z]\\d{3}$', na=False).sum() if df[col].dtype == 'object' else 0\n",
    "                total_non_null = df[col].notna().sum()\n",
    "                if total_non_null > 0 and (valid_pattern / total_non_null) < 0.8:\n",
    "                    accuracy['invalid_codes'].append(f\"{col} has non-standard plant codes\")\n",
    "        \n",
    "        # Check order number formats\n",
    "        order_columns = ['AUFNR', 'ABNUM']\n",
    "        for col in order_columns:\n",
    "            if col in df.columns and df[col].dtype == 'object':\n",
    "                # Basic order number validation\n",
    "                invalid_orders = df[col].str.len().fillna(0)\n",
    "                if (invalid_orders < 6).sum() > 0 and df[col].notna().sum() > 0:\n",
    "                    accuracy['invalid_codes'].append(f\"{col} has suspiciously short order numbers\")\n",
    "        \n",
    "        # Check for logical inconsistencies\n",
    "        if table_name == 'df_afko':\n",
    "            # Start date should be before end date\n",
    "            if 'GSTRP' in df.columns and 'GLTRP' in df.columns:\n",
    "                try:\n",
    "                    start_dates = pd.to_datetime(df['GSTRP'], errors='coerce')\n",
    "                    end_dates = pd.to_datetime(df['GLTRP'], errors='coerce')\n",
    "                    invalid_dates = (start_dates > end_dates).sum()\n",
    "                    if invalid_dates > 0:\n",
    "                        accuracy['logical_inconsistencies'].append(\n",
    "                            f\"{invalid_dates} orders have start date after end date\"\n",
    "                        )\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if table_name == 'df_afpo':\n",
    "            # Planned quantity should be positive\n",
    "            if 'PSMNG' in df.columns:\n",
    "                negative_qty = (df['PSMNG'] < 0).sum()\n",
    "                if negative_qty > 0:\n",
    "                    accuracy['logical_inconsistencies'].append(\n",
    "                        f\"{negative_qty} items have negative planned quantities\"\n",
    "                    )\n",
    "        \n",
    "        if table_name == 'df_qmel':\n",
    "            # Quality notification dates should be reasonable\n",
    "            if 'QMDAT' in df.columns:\n",
    "                try:\n",
    "                    qm_dates = pd.to_datetime(df['QMDAT'], errors='coerce')\n",
    "                    future_dates = (qm_dates > datetime.now()).sum()\n",
    "                    old_dates = (qm_dates < datetime(2000, 1, 1)).sum()\n",
    "                    \n",
    "                    if future_dates > 0:\n",
    "                        accuracy['logical_inconsistencies'].append(\n",
    "                            f\"{future_dates} quality notifications have future dates\"\n",
    "                        )\n",
    "                    if old_dates > 0:\n",
    "                        accuracy['logical_inconsistencies'].append(\n",
    "                            f\"{old_dates} quality notifications have very old dates\"\n",
    "                        )\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Combine all issues\n",
    "        if accuracy['invalid_codes']:\n",
    "            accuracy['issues'].extend(accuracy['invalid_codes'])\n",
    "        if accuracy['logical_inconsistencies']:\n",
    "            accuracy['issues'].extend(accuracy['logical_inconsistencies'])\n",
    "        if accuracy['reference_integrity_issues']:\n",
    "            accuracy['issues'].extend(accuracy['reference_integrity_issues'])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def _validate_table_specific(self, df, table_name):\n",
    "        \"\"\"\n",
    "        Table-specific validation rules\n",
    "        \"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        if table_name == 'df_aufk':\n",
    "            # Order master specific checks\n",
    "            if 'AUFNR' in df.columns:\n",
    "                if df['AUFNR'].duplicated().any():\n",
    "                    issues.append(\"Duplicate order numbers in master data\")\n",
    "            \n",
    "            if 'ERDAT' in df.columns:\n",
    "                try:\n",
    "                    creation_dates = pd.to_datetime(df['ERDAT'], errors='coerce')\n",
    "                    recent_cutoff = datetime.now() - timedelta(days=3650)  # 10 years\n",
    "                    very_old = (creation_dates < recent_cutoff).sum()\n",
    "                    if very_old > len(df) * 0.1:  # More than 10% very old\n",
    "                        issues.append(\"High percentage of very old orders\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        elif table_name == 'df_afko':\n",
    "            # Order header specific checks\n",
    "            if 'GASMG' in df.columns and 'GAMNG' in df.columns:\n",
    "                # Confirmed quantity shouldn't exceed ordered quantity significantly\n",
    "                over_confirmed = ((df['GAMNG'] / df['GASMG'].replace(0, 1)) > 1.1).sum()\n",
    "                if over_confirmed > 0:\n",
    "                    issues.append(f\"{over_confirmed} orders have confirmed quantity > 110% of ordered\")\n",
    "        \n",
    "        elif table_name == 'df_qmel':\n",
    "            # Quality notification specific checks\n",
    "            if 'PRIOK' in df.columns:\n",
    "                # Priority should be within expected range\n",
    "                invalid_priority = (~df['PRIOK'].between(1, 9, na=True)).sum()\n",
    "                if invalid_priority > 0:\n",
    "                    issues.append(f\"{invalid_priority} notifications have invalid priority values\")\n",
    "            \n",
    "            if 'QMART' in df.columns:\n",
    "                # Should have valid notification types\n",
    "                if df['QMART'].notna().sum() == 0:\n",
    "                    issues.append(\"No quality notification types specified\")\n",
    "        \n",
    "        elif table_name == 'df_qmfe':\n",
    "            # Quality defect specific checks\n",
    "            if 'ANZFEHLER' in df.columns:\n",
    "                # Number of defects should be reasonable\n",
    "                extreme_defects = (df['ANZFEHLER'] > 1000).sum()\n",
    "                if extreme_defects > 0:\n",
    "                    issues.append(f\"{extreme_defects} defect records have unrealistic defect counts\")\n",
    "        \n",
    "        elif table_name == 'df_aufm':\n",
    "            # Goods movement specific checks\n",
    "            if 'MENGE' in df.columns and 'SHKZG' in df.columns:\n",
    "                # Quantity and debit/credit should be consistent\n",
    "                inconsistent_signs = 0\n",
    "                for idx, row in df.iterrows():\n",
    "                    if pd.notna(row.get('MENGE')) and pd.notna(row.get('SHKZG')):\n",
    "                        if row['SHKZG'] == 'H' and row['MENGE'] > 0:  # Credit with positive qty\n",
    "                            inconsistent_signs += 1\n",
    "                        elif row['SHKZG'] == 'S' and row['MENGE'] < 0:  # Debit with negative qty\n",
    "                            inconsistent_signs += 1\n",
    "                \n",
    "                if inconsistent_signs > 0:\n",
    "                    issues.append(f\"{inconsistent_signs} movements have inconsistent debit/credit signs\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def _get_critical_columns(self, table_name):\n",
    "        \"\"\"\n",
    "        Get critical columns for each table type\n",
    "        \"\"\"\n",
    "        critical_columns = {\n",
    "            'df_aufk': ['AUFNR', 'AUART', 'ERDAT'],\n",
    "            'df_afko': ['AUFNR', 'GSTRP', 'GLTRP'],\n",
    "            'df_afpo': ['AUFNR', 'POSNR', 'MATNR'],\n",
    "            'df_aufm': ['AUFNR', 'MBLNR', 'BWART', 'MATNR'],\n",
    "            'df_qmel': ['QMNUM', 'QMART', 'AUFNR'],\n",
    "            'df_qmfe': ['QMNUM', 'FENUM', 'FECOD'],\n",
    "            'df_qmur': ['QMNUM', 'URNUM', 'URCOD'],\n",
    "            'df_qmih': ['QMNUM', 'IWERK'],\n",
    "            'df_qpcd': ['KATALOGART', 'CODEGRUPPE', 'CODE'],\n",
    "            'df_qpct': ['KATALOGART', 'CODEGRUPPE', 'CODE', 'KURZTEXT'],\n",
    "            'df_crhd_v1': ['OBJID', 'ARBPL'],\n",
    "            'df_jest': ['OBJNR', 'STAT']\n",
    "        }\n",
    "        \n",
    "        return critical_columns.get(table_name, [])\n",
    "    \n",
    "    def _get_key_columns(self, table_name):\n",
    "        \"\"\"\n",
    "        Get key columns for duplicate detection\n",
    "        \"\"\"\n",
    "        key_columns = {\n",
    "            'df_aufk': ['MANDT', 'AUFNR'],\n",
    "            'df_afko': ['MANDT', 'AUFNR'],\n",
    "            'df_afpo': ['MANDT', 'AUFNR', 'POSNR'],\n",
    "            'df_aufm': ['MANDT', 'MBLNR', 'MJAHR', 'ZEILE'],\n",
    "            'df_qmel': ['MANDT', 'QMNUM'],\n",
    "            'df_qmfe': ['MANDT', 'QMNUM', 'FENUM'],\n",
    "            'df_qmur': ['MANDT', 'QMNUM', 'FENUM', 'URNUM'],\n",
    "            'df_qmih': ['MANDT', 'QMNUM'],\n",
    "            'df_qpcd': ['MANDT', 'KATALOGART', 'CODEGRUPPE', 'CODE'],\n",
    "            'df_qpct': ['MANDT', 'KATALOGART', 'CODEGRUPPE', 'CODE', 'SPRACHE'],\n",
    "            'df_crhd_v1': ['MANDT', 'OBJTY', 'OBJID', 'SPRAS'],\n",
    "            'df_jest': ['MANDT', 'OBJNR', 'STAT']\n",
    "        }\n",
    "        \n",
    "        return key_columns.get(table_name, [])\n",
    "    \n",
    "    def _generate_recommendations(self, validation_summary):\n",
    "        \"\"\"\n",
    "        Generate specific recommendations based on validation results\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        overall_score = validation_summary['overall_quality_score']\n",
    "        \n",
    "        # Overall score recommendations\n",
    "        if overall_score < 70:\n",
    "            recommendations.append(\"CRITICAL: Data quality is poor. Immediate data cleansing required.\")\n",
    "        elif overall_score < 85:\n",
    "            recommendations.append(\"Data quality needs improvement. Implement data governance processes.\")\n",
    "        else:\n",
    "            recommendations.append(\"Data quality is good. Continue monitoring and maintenance.\")\n",
    "        \n",
    "        # Specific issue recommendations\n",
    "        for issue in validation_summary['issues_found']:\n",
    "            if 'duplicate' in issue.lower():\n",
    "                recommendations.append(\"Implement data deduplication process for affected tables\")\n",
    "            elif 'missing' in issue.lower() or 'completeness' in issue.lower():\n",
    "                recommendations.append(\"Improve data entry processes to reduce missing values\")\n",
    "            elif 'date' in issue.lower():\n",
    "                recommendations.append(\"Validate date formats and ranges during data entry\")\n",
    "            elif 'quantity' in issue.lower() or 'negative' in issue.lower():\n",
    "                recommendations.append(\"Add business logic validation for numeric fields\")\n",
    "        \n",
    "        # Table-specific recommendations\n",
    "        for table_name, score in validation_summary['quality_scores'].items():\n",
    "            if score < 70:\n",
    "                recommendations.append(f\"Focus data improvement efforts on {table_name.replace('df_', '').upper()} table\")\n",
    "        \n",
    "        return list(set(recommendations))  # Remove duplicates\n",
    "    \n",
    "    def generate_data_quality_report(self, validation_summary, output_file=None):\n",
    "        \"\"\"\n",
    "        Generate comprehensive data quality report\n",
    "        \"\"\"\n",
    "        print(\"\\nüìä GENERATING DATA QUALITY REPORT...\")\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        if output_file is None:\n",
    "            output_file = f\"sap_data_quality_report_{timestamp}.txt\"\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"SAP DATA QUALITY ASSESSMENT REPORT\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            f.write(f\"Generated: {validation_summary['timestamp']}\\n\")\n",
    "            f.write(f\"Overall Quality Score: {validation_summary['overall_quality_score']}/100\\n\\n\")\n",
    "            \n",
    "            # Executive Summary\n",
    "            f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Tables Validated: {validation_summary['tables_validated']}\\n\")\n",
    "            f.write(f\"Total Records: {validation_summary['total_records']:,}\\n\")\n",
    "            f.write(f\"Issues Identified: {len(validation_summary['issues_found'])}\\n\\n\")\n",
    "            \n",
    "            # Quality Scores by Table\n",
    "            f.write(\"QUALITY SCORES BY TABLE\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for table, score in validation_summary['quality_scores'].items():\n",
    "                status = \"‚úì GOOD\" if score >= 85 else \"‚ö† NEEDS IMPROVEMENT\" if score >= 70 else \"‚ùå POOR\"\n",
    "                f.write(f\"{table.replace('df_', '').upper():15} {score:6.1f}/100  {status}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Issues Found\n",
    "            if validation_summary['issues_found']:\n",
    "                f.write(\"ISSUES IDENTIFIED\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                for i, issue in enumerate(validation_summary['issues_found'], 1):\n",
    "                    f.write(f\"{i:2d}. {issue}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            f.write(\"RECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for i, rec in enumerate(validation_summary['recommendations'], 1):\n",
    "                f.write(f\"{i:2d}. {rec}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Detailed Findings\n",
    "            f.write(\"DETAILED FINDINGS BY TABLE\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for table_name, results in self.validation_results.items():\n",
    "                f.write(f\"\\n{table_name.replace('df_', '').upper()}\\n\")\n",
    "                f.write(f\"Records: {results['record_count']:,}\\n\")\n",
    "                f.write(f\"Columns: {results['column_count']}\\n\")\n",
    "                f.write(f\"Quality Score: {results['quality_score']}/100\\n\")\n",
    "                \n",
    "                if results['data_completeness']['overall_completeness'] < 95:\n",
    "                    f.write(f\"Data Completeness: {results['data_completeness']['overall_completeness']}%\\n\")\n",
    "                \n",
    "                if results['issues']:\n",
    "                    f.write(\"Issues:\\n\")\n",
    "                    for issue in results['issues']:\n",
    "                        f.write(f\"  ‚Ä¢ {issue}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"   ‚úÖ Report saved to: {output_file}\")\n",
    "        return output_file\n",
    "    \n",
    "    def suggest_data_improvements(self, validation_summary):\n",
    "        \"\"\"\n",
    "        Suggest specific data improvement actions\n",
    "        \"\"\"\n",
    "        print(\"\\nüí° GENERATING DATA IMPROVEMENT SUGGESTIONS...\")\n",
    "        \n",
    "        improvements = {\n",
    "            'immediate_actions': [],\n",
    "            'process_improvements': [],\n",
    "            'system_enhancements': [],\n",
    "            'training_needs': []\n",
    "        }\n",
    "        \n",
    "        # Analyze validation results for specific suggestions\n",
    "        overall_score = validation_summary['overall_quality_score']\n",
    "        \n",
    "        if overall_score < 70:\n",
    "            improvements['immediate_actions'].extend([\n",
    "                \"Stop using affected data for critical analysis until cleaned\",\n",
    "                \"Perform emergency data cleansing on critical tables\",\n",
    "                \"Identify and fix root causes of data quality issues\"\n",
    "            ])\n",
    "        \n",
    "        # Check for common issues\n",
    "        issues_text = ' '.join(validation_summary['issues_found']).lower()\n",
    "        \n",
    "        if 'duplicate' in issues_text:\n",
    "            improvements['process_improvements'].append(\n",
    "                \"Implement master data management (MDM) processes\"\n",
    "            )\n",
    "            improvements['system_enhancements'].append(\n",
    "                \"Add duplicate detection rules to data entry systems\"\n",
    "            )\n",
    "        \n",
    "        if 'missing' in issues_text or 'completeness' in issues_text:\n",
    "            improvements['process_improvements'].append(\n",
    "                \"Make critical fields mandatory in data entry forms\"\n",
    "            )\n",
    "            improvements['training_needs'].append(\n",
    "                \"Train users on importance of complete data entry\"\n",
    "            )\n",
    "        \n",
    "        if 'date' in issues_text:\n",
    "            improvements['system_enhancements'].append(\n",
    "                \"Implement date validation controls in SAP\"\n",
    "            )\n",
    "            improvements['training_needs'].append(\n",
    "                \"Provide training on proper date entry formats\"\n",
    "            )\n",
    "        \n",
    "        if 'quantity' in issues_text or 'negative' in issues_text:\n",
    "            improvements['process_improvements'].append(\n",
    "                \"Add business logic validation for quantity fields\"\n",
    "            )\n",
    "            improvements['system_enhancements'].append(\n",
    "                \"Implement range checks for numeric fields\"\n",
    "            )\n",
    "        \n",
    "        # Table-specific suggestions\n",
    "        poor_tables = [table for table, score in validation_summary['quality_scores'].items() if score < 70]\n",
    "        if poor_tables:\n",
    "            improvements['immediate_actions'].append(\n",
    "                f\"Focus data cleansing efforts on: {', '.join([t.replace('df_', '') for t in poor_tables])}\"\n",
    "            )\n",
    "        \n",
    "        print(\"   ‚úÖ Data improvement suggestions generated\")\n",
    "        return improvements\n",
    "\n",
    "def create_data_monitoring_plan(validation_summary):\n",
    "    \"\"\"\n",
    "    Create ongoing data monitoring plan\n",
    "    \"\"\"\n",
    "    print(\"üìã CREATING DATA MONITORING PLAN...\")\n",
    "    \n",
    "    monitoring_plan = {\n",
    "        'daily_checks': [],\n",
    "        'weekly_checks': [],\n",
    "        'monthly_checks': [],\n",
    "        'automated_alerts': [],\n",
    "        'data_steward_responsibilities': {}\n",
    "    }\n",
    "    \n",
    "    # Daily checks for critical issues\n",
    "    if validation_summary['overall_quality_score'] < 85:\n",
    "        monitoring_plan['daily_checks'].extend([\n",
    "            \"Check for new duplicate records in master data tables\",\n",
    "            \"Monitor completeness of critical fields\",\n",
    "            \"Validate new order entries for data consistency\"\n",
    "        ])\n",
    "    \n",
    "    # Weekly checks\n",
    "    monitoring_plan['weekly_checks'].extend([\n",
    "        \"Review data quality metrics dashboard\",\n",
    "        \"Analyze data completeness trends\",\n",
    "        \"Check for new data validation rule violations\",\n",
    "        \"Review and resolve data quality alerts\"\n",
    "    ])\n",
    "    \n",
    "    # Monthly checks\n",
    "    monitoring_plan['monthly_checks'].extend([\n",
    "        \"Perform comprehensive data quality assessment\",\n",
    "        \"Review and update data validation rules\",\n",
    "        \"Analyze data quality trends and patterns\",\n",
    "        \"Generate data quality report for management\"\n",
    "    ])\n",
    "    \n",
    "    # Automated alerts\n",
    "    monitoring_plan['automated_alerts'].extend([\n",
    "        \"Alert when duplicate records exceed threshold\",\n",
    "        \"Alert when data completeness drops below 90%\",\n",
    "        \"Alert for unusual data patterns or outliers\",\n",
    "        \"Alert for failed data validation checks\"\n",
    "    ])\n",
    "    \n",
    "    # Data steward responsibilities\n",
    "    monitoring_plan['data_steward_responsibilities'] = {\n",
    "        'Master Data Steward': [\n",
    "            \"Monitor AUFK and AFKO data quality\",\n",
    "            \"Resolve master data inconsistencies\",\n",
    "            \"Maintain data validation rules\"\n",
    "        ],\n",
    "        'Quality Data Steward': [\n",
    "            \"Monitor QMEL, QMFE, QMUR data quality\",\n",
    "            \"Ensure quality notification completeness\",\n",
    "            \"Validate quality code consistency\"\n",
    "        ],\n",
    "        'Production Data Steward': [\n",
    "            \"Monitor AFPO and AUFM data quality\",\n",
    "            \"Validate production quantities and movements\",\n",
    "            \"Ensure material master data accuracy\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"   ‚úÖ Data monitoring plan created\")\n",
    "    return monitoring_plan\n",
    "\n",
    "def run_complete_validation_suite(df_aufk=None, df_afko=None, df_afpo=None, df_aufm=None,\n",
    "                                df_qmel=None, df_qmfe=None, df_qmur=None, df_qmih=None,\n",
    "                                df_qpcd=None, df_qpct=None, df_qpgt=None, \n",
    "                                df_crhd_v1=None, df_jest=None, df_plant_description=None,\n",
    "                                generate_reports=True):\n",
    "    \"\"\"\n",
    "    Run complete validation suite on all SAP tables\n",
    "    \"\"\"\n",
    "    print(\"üöÄ SAP COMPLETE DATA VALIDATION SUITE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize validator\n",
    "    validator = SAPDataValidator()\n",
    "    \n",
    "    # Prepare tables dictionary\n",
    "    tables = {\n",
    "        'df_aufk': df_aufk,\n",
    "        'df_afko': df_afko,\n",
    "        'df_afpo': df_afpo,\n",
    "        'df_aufm': df_aufm,\n",
    "        'df_qmel': df_qmel,\n",
    "        'df_qmfe': df_qmfe,\n",
    "        'df_qmur': df_qmur,\n",
    "        'df_qmih': df_qmih,\n",
    "        'df_qpcd': df_qpcd,\n",
    "        'df_qpct': df_qpct,\n",
    "        'df_qpgt': df_qpgt,\n",
    "        'df_crhd_v1': df_crhd_v1,\n",
    "        'df_jest': df_jest,\n",
    "        'df_plant_description': df_plant_description\n",
    "    }\n",
    "    \n",
    "    # Remove None tables\n",
    "    tables = {name: df for name, df in tables.items() if df is not None and not df.empty}\n",
    "    \n",
    "    # Run validation\n",
    "    validation_summary = validator.validate_all_tables(**tables)\n",
    "    \n",
    "    # Generate additional analyses\n",
    "    improvements = validator.suggest_data_improvements(validation_summary)\n",
    "    monitoring_plan = create_data_monitoring_plan(validation_summary)\n",
    "    \n",
    "    # Combine all results\n",
    "    complete_results = {\n",
    "        'validation_summary': validation_summary,\n",
    "        'detailed_results': validator.validation_results,\n",
    "        'improvement_suggestions': improvements,\n",
    "        'monitoring_plan': monitoring_plan\n",
    "    }\n",
    "    \n",
    "    # Generate reports if requested\n",
    "    if generate_reports:\n",
    "        print(\"\\nüìÑ GENERATING REPORTS...\")\n",
    "        \n",
    "        # Main data quality report\n",
    "        report_file = validator.generate_data_quality_report(validation_summary)\n",
    "        \n",
    "        # Export complete results to Excel\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        excel_file = f\"sap_validation_complete_{timestamp}.xlsx\"\n",
    "        \n",
    "        try:\n",
    "            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "                # Summary sheet\n",
    "                summary_df = pd.DataFrame([validation_summary])\n",
    "                summary_df.to_excel(writer, sheet_name='Validation_Summary', index=False)\n",
    "                \n",
    "                # Quality scores\n",
    "                scores_df = pd.DataFrame(list(validation_summary['quality_scores'].items()),\n",
    "                                       columns=['Table', 'Quality_Score'])\n",
    "                scores_df.to_excel(writer, sheet_name='Quality_Scores', index=False)\n",
    "                \n",
    "                # Issues\n",
    "                if validation_summary['issues_found']:\n",
    "                    issues_df = pd.DataFrame(validation_summary['issues_found'], columns=['Issue'])\n",
    "                    issues_df.to_excel(writer, sheet_name='Issues_Found', index=False)\n",
    "                \n",
    "                # Recommendations\n",
    "                rec_df = pd.DataFrame(validation_summary['recommendations'], columns=['Recommendation'])\n",
    "                rec_df.to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "                \n",
    "                # Improvement suggestions\n",
    "                for category, suggestions in improvements.items():\n",
    "                    if suggestions:\n",
    "                        imp_df = pd.DataFrame(suggestions, columns=[category.replace('_', ' ').title()])\n",
    "                        imp_df.to_excel(writer, sheet_name=f'Improvements_{category}', index=False)\n",
    "            \n",
    "            print(f\"   ‚úÖ Excel report created: {excel_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Excel export failed: {e}\")\n",
    "            excel_file = None\n",
    "        \n",
    "        print(f\"\\nüìÅ VALIDATION REPORTS CREATED:\")\n",
    "        print(f\"   ‚Ä¢ Text Report: {report_file}\")\n",
    "        if excel_file:\n",
    "            print(f\"   ‚Ä¢ Excel Report: {excel_file}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ VALIDATION SUITE COMPLETED!\")\n",
    "    print(f\"   üìä Overall Quality Score: {validation_summary['overall_quality_score']}/100\")\n",
    "    print(f\"   üîç Tables Validated: {validation_summary['tables_validated']}\")\n",
    "    print(f\"   üìã Total Issues: {len(validation_summary['issues_found'])}\")\n",
    "    \n",
    "    return complete_results\n",
    "\n",
    "# Usage example\n",
    "def example_validation_usage():\n",
    "    \"\"\"\n",
    "    Example of how to use the validation suite\n",
    "    \"\"\"\n",
    "    print(\"üìö SAP DATA VALIDATION USAGE EXAMPLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n1. BASIC VALIDATION:\")\n",
    "    print(\"   # Validate all your tables\")\n",
    "    print(\"   results = run_complete_validation_suite(\")\n",
    "    print(\"       df_aufk=df_aufk, df_afko=df_afko, df_afpo=df_afpo,\")\n",
    "    print(\"       df_qmel=df_qmel, df_qmfe=df_qmfe, # ... other tables\")\n",
    "    print(\"   )\")\n",
    "    \n",
    "    print(\"\\n2. ACCESS RESULTS:\")\n",
    "    print(\"   # Get overall quality score\")\n",
    "    print(\"   quality_score = results['validation_summary']['overall_quality_score']\")\n",
    "    print(\"   \")\n",
    "    print(\"   # Get specific table issues\")\n",
    "    print(\"   aufk_issues = results['detailed_results']['df_aufk']['issues']\")\n",
    "    print(\"   \")\n",
    "    print(\"   # Get improvement suggestions\")\n",
    "    print(\"   improvements = results['improvement_suggestions']\")\n",
    "    \n",
    "    print(\"\\n3. IMPLEMENT MONITORING:\")\n",
    "    print(\"   # Use monitoring plan for ongoing data quality\")\n",
    "    print(\"   monitoring = results['monitoring_plan']\")\n",
    "    print(\"   daily_checks = monitoring['daily_checks']\")\n",
    "    \n",
    "    print(\"\\n4. SCHEDULED VALIDATION:\")\n",
    "    print(\"   # Run this weekly/monthly for ongoing monitoring\")\n",
    "    print(\"   # Set up automated alerts based on quality scores\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_validation_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9134178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SAP COMPREHENSIVE QUALITY MANAGEMENT INTEGRATION\n",
      "================================================================================\n",
      "‚úì Created plant description from your data\n",
      "üìä Input Data Summary:\n",
      "   ‚Ä¢ AUFK (Orders): 3 records\n",
      "   ‚Ä¢ AFKO (Headers): 2 records\n",
      "   ‚Ä¢ AFPO (Items): 3 records\n",
      "   ‚Ä¢ QMEL (Quality): 2 records\n",
      "   ‚Ä¢ Plants: 13 plants\n",
      "\n",
      "üè≠ STEP 1: Building Production Order Foundation...\n",
      "   ‚úì Order headers joined: 3 orders\n",
      "\n",
      "üì¶ STEP 2: Processing Order Items...\n",
      "   ‚úì Order items summary added: 3 orders\n",
      "\n",
      "üìä STEP 3: Processing Goods Movements...\n",
      "   ‚úì Goods movements summary added: 3 orders\n",
      "\n",
      "üîç STEP 4: Processing Quality Notifications...\n",
      "   ‚ö†Ô∏è  Quality notifications processing failed: \"Column(s) ['PRIOK'] do not exist\"\n",
      "\n",
      "‚ö†Ô∏è  STEP 5: Processing Quality Defects...\n",
      "   ‚úì Quality defects added: 3 orders\n",
      "\n",
      "üéØ STEP 6: Processing Root Causes...\n",
      "   ‚úì Root causes added: 3 orders\n",
      "\n",
      "‚öôÔ∏è  STEP 7: Adding Work Center Information...\n",
      "   ‚úì Work centers linked via OBJID: 3 orders\n",
      "\n",
      "üìä STEP 8: Adding Status Information...\n",
      "\n",
      "üõ†Ô∏è  STEP 9: Creating KPIs and Derived Fields...\n",
      "   ‚úì KPIs and derived fields created\n",
      "   ‚úì Final comprehensive dataset: 3 orders with 26 columns\n",
      "\n",
      "üìà STEP 10: Generating Summary Statistics...\n",
      "\n",
      "‚úÖ INTEGRATION COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "üìä Final Results Summary:\n",
      "   ‚Ä¢ Total Orders: 3\n",
      "   ‚Ä¢ Total Columns: 26\n",
      "   ‚Ä¢ Plants Analyzed: 2\n",
      "   ‚Ä¢ Orders with Quality Issues: 0 (0.0%)\n",
      "   ‚Ä¢ Average Quality Score: 80.0/100\n",
      "   ‚Ä¢ Total Quality Notifications: 0\n",
      "   ‚Ä¢ Total Defects: 2.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_complete_advanced_analytics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m comprehensive_df, summary, quality_details = create_comprehensive_sap_view(\n\u001b[32m      3\u001b[39m     df_aufk, df_afko, df_afpo, df_aufm, df_qmel, df_qmfe, df_qmur, \n\u001b[32m      4\u001b[39m     df_qmih, df_qpcd, df_qpct, df_qpgt, df_crhd_v1, df_jest\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 2. Run advanced analytics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m all_analyses = \u001b[43mrun_complete_advanced_analytics\u001b[49m(\n\u001b[32m      9\u001b[39m     comprehensive_df, summary, quality_details\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 3. Validate data quality\u001b[39;00m\n\u001b[32m     13\u001b[39m validation_results = run_complete_validation_suite(\n\u001b[32m     14\u001b[39m     df_aufk, df_afko, df_afpo, df_qmel, df_qmfe  \u001b[38;5;66;03m# ... all tables\u001b[39;00m\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'run_complete_advanced_analytics' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Run core integration\n",
    "comprehensive_df, summary, quality_details = create_comprehensive_sap_view(\n",
    "    df_aufk, df_afko, df_afpo, df_aufm, df_qmel, df_qmfe, df_qmur, \n",
    "    df_qmih, df_qpcd, df_qpct, df_qpgt, df_crhd_v1, df_jest\n",
    ")\n",
    "\n",
    "# 2. Run advanced analytics\n",
    "all_analyses = run_complete_advanced_analytics(\n",
    "    comprehensive_df, summary, quality_details\n",
    ")\n",
    "\n",
    "# 3. Validate data quality\n",
    "validation_results = run_complete_validation_suite(\n",
    "    df_aufk, df_afko, df_afpo, df_qmel, df_qmfe  # ... all tables\n",
    ")\n",
    "\n",
    "# 4. Access results\n",
    "quality_score = comprehensive_df['QUALITY_SCORE'].mean()\n",
    "plant_performance = all_analyses['plant_performance']\n",
    "action_plans = all_analyses['action_plans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6c26114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ SAP RESULTS MANAGER\n",
      "==================================================\n",
      "\n",
      "Choose an option:\n",
      "1. demonstrate_folder_structure() - See folder layout\n",
      "2. example_complete_workflow() - See usage example\n",
      "3. create_standalone_result_manager() - Create manager instance\n",
      "\n",
      "Main function:\n",
      "run_complete_analysis_with_results_folder(...)\n",
      "üìÅ FOLDER STRUCTURE PREVIEW\n",
      "==================================================\n",
      "\n",
      "result/\n",
      "‚îî‚îÄ‚îÄ sap_analysis_YYYYMMDD_HHMMSS/\n",
      "    ‚îú‚îÄ‚îÄ INDEX_README_YYYYMMDD_HHMMSS.txt\n",
      "    ‚îú‚îÄ‚îÄ 01_comprehensive_data/\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ comprehensive_dataset_YYYYMMDD_HHMMSS.xlsx\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ comprehensive_dataset_YYYYMMDD_HHMMSS.csv\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ summary_stats_YYYYMMDD_HHMMSS.json\n",
      "    ‚îú‚îÄ‚îÄ 02_advanced_analytics/\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ advanced_analytics_YYYYMMDD_HHMMSS.xlsx\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ advanced_analytics_YYYYMMDD_HHMMSS.json\n",
      "    ‚îú‚îÄ‚îÄ 03_data_validation/\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ data_quality_report_YYYYMMDD_HHMMSS.txt\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ data_validation_complete_YYYYMMDD_HHMMSS.xlsx\n",
      "    ‚îú‚îÄ‚îÄ 04_executive_reports/\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ executive_summary_YYYYMMDD_HHMMSS.txt\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ executive_dashboard_YYYYMMDD_HHMMSS.json\n",
      "    ‚îú‚îÄ‚îÄ 05_detailed_analysis/\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ (Future: Additional detailed reports)\n",
      "    ‚îú‚îÄ‚îÄ 06_action_plans/\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ action_plan_YYYYMMDD_HHMMSS.txt\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ action_plan_tracker_YYYYMMDD_HHMMSS.xlsx\n",
      "    ‚îú‚îÄ‚îÄ 07_dashboards/\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ (Future: Dashboard configurations)\n",
      "    ‚îî‚îÄ‚îÄ 08_quality_scorecards/\n",
      "        ‚îú‚îÄ‚îÄ quality_scorecard_YYYYMMDD_HHMMSS.txt\n",
      "        ‚îú‚îÄ‚îÄ scorecard_A110_YYYYMMDD_HHMMSS.txt\n",
      "        ‚îú‚îÄ‚îÄ scorecard_A111_YYYYMMDD_HHMMSS.txt\n",
      "        ‚îî‚îÄ‚îÄ ... (one per plant)\n",
      "    \n",
      "\n",
      "üìã FILE DESCRIPTIONS:\n",
      "‚Ä¢ INDEX_README              - Main index with file listing and quick start guide\n",
      "‚Ä¢ comprehensive_dataset.xlsx - Main integrated dataset with all SAP data\n",
      "‚Ä¢ comprehensive_dataset.csv - CSV version for analysis tools\n",
      "‚Ä¢ advanced_analytics.xlsx   - Detailed analytics with multiple sheets\n",
      "‚Ä¢ data_quality_report.txt   - Data validation and quality assessment\n",
      "‚Ä¢ executive_summary.txt     - Management-level summary and KPIs\n",
      "‚Ä¢ action_plan.txt           - Specific improvement actions with timelines\n",
      "‚Ä¢ quality_scorecard.txt     - Performance metrics and targets\n",
      "üìö COMPLETE WORKFLOW EXAMPLE\n",
      "============================================================\n",
      "\n",
      "1. PREPARE YOUR DATA:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Load your SAP tables\n",
      "df_aufk = pd.read_csv('aufk_data.csv')\n",
      "df_afko = pd.read_csv('afko_data.csv')\n",
      "df_afpo = pd.read_csv('afpo_data.csv')\n",
      "df_qmel = pd.read_csv('qmel_data.csv')\n",
      "# ... load other tables\n",
      "```\n",
      "\n",
      "2. RUN COMPLETE ANALYSIS:\n",
      "```python\n",
      "# Run everything with organized results\n",
      "results = run_complete_analysis_with_results_folder(\n",
      "    df_aufk=df_aufk,\n",
      "    df_afko=df_afko,\n",
      "    df_afpo=df_afpo,\n",
      "    df_qmel=df_qmel,\n",
      "    df_qmfe=df_qmfe,\n",
      "    # ... other tables\n",
      "    result_folder='my_sap_analysis'  # Custom folder name\n",
      ")\n",
      "```\n",
      "\n",
      "3. ACCESS RESULTS:\n",
      "```python\n",
      "# Get the session folder path\n",
      "session_folder = results['session_folder']\n",
      "print(f'Results saved to: {session_folder}')\n",
      "\n",
      "# Access the data\n",
      "comprehensive_data = results['comprehensive_df']\n",
      "analytics = results['all_analyses']\n",
      "validation = results['validation_results']\n",
      "\n",
      "# List all created files\n",
      "for file in results['created_files']:\n",
      "    print(file)\n",
      "```\n",
      "\n",
      "4. REVIEW REPORTS:\n",
      "```\n",
      "# Start with the executive summary\n",
      "result/sap_analysis_*/04_executive_reports/executive_summary_*.txt\n",
      "\n",
      "# Check data quality\n",
      "result/sap_analysis_*/03_data_validation/data_quality_report_*.txt\n",
      "\n",
      "# Review action plans\n",
      "result/sap_analysis_*/06_action_plans/action_plan_*.txt\n",
      "\n",
      "# Use the Excel files for detailed analysis\n",
      "result/sap_analysis_*/01_comprehensive_data/comprehensive_dataset_*.xlsx\n",
      "```\n",
      "\n",
      "5. IMPLEMENT IMPROVEMENTS:\n",
      "‚Ä¢ Use action_plan_tracker.xlsx to track progress\n",
      "‚Ä¢ Share executive_summary.txt with management\n",
      "‚Ä¢ Use plant scorecards for operational reviews\n",
      "‚Ä¢ Monitor data quality scores regularly\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SAPResultsManager:\n",
    "    \"\"\"\n",
    "    Manages all SAP analysis results and exports to organized folder structure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_folder=\"result\"):\n",
    "        self.base_folder = base_folder\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.session_folder = None\n",
    "        self.created_files = []\n",
    "        \n",
    "    def setup_folder_structure(self):\n",
    "        \"\"\"\n",
    "        Create organized folder structure for results\n",
    "        \"\"\"\n",
    "        print(f\"üìÅ Setting up folder structure in '{self.base_folder}'...\")\n",
    "        \n",
    "        # Create main result folder\n",
    "        if not os.path.exists(self.base_folder):\n",
    "            os.makedirs(self.base_folder)\n",
    "            print(f\"   ‚úì Created main folder: {self.base_folder}\")\n",
    "        \n",
    "        # Create session-specific subfolder\n",
    "        self.session_folder = os.path.join(self.base_folder, f\"sap_analysis_{self.timestamp}\")\n",
    "        os.makedirs(self.session_folder, exist_ok=True)\n",
    "        print(f\"   ‚úì Created session folder: {self.session_folder}\")\n",
    "        \n",
    "        # Create subfolders for different types of outputs\n",
    "        subfolders = [\n",
    "            \"01_comprehensive_data\",\n",
    "            \"02_advanced_analytics\", \n",
    "            \"03_data_validation\",\n",
    "            \"04_executive_reports\",\n",
    "            \"05_detailed_analysis\",\n",
    "            \"06_action_plans\",\n",
    "            \"07_dashboards\",\n",
    "            \"08_quality_scorecards\"\n",
    "        ]\n",
    "        \n",
    "        for subfolder in subfolders:\n",
    "            folder_path = os.path.join(self.session_folder, subfolder)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            print(f\"   ‚úì Created: {subfolder}\")\n",
    "        \n",
    "        print(f\"   ‚úÖ Folder structure ready!\")\n",
    "        return self.session_folder\n",
    "    \n",
    "    def save_comprehensive_data(self, comprehensive_df, summary_stats, quality_details):\n",
    "        \"\"\"\n",
    "        Save comprehensive integration results\n",
    "        \"\"\"\n",
    "        print(\"\\nüíæ Saving Comprehensive Data...\")\n",
    "        folder = os.path.join(self.session_folder, \"01_comprehensive_data\")\n",
    "        \n",
    "        # Main comprehensive dataset\n",
    "        main_file = os.path.join(folder, f\"comprehensive_dataset_{self.timestamp}.xlsx\")\n",
    "        \n",
    "        with pd.ExcelWriter(main_file, engine='openpyxl') as writer:\n",
    "            # Main data\n",
    "            comprehensive_df.to_excel(writer, sheet_name='Comprehensive_Data', index=False)\n",
    "            \n",
    "            # Summary statistics\n",
    "            summary_df = pd.json_normalize(summary_stats, sep='_')\n",
    "            summary_df.to_excel(writer, sheet_name='Summary_Statistics', index=False)\n",
    "            \n",
    "            # Quality distribution\n",
    "            if 'QUALITY_CATEGORY' in comprehensive_df.columns:\n",
    "                quality_dist = comprehensive_df['QUALITY_CATEGORY'].value_counts().reset_index()\n",
    "                quality_dist.to_excel(writer, sheet_name='Quality_Distribution', index=False)\n",
    "            \n",
    "            # Plant analysis\n",
    "            plant_col = self._get_plant_column(comprehensive_df)\n",
    "            if plant_col:\n",
    "                plant_summary = comprehensive_df.groupby(plant_col).agg({\n",
    "                    'AUFNR': 'count',\n",
    "                    'QUALITY_NOTIF_COUNT': 'sum',\n",
    "                    'DEFECT_COUNT': 'sum',\n",
    "                    'QUALITY_SCORE': 'mean'\n",
    "                }).reset_index()\n",
    "                plant_summary.to_excel(writer, sheet_name='Plant_Analysis', index=False)\n",
    "            \n",
    "            # Top quality issues\n",
    "            if 'QUALITY_NOTIF_COUNT' in comprehensive_df.columns:\n",
    "                top_issues = comprehensive_df[comprehensive_df['QUALITY_NOTIF_COUNT'] > 0].nlargest(20, 'QUALITY_NOTIF_COUNT')\n",
    "                top_issues.to_excel(writer, sheet_name='Top_Quality_Issues', index=False)\n",
    "        \n",
    "        self.created_files.append(main_file)\n",
    "        print(f\"   ‚úì Comprehensive Excel: {os.path.basename(main_file)}\")\n",
    "        \n",
    "        # CSV export for easy analysis\n",
    "        csv_file = os.path.join(folder, f\"comprehensive_dataset_{self.timestamp}.csv\")\n",
    "        comprehensive_df.to_csv(csv_file, index=False)\n",
    "        self.created_files.append(csv_file)\n",
    "        print(f\"   ‚úì Comprehensive CSV: {os.path.basename(csv_file)}\")\n",
    "        \n",
    "        # JSON export for APIs\n",
    "        json_file = os.path.join(folder, f\"summary_stats_{self.timestamp}.json\")\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(summary_stats, f, indent=2, default=str)\n",
    "        self.created_files.append(json_file)\n",
    "        print(f\"   ‚úì Summary JSON: {os.path.basename(json_file)}\")\n",
    "        \n",
    "        return main_file, csv_file, json_file\n",
    "    \n",
    "    def save_advanced_analytics(self, all_analyses, comprehensive_df):\n",
    "        \"\"\"\n",
    "        Save advanced analytics results\n",
    "        \"\"\"\n",
    "        print(\"\\nüìä Saving Advanced Analytics...\")\n",
    "        folder = os.path.join(self.session_folder, \"02_advanced_analytics\")\n",
    "        \n",
    "        # Main analytics Excel\n",
    "        analytics_file = os.path.join(folder, f\"advanced_analytics_{self.timestamp}.xlsx\")\n",
    "        \n",
    "        with pd.ExcelWriter(analytics_file, engine='openpyxl') as writer:\n",
    "            # Dashboard data\n",
    "            if 'dashboard_data' in all_analyses:\n",
    "                dashboard_df = pd.json_normalize(all_analyses['dashboard_data'], sep='_')\n",
    "                dashboard_df.to_excel(writer, sheet_name='Dashboard_Data', index=False)\n",
    "            \n",
    "            # Material quality analysis\n",
    "            if 'material_quality' in all_analyses and 'top_risk_materials' in all_analyses['material_quality']:\n",
    "                material_df = pd.DataFrame(all_analyses['material_quality']['top_risk_materials']).T\n",
    "                material_df.to_excel(writer, sheet_name='Material_Quality')\n",
    "            \n",
    "            # Operational efficiency\n",
    "            if 'operational_efficiency' in all_analyses:\n",
    "                efficiency_df = pd.json_normalize(all_analyses['operational_efficiency'], sep='_')\n",
    "                efficiency_df.to_excel(writer, sheet_name='Operational_Efficiency', index=False)\n",
    "            \n",
    "            # Defect patterns\n",
    "            if 'defect_patterns' in all_analyses:\n",
    "                if 'top_defect_patterns' in all_analyses['defect_patterns']:\n",
    "                    defect_df = pd.DataFrame(all_analyses['defect_patterns']['top_defect_patterns']).T\n",
    "                    defect_df.to_excel(writer, sheet_name='Defect_Patterns')\n",
    "            \n",
    "            # Quality costs\n",
    "            if 'quality_costs' in all_analyses and 'cost_by_plant' in all_analyses['quality_costs']:\n",
    "                cost_df = pd.DataFrame(all_analyses['quality_costs']['cost_by_plant']).T\n",
    "                cost_df.to_excel(writer, sheet_name='Quality_Costs')\n",
    "            \n",
    "            # High-risk orders\n",
    "            if 'QUALITY_SCORE' in comprehensive_df.columns:\n",
    "                high_risk = comprehensive_df[comprehensive_df['QUALITY_SCORE'] < 70].copy()\n",
    "                if not high_risk.empty:\n",
    "                    high_risk.to_excel(writer, sheet_name='High_Risk_Orders', index=False)\n",
    "            \n",
    "            # Top performers\n",
    "            if 'QUALITY_SCORE' in comprehensive_df.columns:\n",
    "                top_performers = comprehensive_df[comprehensive_df['QUALITY_SCORE'] >= 95].copy()\n",
    "                if not top_performers.empty:\n",
    "                    top_performers.to_excel(writer, sheet_name='Top_Performers', index=False)\n",
    "        \n",
    "        self.created_files.append(analytics_file)\n",
    "        print(f\"   ‚úì Advanced Analytics Excel: {os.path.basename(analytics_file)}\")\n",
    "        \n",
    "        # JSON export for APIs/dashboards\n",
    "        json_file = os.path.join(folder, f\"advanced_analytics_{self.timestamp}.json\")\n",
    "        \n",
    "        # Convert non-serializable objects\n",
    "        def convert_for_json(obj):\n",
    "            if isinstance(obj, (pd.Timestamp, datetime)):\n",
    "                return obj.isoformat()\n",
    "            elif isinstance(obj, pd.Series):\n",
    "                return obj.to_dict()\n",
    "            elif isinstance(obj, pd.DataFrame):\n",
    "                return obj.to_dict('records')\n",
    "            elif isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return obj\n",
    "        \n",
    "        json_analyses = json.loads(json.dumps(all_analyses, default=convert_for_json))\n",
    "        \n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(json_analyses, f, indent=2)\n",
    "        \n",
    "        self.created_files.append(json_file)\n",
    "        print(f\"   ‚úì Advanced Analytics JSON: {os.path.basename(json_file)}\")\n",
    "        \n",
    "        return analytics_file, json_file\n",
    "    \n",
    "    def save_data_validation_results(self, validation_results):\n",
    "        \"\"\"\n",
    "        Save data validation results\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Saving Data Validation Results...\")\n",
    "        folder = os.path.join(self.session_folder, \"03_data_validation\")\n",
    "        \n",
    "        validation_summary = validation_results['validation_summary']\n",
    "        detailed_results = validation_results['detailed_results']\n",
    "        improvements = validation_results['improvement_suggestions']\n",
    "        monitoring_plan = validation_results['monitoring_plan']\n",
    "        \n",
    "        # Main validation report\n",
    "        report_file = os.path.join(folder, f\"data_quality_report_{self.timestamp}.txt\")\n",
    "        \n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(\"SAP DATA QUALITY ASSESSMENT REPORT\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            f.write(f\"Generated: {validation_summary['timestamp']}\\n\")\n",
    "            f.write(f\"Overall Quality Score: {validation_summary['overall_quality_score']}/100\\n\\n\")\n",
    "            \n",
    "            # Executive Summary\n",
    "            f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Tables Validated: {validation_summary['tables_validated']}\\n\")\n",
    "            f.write(f\"Total Records: {validation_summary['total_records']:,}\\n\")\n",
    "            f.write(f\"Issues Identified: {len(validation_summary['issues_found'])}\\n\\n\")\n",
    "            \n",
    "            # Quality Scores by Table\n",
    "            f.write(\"QUALITY SCORES BY TABLE\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for table, score in validation_summary['quality_scores'].items():\n",
    "                status = \"‚úì GOOD\" if score >= 85 else \"‚ö† NEEDS IMPROVEMENT\" if score >= 70 else \"‚ùå POOR\"\n",
    "                f.write(f\"{table.replace('df_', '').upper():15} {score:6.1f}/100  {status}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Issues Found\n",
    "            if validation_summary['issues_found']:\n",
    "                f.write(\"ISSUES IDENTIFIED\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                for i, issue in enumerate(validation_summary['issues_found'], 1):\n",
    "                    f.write(f\"{i:2d}. {issue}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            f.write(\"RECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for i, rec in enumerate(validation_summary['recommendations'], 1):\n",
    "                f.write(f\"{i:2d}. {rec}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        self.created_files.append(report_file)\n",
    "        print(f\"   ‚úì Data Quality Report: {os.path.basename(report_file)}\")\n",
    "        \n",
    "        # Excel export\n",
    "        excel_file = os.path.join(folder, f\"data_validation_complete_{self.timestamp}.xlsx\")\n",
    "        \n",
    "        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "            # Summary sheet\n",
    "            summary_df = pd.DataFrame([validation_summary])\n",
    "            summary_df.to_excel(writer, sheet_name='Validation_Summary', index=False)\n",
    "            \n",
    "            # Quality scores\n",
    "            scores_df = pd.DataFrame(list(validation_summary['quality_scores'].items()),\n",
    "                                   columns=['Table', 'Quality_Score'])\n",
    "            scores_df.to_excel(writer, sheet_name='Quality_Scores', index=False)\n",
    "            \n",
    "            # Issues\n",
    "            if validation_summary['issues_found']:\n",
    "                issues_df = pd.DataFrame(validation_summary['issues_found'], columns=['Issue'])\n",
    "                issues_df.to_excel(writer, sheet_name='Issues_Found', index=False)\n",
    "            \n",
    "            # Recommendations\n",
    "            rec_df = pd.DataFrame(validation_summary['recommendations'], columns=['Recommendation'])\n",
    "            rec_df.to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "            \n",
    "            # Improvement suggestions\n",
    "            for category, suggestions in improvements.items():\n",
    "                if suggestions:\n",
    "                    imp_df = pd.DataFrame(suggestions, columns=[category.replace('_', ' ').title()])\n",
    "                    imp_df.to_excel(writer, sheet_name=f'Improvements_{category[:15]}', index=False)\n",
    "        \n",
    "        self.created_files.append(excel_file)\n",
    "        print(f\"   ‚úì Data Validation Excel: {os.path.basename(excel_file)}\")\n",
    "        \n",
    "        return report_file, excel_file\n",
    "    \n",
    "    def save_executive_reports(self, all_analyses, comprehensive_df, validation_results):\n",
    "        \"\"\"\n",
    "        Save executive-level reports\n",
    "        \"\"\"\n",
    "        print(\"\\nüëî Saving Executive Reports...\")\n",
    "        folder = os.path.join(self.session_folder, \"04_executive_reports\")\n",
    "        \n",
    "        # Executive Summary Report\n",
    "        exec_report = os.path.join(folder, f\"executive_summary_{self.timestamp}.txt\")\n",
    "        \n",
    "        with open(exec_report, 'w') as f:\n",
    "            f.write(\"SAP QUALITY MANAGEMENT - EXECUTIVE SUMMARY\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            f.write(f\"Report Date: {datetime.now().strftime('%B %d, %Y')}\\n\")\n",
    "            f.write(f\"Analysis Period: {self.timestamp}\\n\\n\")\n",
    "            \n",
    "            # Key Metrics\n",
    "            f.write(\"KEY PERFORMANCE INDICATORS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            if 'executive_summary' in all_analyses:\n",
    "                exec_sum = all_analyses['executive_summary']\n",
    "                f.write(f\"Total Orders Analyzed: {exec_sum['overview']['total_orders']:,}\\n\")\n",
    "                f.write(f\"Data Coverage: {exec_sum['overview']['data_coverage']}%\\n\")\n",
    "                \n",
    "                if exec_sum['key_findings']:\n",
    "                    f.write(\"\\nKEY FINDINGS:\\n\")\n",
    "                    for finding in exec_sum['key_findings']:\n",
    "                        f.write(f\"‚Ä¢ {finding}\\n\")\n",
    "            \n",
    "            # Data Quality Status\n",
    "            if validation_results:\n",
    "                val_summary = validation_results['validation_summary']\n",
    "                f.write(f\"\\nDATA QUALITY STATUS\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                f.write(f\"Overall Data Quality Score: {val_summary['overall_quality_score']}/100\\n\")\n",
    "                \n",
    "                quality_status = \"EXCELLENT\" if val_summary['overall_quality_score'] >= 90 else \\\n",
    "                               \"GOOD\" if val_summary['overall_quality_score'] >= 80 else \\\n",
    "                               \"NEEDS IMPROVEMENT\" if val_summary['overall_quality_score'] >= 70 else \"POOR\"\n",
    "                f.write(f\"Data Quality Status: {quality_status}\\n\")\n",
    "            \n",
    "            # Quality Performance\n",
    "            if 'quality_analysis' in all_analyses.get('dashboard_data', {}):\n",
    "                qa = all_analyses['dashboard_data']['quality_analysis'] \n",
    "                f.write(f\"\\nQUALITY PERFORMANCE\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                f.write(f\"Quality Issue Rate: {qa.get('quality_issue_rate', 'N/A')}%\\n\")\n",
    "                f.write(f\"Average Quality Score: {qa.get('avg_quality_score', 'N/A')}/100\\n\")\n",
    "                f.write(f\"Orders with Quality Issues: {qa.get('orders_with_quality_issues', 'N/A'):,}\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            if 'executive_summary' in all_analyses and all_analyses['executive_summary'].get('recommendations'):\n",
    "                f.write(f\"\\nSTRATEGIC RECOMMENDATIONS\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                for i, rec in enumerate(all_analyses['executive_summary']['recommendations'], 1):\n",
    "                    f.write(f\"{i}. {rec}\\n\")\n",
    "            \n",
    "            # Next Steps\n",
    "            if 'executive_summary' in all_analyses and all_analyses['executive_summary'].get('next_steps'):\n",
    "                f.write(f\"\\nNEXT STEPS\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                for i, step in enumerate(all_analyses['executive_summary']['next_steps'], 1):\n",
    "                    f.write(f\"{i}. {step}\\n\")\n",
    "        \n",
    "        self.created_files.append(exec_report)\n",
    "        print(f\"   ‚úì Executive Summary: {os.path.basename(exec_report)}\")\n",
    "        \n",
    "        # Executive Dashboard Data (JSON for visualizations)\n",
    "        dashboard_json = os.path.join(folder, f\"executive_dashboard_{self.timestamp}.json\")\n",
    "        \n",
    "        exec_dashboard = {\n",
    "            'timestamp': self.timestamp,\n",
    "            'report_date': datetime.now().isoformat(),\n",
    "            'kpis': {},\n",
    "            'charts': {},\n",
    "            'alerts': []\n",
    "        }\n",
    "        \n",
    "        # Extract KPIs\n",
    "        if 'dashboard_data' in all_analyses and 'quality_kpis' in all_analyses['dashboard_data']:\n",
    "            exec_dashboard['kpis'] = all_analyses['dashboard_data']['quality_kpis']\n",
    "        \n",
    "        # Plant performance for charts\n",
    "        if 'dashboard_data' in all_analyses and 'plant_performance' in all_analyses['dashboard_data']:\n",
    "            exec_dashboard['charts']['plant_performance'] = all_analyses['dashboard_data']['plant_performance']\n",
    "        \n",
    "        # Quality alerts\n",
    "        if validation_results:\n",
    "            val_summary = validation_results['validation_summary']\n",
    "            if val_summary['overall_quality_score'] < 80:\n",
    "                exec_dashboard['alerts'].append({\n",
    "                    'type': 'warning',\n",
    "                    'message': f\"Data quality score ({val_summary['overall_quality_score']}) below target (80)\",\n",
    "                    'priority': 'high'\n",
    "                })\n",
    "            \n",
    "            if len(val_summary['issues_found']) > 10:\n",
    "                exec_dashboard['alerts'].append({\n",
    "                    'type': 'warning', \n",
    "                    'message': f\"{len(val_summary['issues_found'])} data quality issues identified\",\n",
    "                    'priority': 'medium'\n",
    "                })\n",
    "        \n",
    "        with open(dashboard_json, 'w') as f:\n",
    "            json.dump(exec_dashboard, f, indent=2, default=str)\n",
    "        \n",
    "        self.created_files.append(dashboard_json)\n",
    "        print(f\"   ‚úì Executive Dashboard JSON: {os.path.basename(dashboard_json)}\")\n",
    "        \n",
    "        return exec_report, dashboard_json\n",
    "    \n",
    "    def save_action_plans(self, all_analyses):\n",
    "        \"\"\"\n",
    "        Save action plans and recommendations\n",
    "        \"\"\"\n",
    "        print(\"\\nüìã Saving Action Plans...\")\n",
    "        folder = os.path.join(self.session_folder, \"06_action_plans\")\n",
    "        \n",
    "        if 'action_plans' not in all_analyses:\n",
    "            print(\"   ‚ö†Ô∏è  No action plans available\")\n",
    "            return None\n",
    "        \n",
    "        action_plans = all_analyses['action_plans']\n",
    "        \n",
    "        # Main action plan document\n",
    "        action_file = os.path.join(folder, f\"action_plan_{self.timestamp}.txt\")\n",
    "        \n",
    "        with open(action_file, 'w') as f:\n",
    "            f.write(\"SAP QUALITY IMPROVEMENT ACTION PLAN\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            # Immediate Actions (1-30 days)\n",
    "            f.write(\"IMMEDIATE ACTIONS (1-30 DAYS)\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for i, action in enumerate(action_plans.get('immediate_actions', []), 1):\n",
    "                f.write(f\"{i}. {action.get('action', action)}\\n\")\n",
    "                if isinstance(action, dict):\n",
    "                    f.write(f\"   Timeline: {action.get('timeline', 'TBD')}\\n\")\n",
    "                    f.write(f\"   Responsible: {action.get('responsible', 'TBD')}\\n\")\n",
    "                    f.write(f\"   Priority: {action.get('priority', 'Medium')}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Short-term Actions (1-3 months)\n",
    "            f.write(\"SHORT-TERM ACTIONS (1-3 MONTHS)\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for i, action in enumerate(action_plans.get('short_term_actions', []), 1):\n",
    "                f.write(f\"{i}. {action.get('action', action)}\\n\")\n",
    "                if isinstance(action, dict):\n",
    "                    f.write(f\"   Timeline: {action.get('timeline', 'TBD')}\\n\")\n",
    "                    f.write(f\"   Responsible: {action.get('responsible', 'TBD')}\\n\")\n",
    "                    f.write(f\"   Priority: {action.get('priority', 'Medium')}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Long-term Actions (3-12 months)\n",
    "            f.write(\"LONG-TERM ACTIONS (3-12 MONTHS)\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for i, action in enumerate(action_plans.get('long_term_actions', []), 1):\n",
    "                f.write(f\"{i}. {action.get('action', action)}\\n\")\n",
    "                if isinstance(action, dict):\n",
    "                    f.write(f\"   Timeline: {action.get('timeline', 'TBD')}\\n\")\n",
    "                    f.write(f\"   Responsible: {action.get('responsible', 'TBD')}\\n\")\n",
    "                    f.write(f\"   Priority: {action.get('priority', 'Medium')}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Responsible Parties\n",
    "            if 'responsible_parties' in action_plans:\n",
    "                f.write(\"RESPONSIBLE PARTIES\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                for party, responsibilities in action_plans['responsible_parties'].items():\n",
    "                    f.write(f\"{party}:\\n\")\n",
    "                    f.write(f\"  {responsibilities}\\n\\n\")\n",
    "        \n",
    "        self.created_files.append(action_file)\n",
    "        print(f\"   ‚úì Action Plan: {os.path.basename(action_file)}\")\n",
    "        \n",
    "        # Excel format for tracking\n",
    "        excel_file = os.path.join(folder, f\"action_plan_tracker_{self.timestamp}.xlsx\")\n",
    "        \n",
    "        # Convert action plans to tracking format\n",
    "        tracking_data = []\n",
    "        \n",
    "        for category, actions in [\n",
    "            ('Immediate', action_plans.get('immediate_actions', [])),\n",
    "            ('Short-term', action_plans.get('short_term_actions', [])),\n",
    "            ('Long-term', action_plans.get('long_term_actions', []))\n",
    "        ]:\n",
    "            for action in actions:\n",
    "                if isinstance(action, dict):\n",
    "                    tracking_data.append({\n",
    "                        'Category': category,\n",
    "                        'Action': action.get('action', ''),\n",
    "                        'Timeline': action.get('timeline', 'TBD'),\n",
    "                        'Responsible': action.get('responsible', 'TBD'),\n",
    "                        'Priority': action.get('priority', 'Medium'),\n",
    "                        'Status': 'Not Started',\n",
    "                        'Progress': 0,\n",
    "                        'Notes': ''\n",
    "                    })\n",
    "                else:\n",
    "                    tracking_data.append({\n",
    "                        'Category': category,\n",
    "                        'Action': str(action),\n",
    "                        'Timeline': 'TBD',\n",
    "                        'Responsible': 'TBD',\n",
    "                        'Priority': 'Medium',\n",
    "                        'Status': 'Not Started',\n",
    "                        'Progress': 0,\n",
    "                        'Notes': ''\n",
    "                    })\n",
    "        \n",
    "        if tracking_data:\n",
    "            tracking_df = pd.DataFrame(tracking_data)\n",
    "            tracking_df.to_excel(excel_file, index=False)\n",
    "            self.created_files.append(excel_file)\n",
    "            print(f\"   ‚úì Action Plan Tracker: {os.path.basename(excel_file)}\")\n",
    "        \n",
    "        return action_file, excel_file\n",
    "    \n",
    "    def save_quality_scorecards(self, all_analyses, comprehensive_df):\n",
    "        \"\"\"\n",
    "        Save quality scorecards\n",
    "        \"\"\"\n",
    "        print(\"\\nüèÜ Saving Quality Scorecards...\")\n",
    "        folder = os.path.join(self.session_folder, \"08_quality_scorecards\")\n",
    "        \n",
    "        # Overall scorecard\n",
    "        if 'quality_scorecard' in all_analyses:\n",
    "            scorecard = all_analyses['quality_scorecard']\n",
    "            \n",
    "            scorecard_file = os.path.join(folder, f\"quality_scorecard_{self.timestamp}.txt\")\n",
    "            \n",
    "            with open(scorecard_file, 'w') as f:\n",
    "                f.write(\"QUALITY PERFORMANCE SCORECARD\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "                f.write(f\"Period: {scorecard.get('period', self.timestamp)}\\n\")\n",
    "                f.write(f\"Scope: {scorecard.get('scope', 'All Plants')}\\n\\n\")\n",
    "                \n",
    "                f.write(\"PERFORMANCE METRICS\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                \n",
    "                for metric, data in scorecard.get('metrics', {}).items():\n",
    "                    f.write(f\"{metric}:\\n\")\n",
    "                    f.write(f\"  Current Value: {data.get('value', 'N/A')}\\n\")\n",
    "                    f.write(f\"  Target: {data.get('target', 'N/A')}\\n\")\n",
    "                    f.write(f\"  Status: {data.get('status', 'N/A')}\\n\")\n",
    "                    f.write(f\"  Trend: {data.get('trend', 'N/A')}\\n\\n\")\n",
    "            \n",
    "            self.created_files.append(scorecard_file)\n",
    "            print(f\"   ‚úì Quality Scorecard: {os.path.basename(scorecard_file)}\")\n",
    "        \n",
    "        # Plant-specific scorecards\n",
    "        plant_col = self._get_plant_column(comprehensive_df)\n",
    "        if plant_col:\n",
    "            plants = comprehensive_df[plant_col].unique()\n",
    "            \n",
    "            for plant in plants[:5]:  # Top 5 plants\n",
    "                if pd.notna(plant):\n",
    "                    plant_scorecard_file = os.path.join(folder, f\"scorecard_{plant}_{self.timestamp}.txt\")\n",
    "                    \n",
    "                    plant_data = comprehensive_df[comprehensive_df[plant_col] == plant]\n",
    "                    \n",
    "                    with open(plant_scorecard_file, 'w') as f:\n",
    "                        f.write(f\"QUALITY SCORECARD - {plant}\\n\")\n",
    "                        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "                        f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d')}\\n\")\n",
    "                        f.write(f\"Total Orders: {len(plant_data):,}\\n\\n\")\n",
    "                        \n",
    "                        if 'QUALITY_SCORE' in plant_data.columns:\n",
    "                            avg_score = plant_data['QUALITY_SCORE'].mean()\n",
    "                            f.write(f\"Average Quality Score: {avg_score:.1f}/100\\n\")\n",
    "                        \n",
    "                        if 'HAS_QUALITY_ISSUES' in plant_data.columns:\n",
    "                            issue_rate = (plant_data['HAS_QUALITY_ISSUES'].sum() / len(plant_data) * 100)\n",
    "                            f.write(f\"Quality Issue Rate: {issue_rate:.1f}%\\n\")\n",
    "                        \n",
    "                        if 'PRODUCTION_EFFICIENCY' in plant_data.columns:\n",
    "                            efficiency = plant_data['PRODUCTION_EFFICIENCY'].mean()\n",
    "                            f.write(f\"Production Efficiency: {efficiency:.1f}%\\n\")\n",
    "                    \n",
    "                    self.created_files.append(plant_scorecard_file)\n",
    "            \n",
    "            print(f\"   ‚úì Plant scorecards created for {min(len(plants), 5)} plants\")\n",
    "    \n",
    "    def generate_summary_index(self):\n",
    "        \"\"\"\n",
    "        Generate an index file listing all created reports\n",
    "        \"\"\"\n",
    "        print(\"\\nüìã Generating Summary Index...\")\n",
    "        \n",
    "        index_file = os.path.join(self.session_folder, f\"INDEX_README_{self.timestamp}.txt\")\n",
    "        \n",
    "        with open(index_file, 'w') as f:\n",
    "            f.write(\"SAP QUALITY MANAGEMENT ANALYSIS - FILE INDEX\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            f.write(f\"Analysis Session: {self.timestamp}\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Files Created: {len(self.created_files)}\\n\\n\")\n",
    "            \n",
    "            f.write(\"FOLDER STRUCTURE AND FILES\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\\n\")\n",
    "            \n",
    "            # Group files by folder\n",
    "            folder_files = {}\n",
    "            for file_path in self.created_files:\n",
    "                folder = os.path.dirname(file_path)\n",
    "                folder_name = os.path.basename(folder)\n",
    "                if folder_name not in folder_files:\n",
    "                    folder_files[folder_name] = []\n",
    "                folder_files[folder_name].append(os.path.basename(file_path))\n",
    "            \n",
    "            for folder, files in sorted(folder_files.items()):\n",
    "                f.write(f\"{folder}/\\n\")\n",
    "                for file in sorted(files):\n",
    "                    f.write(f\"  ‚îú‚îÄ‚îÄ {file}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"FILE DESCRIPTIONS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(\"01_comprehensive_data/    - Main integrated dataset and summaries\\n\")\n",
    "            f.write(\"02_advanced_analytics/    - Detailed analysis results and insights\\n\")\n",
    "            f.write(\"03_data_validation/       - Data quality assessment reports\\n\")\n",
    "            f.write(\"04_executive_reports/     - Management-level summaries\\n\")\n",
    "            f.write(\"05_detailed_analysis/     - Deep-dive analysis results\\n\")\n",
    "            f.write(\"06_action_plans/          - Improvement action plans and trackers\\n\")\n",
    "            f.write(\"07_dashboards/            - Dashboard configurations and data\\n\")\n",
    "            f.write(\"08_quality_scorecards/    - Performance scorecards by plant\\n\\n\")\n",
    "            \n",
    "            f.write(\"QUICK START GUIDE\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(\"1. Start with: 04_executive_reports/executive_summary_*.txt\\n\")\n",
    "            f.write(\"2. Review: 01_comprehensive_data/comprehensive_dataset_*.xlsx\\n\")\n",
    "            f.write(\"3. Check: 03_data_validation/data_quality_report_*.txt\\n\")\n",
    "            f.write(\"4. Implement: 06_action_plans/action_plan_*.txt\\n\")\n",
    "            f.write(\"5. Monitor: 08_quality_scorecards/quality_scorecard_*.txt\\n\")\n",
    "        \n",
    "        self.created_files.append(index_file)\n",
    "        print(f\"   ‚úì Index file: {os.path.basename(index_file)}\")\n",
    "        \n",
    "        return index_file\n",
    "    \n",
    "    def _get_plant_column(self, df):\n",
    "        \"\"\"Helper method to find the best plant column\"\"\"\n",
    "        for col in ['Plant_Name', 'Plant_Code', 'WERKS']:\n",
    "            if col in df.columns and df[col].notna().sum() > 0:\n",
    "                return col\n",
    "        return None\n",
    "\n",
    "def run_complete_analysis_with_results_folder(\n",
    "    df_aufk=None, df_afko=None, df_afpo=None, df_aufm=None,\n",
    "    df_qmel=None, df_qmfe=None, df_qmur=None, df_qmih=None,\n",
    "    df_qpcd=None, df_qpct=None, df_qpgt=None, \n",
    "    df_crhd_v1=None, df_jest=None, df_plant_description=None,\n",
    "    result_folder=\"result\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run complete SAP analysis and save all results to organized folder structure\n",
    "    \"\"\"\n",
    "    print(\"üöÄ SAP COMPLETE ANALYSIS WITH RESULTS MANAGEMENT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize results manager\n",
    "    results_manager = SAPResultsManager(result_folder)\n",
    "    session_folder = results_manager.setup_folder_structure()\n",
    "    \n",
    "    try:\n",
    "        # Import the required functions (assuming they're available)\n",
    "        from complete_sap_integration import create_comprehensive_sap_view\n",
    "        from sap_advanced_analytics import create_all_advanced_analyses\n",
    "        from sap_data_validation import run_complete_validation_suite\n",
    "        \n",
    "        print(\"\\nüîÑ Running Core Integration...\")\n",
    "        \n",
    "        # Step 1: Create comprehensive view\n",
    "        comprehensive_df, summary_stats, quality_details = create_comprehensive_sap_view(\n",
    "            df_aufk, df_afko, df_afpo, df_aufm, df_qmel, df_qmfe, df_qmur, \n",
    "            df_qmih, df_qpcd, df_qpct, df_qpgt, df_crhd_v1, df_jest, df_plant_description\n",
    "        )\n",
    "        \n",
    "        # Save comprehensive data\n",
    "        results_manager.save_comprehensive_data(comprehensive_df, summary_stats, quality_details)\n",
    "        \n",
    "        print(\"\\nüìä Running Advanced Analytics...\")\n",
    "        \n",
    "        # Step 2: Run advanced analytics\n",
    "        all_analyses = create_all_advanced_analyses(comprehensive_df, summary_stats, quality_details)\n",
    "        \n",
    "        # Save advanced analytics\n",
    "        results_manager.save_advanced_analytics(all_analyses, comprehensive_df)\n",
    "        \n",
    "        print(\"\\nüîç Running Data Validation...\")\n",
    "        \n",
    "        # Step 3: Run data validation\n",
    "        tables = {\n",
    "            'df_aufk': df_aufk, 'df_afko': df_afko, 'df_afpo': df_afpo, 'df_aufm': df_aufm,\n",
    "            'df_qmel': df_qmel, 'df_qmfe': df_qmfe, 'df_qmur': df_qmur, 'df_qmih': df_qmih,\n",
    "            'df_qpcd': df_qpcd, 'df_qpct': df_qpct, 'df_qpgt': df_qpgt,\n",
    "            'df_crhd_v1': df_crhd_v1, 'df_jest': df_jest, 'df_plant_description': df_plant_description\n",
    "        }\n",
    "        \n",
    "        # Remove None tables\n",
    "        tables = {name: df for name, df in tables.items() if df is not None and not df.empty}\n",
    "        \n",
    "        validation_results = run_complete_validation_suite(**tables, generate_reports=False)\n",
    "        \n",
    "        # Save validation results\n",
    "        results_manager.save_data_validation_results(validation_results)\n",
    "        \n",
    "        print(\"\\nüìã Generating Executive Reports...\")\n",
    "        \n",
    "        # Step 4: Save executive reports\n",
    "        results_manager.save_executive_reports(all_analyses, comprehensive_df, validation_results)\n",
    "        \n",
    "        print(\"\\nüìù Saving Action Plans...\")\n",
    "        \n",
    "        # Step 5: Save action plans\n",
    "        results_manager.save_action_plans(all_analyses)\n",
    "        \n",
    "        print(\"\\nüèÜ Creating Quality Scorecards...\")\n",
    "        \n",
    "        # Step 6: Save quality scorecards\n",
    "        results_manager.save_quality_scorecards(all_analyses, comprehensive_df)\n",
    "        \n",
    "        print(\"\\nüìë Generating Summary Index...\")\n",
    "        \n",
    "        # Step 7: Generate index file\n",
    "        index_file = results_manager.generate_summary_index()\n",
    "        \n",
    "        print(f\"\\n‚úÖ ANALYSIS COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìÅ Results saved to: {session_folder}\")\n",
    "        print(f\"üìã Total files created: {len(results_manager.created_files)}\")\n",
    "        print(f\"üìÑ Start with: {os.path.basename(index_file)}\")\n",
    "        \n",
    "        # Return summary for further use\n",
    "        return {\n",
    "            'session_folder': session_folder,\n",
    "            'comprehensive_df': comprehensive_df,\n",
    "            'all_analyses': all_analyses,\n",
    "            'validation_results': validation_results,\n",
    "            'created_files': results_manager.created_files,\n",
    "            'results_manager': results_manager\n",
    "        }\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Import Error: {e}\")\n",
    "        print(\"Please ensure all required modules are available\")\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Analysis Error: {e}\")\n",
    "        print(\"Check your data and try again\")\n",
    "        return None\n",
    "\n",
    "def create_standalone_result_manager():\n",
    "    \"\"\"\n",
    "    Create a standalone result manager for use with existing analysis results\n",
    "    \"\"\"\n",
    "    print(\"üìÅ STANDALONE RESULT MANAGER\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nThis function helps you organize existing analysis results into folders.\")\n",
    "    print(\"\\nUsage example:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# After running your analysis\")\n",
    "    print(\"results = run_complete_analysis_with_results_folder(\")\n",
    "    print(\"    df_aufk=df_aufk, df_afko=df_afko, df_afpo=df_afpo,\")\n",
    "    print(\"    df_qmel=df_qmel, df_qmfe=df_qmfe, df_qmur=df_qmur,\")\n",
    "    print(\"    # ... other tables\")\n",
    "    print(\"    result_folder='result'  # Your chosen folder name\")\n",
    "    print(\")\")\n",
    "    print(\"\")\n",
    "    print(\"# Access results\")\n",
    "    print(\"session_folder = results['session_folder']\")\n",
    "    print(\"comprehensive_data = results['comprehensive_df']\")\n",
    "    print(\"analytics = results['all_analyses']\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    return SAPResultsManager()\n",
    "\n",
    "def demonstrate_folder_structure():\n",
    "    \"\"\"\n",
    "    Show the folder structure that will be created\n",
    "    \"\"\"\n",
    "    print(\"üìÅ FOLDER STRUCTURE PREVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    structure = \"\"\"\n",
    "result/\n",
    "‚îî‚îÄ‚îÄ sap_analysis_YYYYMMDD_HHMMSS/\n",
    "    ‚îú‚îÄ‚îÄ INDEX_README_YYYYMMDD_HHMMSS.txt\n",
    "    ‚îú‚îÄ‚îÄ 01_comprehensive_data/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ comprehensive_dataset_YYYYMMDD_HHMMSS.xlsx\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ comprehensive_dataset_YYYYMMDD_HHMMSS.csv\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ summary_stats_YYYYMMDD_HHMMSS.json\n",
    "    ‚îú‚îÄ‚îÄ 02_advanced_analytics/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ advanced_analytics_YYYYMMDD_HHMMSS.xlsx\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ advanced_analytics_YYYYMMDD_HHMMSS.json\n",
    "    ‚îú‚îÄ‚îÄ 03_data_validation/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ data_quality_report_YYYYMMDD_HHMMSS.txt\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ data_validation_complete_YYYYMMDD_HHMMSS.xlsx\n",
    "    ‚îú‚îÄ‚îÄ 04_executive_reports/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ executive_summary_YYYYMMDD_HHMMSS.txt\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ executive_dashboard_YYYYMMDD_HHMMSS.json\n",
    "    ‚îú‚îÄ‚îÄ 05_detailed_analysis/\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ (Future: Additional detailed reports)\n",
    "    ‚îú‚îÄ‚îÄ 06_action_plans/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ action_plan_YYYYMMDD_HHMMSS.txt\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ action_plan_tracker_YYYYMMDD_HHMMSS.xlsx\n",
    "    ‚îú‚îÄ‚îÄ 07_dashboards/\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ (Future: Dashboard configurations)\n",
    "    ‚îî‚îÄ‚îÄ 08_quality_scorecards/\n",
    "        ‚îú‚îÄ‚îÄ quality_scorecard_YYYYMMDD_HHMMSS.txt\n",
    "        ‚îú‚îÄ‚îÄ scorecard_A110_YYYYMMDD_HHMMSS.txt\n",
    "        ‚îú‚îÄ‚îÄ scorecard_A111_YYYYMMDD_HHMMSS.txt\n",
    "        ‚îî‚îÄ‚îÄ ... (one per plant)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(structure)\n",
    "    \n",
    "    print(\"\\nüìã FILE DESCRIPTIONS:\")\n",
    "    descriptions = {\n",
    "        \"INDEX_README\": \"Main index with file listing and quick start guide\",\n",
    "        \"comprehensive_dataset.xlsx\": \"Main integrated dataset with all SAP data\",\n",
    "        \"comprehensive_dataset.csv\": \"CSV version for analysis tools\",\n",
    "        \"advanced_analytics.xlsx\": \"Detailed analytics with multiple sheets\",\n",
    "        \"data_quality_report.txt\": \"Data validation and quality assessment\",\n",
    "        \"executive_summary.txt\": \"Management-level summary and KPIs\",\n",
    "        \"action_plan.txt\": \"Specific improvement actions with timelines\",\n",
    "        \"quality_scorecard.txt\": \"Performance metrics and targets\"\n",
    "    }\n",
    "    \n",
    "    for file, desc in descriptions.items():\n",
    "        print(f\"‚Ä¢ {file:25} - {desc}\")\n",
    "\n",
    "def example_complete_workflow():\n",
    "    \"\"\"\n",
    "    Complete example workflow with result folder management\n",
    "    \"\"\"\n",
    "    print(\"üìö COMPLETE WORKFLOW EXAMPLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n1. PREPARE YOUR DATA:\")\n",
    "    print(\"```python\")\n",
    "    print(\"import pandas as pd\")\n",
    "    print(\"\")\n",
    "    print(\"# Load your SAP tables\")\n",
    "    print(\"df_aufk = pd.read_csv('aufk_data.csv')\")\n",
    "    print(\"df_afko = pd.read_csv('afko_data.csv')\")\n",
    "    print(\"df_afpo = pd.read_csv('afpo_data.csv')\")\n",
    "    print(\"df_qmel = pd.read_csv('qmel_data.csv')\")\n",
    "    print(\"# ... load other tables\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n2. RUN COMPLETE ANALYSIS:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Run everything with organized results\")\n",
    "    print(\"results = run_complete_analysis_with_results_folder(\")\n",
    "    print(\"    df_aufk=df_aufk,\")\n",
    "    print(\"    df_afko=df_afko,\")\n",
    "    print(\"    df_afpo=df_afpo,\")\n",
    "    print(\"    df_qmel=df_qmel,\")\n",
    "    print(\"    df_qmfe=df_qmfe,\")\n",
    "    print(\"    # ... other tables\")\n",
    "    print(\"    result_folder='my_sap_analysis'  # Custom folder name\")\n",
    "    print(\")\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n3. ACCESS RESULTS:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Get the session folder path\")\n",
    "    print(\"session_folder = results['session_folder']\")\n",
    "    print(\"print(f'Results saved to: {session_folder}')\")\n",
    "    print(\"\")\n",
    "    print(\"# Access the data\")\n",
    "    print(\"comprehensive_data = results['comprehensive_df']\")\n",
    "    print(\"analytics = results['all_analyses']\")\n",
    "    print(\"validation = results['validation_results']\")\n",
    "    print(\"\")\n",
    "    print(\"# List all created files\")\n",
    "    print(\"for file in results['created_files']:\")\n",
    "    print(\"    print(file)\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n4. REVIEW REPORTS:\")\n",
    "    print(\"```\")\n",
    "    print(\"# Start with the executive summary\")\n",
    "    print(\"result/sap_analysis_*/04_executive_reports/executive_summary_*.txt\")\n",
    "    print(\"\")\n",
    "    print(\"# Check data quality\")\n",
    "    print(\"result/sap_analysis_*/03_data_validation/data_quality_report_*.txt\")\n",
    "    print(\"\")\n",
    "    print(\"# Review action plans\")\n",
    "    print(\"result/sap_analysis_*/06_action_plans/action_plan_*.txt\")\n",
    "    print(\"\")\n",
    "    print(\"# Use the Excel files for detailed analysis\")\n",
    "    print(\"result/sap_analysis_*/01_comprehensive_data/comprehensive_dataset_*.xlsx\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n5. IMPLEMENT IMPROVEMENTS:\")\n",
    "    print(\"‚Ä¢ Use action_plan_tracker.xlsx to track progress\")\n",
    "    print(\"‚Ä¢ Share executive_summary.txt with management\")\n",
    "    print(\"‚Ä¢ Use plant scorecards for operational reviews\")\n",
    "    print(\"‚Ä¢ Monitor data quality scores regularly\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ SAP RESULTS MANAGER\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nChoose an option:\")\n",
    "    print(\"1. demonstrate_folder_structure() - See folder layout\")\n",
    "    print(\"2. example_complete_workflow() - See usage example\")\n",
    "    print(\"3. create_standalone_result_manager() - Create manager instance\")\n",
    "    \n",
    "    print(\"\\nMain function:\")\n",
    "    print(\"run_complete_analysis_with_results_folder(...)\")\n",
    "    \n",
    "    demonstrate_folder_structure()\n",
    "    example_complete_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c884ef5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 4. Extract key insights for your report\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m comprehensive_df = \u001b[43mresults\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mcomprehensive_df\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      3\u001b[39m all_analyses = results[\u001b[33m'\u001b[39m\u001b[33mall_analyses\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Key metrics for your assessment\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Extract key insights for your report\n",
    "comprehensive_df = results['comprehensive_df']\n",
    "all_analyses = results['all_analyses']\n",
    "\n",
    "# Key metrics for your assessment\n",
    "print(\"=== KEY FINDINGS ===\")\n",
    "print(f\"Total Orders Analyzed: {len(comprehensive_df):,}\")\n",
    "print(f\"Quality Issue Rate: {(comprehensive_df['HAS_QUALITY_ISSUES'].sum()/len(comprehensive_df)*100):.1f}%\")\n",
    "print(f\"Average Quality Score: {comprehensive_df['QUALITY_SCORE'].mean():.1f}/100\")\n",
    "\n",
    "# Plant performance analysis\n",
    "plant_performance = comprehensive_df.groupby('Plant_Code').agg({\n",
    "    'QUALITY_SCORE': 'mean',\n",
    "    'PRODUCTION_EFFICIENCY': 'mean',\n",
    "    'QUALITY_NOTIF_COUNT': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n=== PLANT PERFORMANCE ===\")\n",
    "print(plant_performance)\n",
    "\n",
    "# Material risk analysis\n",
    "if 'material_quality' in all_analyses:\n",
    "    material_risks = all_analyses['material_quality']['top_risk_materials']\n",
    "    print(\"\\n=== HIGH RISK MATERIALS ===\")\n",
    "    for material, data in list(material_risks.items())[:5]:\n",
    "        print(f\"{material}: Risk Score {data['QUALITY_RISK_SCORE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c639c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö TOLARAM ASSESSMENT - ML USAGE GUIDE\n",
      "============================================================\n",
      "\n",
      "1. LOAD YOUR DATA:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Load the Excel file\n",
      "excel_file = 'Project_Assessment_Data.xlsx'\n",
      "sheet_names = pd.ExcelFile(excel_file).sheet_names\n",
      "\n",
      "# Load each sheet\n",
      "tables = {}\n",
      "for sheet in sheet_names:\n",
      "    tables[f'df_{sheet.lower()}'] = pd.read_excel(excel_file, sheet_name=sheet)\n",
      "```\n",
      "\n",
      "2. RUN COMPLETE ANALYSIS WITH ML:\n",
      "```python\n",
      "# Run everything including ML\n",
      "assessment_results = complete_tolaram_assessment_with_ml(\n",
      "    df_aufk=tables.get('df_aufk'),\n",
      "    df_afko=tables.get('df_afko'),\n",
      "    df_afpo=tables.get('df_afpo'),\n",
      "    df_qmel=tables.get('df_qmel'),\n",
      "    # ... other tables as available\n",
      ")\n",
      "```\n",
      "\n",
      "3. ACCESS ML INSIGHTS:\n",
      "```python\n",
      "# Get ML predictions\n",
      "predictions = assessment_results['ml_assessment']['comprehensive_predictions']\n",
      "\n",
      "# Get business insights\n",
      "insights = assessment_results['ml_assessment']['assessment_insights']\n",
      "\n",
      "# Print key findings\n",
      "print('Risk Level:', insights['risk_assessment']['quality_risk_level'])\n",
      "print('Potential Savings:', insights['business_impact']['estimated_annual_savings'])\n",
      "```\n",
      "\n",
      "4. ASSESSMENT REPORT SECTIONS:\n",
      "‚Ä¢ Executive Summary: ML-driven KPIs and risk assessment\n",
      "‚Ä¢ Predictive Models: Quality issue and efficiency prediction\n",
      "‚Ä¢ Pattern Analysis: Clustering and anomaly detection\n",
      "‚Ä¢ Business Impact: Quantified savings and ROI\n",
      "‚Ä¢ Recommendations: ML-backed improvement actions\n",
      "\n",
      "5. KEY ML CAPABILITIES:\n",
      "‚Ä¢ Quality Issue Prediction (Classification)\n",
      "‚Ä¢ Production Efficiency Prediction (Regression)\n",
      "‚Ä¢ Quality Pattern Clustering (Unsupervised)\n",
      "‚Ä¢ Anomaly Detection (Outlier Analysis)\n",
      "‚Ä¢ Feature Importance Analysis\n",
      "‚Ä¢ Risk Scoring and Prioritization\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SAPMachineLearningAnalytics:\n",
    "    \"\"\"\n",
    "    Comprehensive Machine Learning Analytics for SAP Manufacturing and Quality Data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, comprehensive_df):\n",
    "        self.comprehensive_df = comprehensive_df\n",
    "        self.models = {}\n",
    "        self.predictions = {}\n",
    "        self.feature_importance = {}\n",
    "        self.model_performance = {}\n",
    "        \n",
    "    def prepare_ml_features(self):\n",
    "        \"\"\"\n",
    "        Prepare features for machine learning models\n",
    "        \"\"\"\n",
    "        print(\"üîß Preparing ML Features...\")\n",
    "        \n",
    "        df = self.comprehensive_df.copy()\n",
    "        \n",
    "        # Create feature engineering\n",
    "        feature_df = pd.DataFrame()\n",
    "        \n",
    "        # Basic order features\n",
    "        if 'AUFNR' in df.columns:\n",
    "            feature_df['ORDER_ID'] = df['AUFNR']\n",
    "        \n",
    "        # Plant features\n",
    "        plant_cols = ['Plant_Code', 'Plant_Name', 'WERKS']\n",
    "        for col in plant_cols:\n",
    "            if col in df.columns:\n",
    "                le = LabelEncoder()\n",
    "                feature_df['PLANT_ENCODED'] = le.fit_transform(df[col].fillna('Unknown'))\n",
    "                feature_df['PLANT_COUNT'] = df.groupby(col)['AUFNR'].transform('count')\n",
    "                break\n",
    "        \n",
    "        # Order type features\n",
    "        if 'AUART' in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            feature_df['ORDER_TYPE_ENCODED'] = le.fit_transform(df['AUART'].fillna('Unknown'))\n",
    "        \n",
    "        # Quantity features\n",
    "        qty_cols = ['TOTAL_PLANNED_QTY', 'TOTAL_RECEIVED_QTY', 'ORDER_ITEM_COUNT']\n",
    "        for col in qty_cols:\n",
    "            if col in df.columns:\n",
    "                feature_df[col] = df[col].fillna(0)\n",
    "                feature_df[f'{col}_LOG'] = np.log1p(df[col].fillna(0))\n",
    "        \n",
    "        # Quality features\n",
    "        quality_cols = ['QUALITY_NOTIF_COUNT', 'DEFECT_COUNT', 'CAUSE_COUNT', 'ROOT_CAUSE_COUNT']\n",
    "        for col in quality_cols:\n",
    "            if col in df.columns:\n",
    "                feature_df[col] = df[col].fillna(0)\n",
    "                feature_df[f'{col}_BINARY'] = (df[col] > 0).astype(int)\n",
    "        \n",
    "        # Production features\n",
    "        prod_cols = ['GOODS_MOVEMENT_COUNT', 'PRODUCTION_EFFICIENCY']\n",
    "        for col in prod_cols:\n",
    "            if col in df.columns:\n",
    "                feature_df[col] = df[col].fillna(0)\n",
    "        \n",
    "        # Time-based features\n",
    "        if 'ERDAT' in df.columns:\n",
    "            try:\n",
    "                dates = pd.to_datetime(df['ERDAT'], errors='coerce')\n",
    "                feature_df['CREATION_YEAR'] = dates.dt.year\n",
    "                feature_df['CREATION_MONTH'] = dates.dt.month\n",
    "                feature_df['CREATION_QUARTER'] = dates.dt.quarter\n",
    "                feature_df['CREATION_DAYOFWEEK'] = dates.dt.dayofweek\n",
    "                feature_df['DAYS_SINCE_CREATION'] = (pd.Timestamp.now() - dates).dt.days\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Target variables\n",
    "        targets = {}\n",
    "        \n",
    "        # Quality prediction targets\n",
    "        if 'HAS_QUALITY_ISSUES' in df.columns:\n",
    "            targets['quality_issues'] = df['HAS_QUALITY_ISSUES'].astype(int)\n",
    "        \n",
    "        if 'QUALITY_SCORE' in df.columns:\n",
    "            targets['quality_score'] = df['QUALITY_SCORE']\n",
    "        \n",
    "        if 'QUALITY_CATEGORY' in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            targets['quality_category'] = le.fit_transform(df['QUALITY_CATEGORY'].fillna('Good'))\n",
    "        \n",
    "        # Production efficiency targets\n",
    "        if 'PRODUCTION_EFFICIENCY' in df.columns:\n",
    "            targets['production_efficiency'] = df['PRODUCTION_EFFICIENCY']\n",
    "            targets['high_efficiency'] = (df['PRODUCTION_EFFICIENCY'] >= 95).astype(int)\n",
    "        \n",
    "        # Defect prediction targets\n",
    "        if 'HAS_DEFECTS' in df.columns:\n",
    "            targets['has_defects'] = df['HAS_DEFECTS'].astype(int)\n",
    "        \n",
    "        # Remove rows with all NaN values\n",
    "        feature_df = feature_df.dropna(how='all')\n",
    "        \n",
    "        print(f\"   ‚úì Created {len(feature_df.columns)} features for {len(feature_df)} samples\")\n",
    "        print(f\"   ‚úì Target variables: {list(targets.keys())}\")\n",
    "        \n",
    "        self.features = feature_df\n",
    "        self.targets = targets\n",
    "        \n",
    "        return feature_df, targets\n",
    "    \n",
    "    def quality_issue_prediction(self):\n",
    "        \"\"\"\n",
    "        Predict quality issues using classification models\n",
    "        \"\"\"\n",
    "        print(\"\\nüéØ Building Quality Issue Prediction Models...\")\n",
    "        \n",
    "        if 'quality_issues' not in self.targets:\n",
    "            print(\"   ‚ö†Ô∏è  Quality issues target not available\")\n",
    "            return None\n",
    "        \n",
    "        X = self.features.select_dtypes(include=[np.number]).fillna(0)\n",
    "        y = self.targets['quality_issues']\n",
    "        \n",
    "        # Remove samples where target is NaN\n",
    "        mask = ~pd.isna(y)\n",
    "        X, y = X[mask], y[mask]\n",
    "        \n",
    "        if len(X) < 50:\n",
    "            print(f\"   ‚ö†Ô∏è  Insufficient data for modeling ({len(X)} samples)\")\n",
    "            return None\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Models to try\n",
    "        models = {\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                # Train model\n",
    "                if name == 'Logistic Regression':\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "                    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = (y_pred == y_test).mean()\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train_scaled if name == 'Logistic Regression' else X_train, \n",
    "                                          y_train, cv=5, scoring='accuracy')\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'accuracy': accuracy,\n",
    "                    'cv_mean': cv_scores.mean(),\n",
    "                    'cv_std': cv_scores.std(),\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_pred_proba,\n",
    "                    'test_indices': X_test.index\n",
    "                }\n",
    "                \n",
    "                print(f\"   ‚úì {name}: Accuracy {accuracy:.3f}, CV {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {name} failed: {e}\")\n",
    "        \n",
    "        # Select best model\n",
    "        if results:\n",
    "            best_model_name = max(results.keys(), key=lambda x: results[x]['cv_mean'])\n",
    "            best_model = results[best_model_name]\n",
    "            \n",
    "            print(f\"   üèÜ Best model: {best_model_name}\")\n",
    "            \n",
    "            # Feature importance\n",
    "            if hasattr(best_model['model'], 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'importance': best_model['model'].feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                self.feature_importance['quality_issues'] = importance_df\n",
    "                print(f\"   üìä Top features: {', '.join(importance_df.head(3)['feature'].tolist())}\")\n",
    "        \n",
    "        self.models['quality_issues'] = results\n",
    "        return results\n",
    "    \n",
    "    def production_efficiency_prediction(self):\n",
    "        \"\"\"\n",
    "        Predict production efficiency using regression models\n",
    "        \"\"\"\n",
    "        print(\"\\nüìà Building Production Efficiency Prediction Models...\")\n",
    "        \n",
    "        if 'production_efficiency' not in self.targets:\n",
    "            print(\"   ‚ö†Ô∏è  Production efficiency target not available\")\n",
    "            return None\n",
    "        \n",
    "        X = self.features.select_dtypes(include=[np.number]).fillna(0)\n",
    "        y = self.targets['production_efficiency']\n",
    "        \n",
    "        # Remove samples where target is NaN\n",
    "        mask = ~pd.isna(y)\n",
    "        X, y = X[mask], y[mask]\n",
    "        \n",
    "        if len(X) < 50:\n",
    "            print(f\"   ‚ö†Ô∏è  Insufficient data for modeling ({len(X)} samples)\")\n",
    "            return None\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Models to try\n",
    "        models = {\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'Linear Regression': LinearRegression()\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                # Train model\n",
    "                if name == 'Linear Regression':\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train_scaled if name == 'Linear Regression' else X_train, \n",
    "                                          y_train, cv=5, scoring='r2')\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'mse': mse,\n",
    "                    'r2': r2,\n",
    "                    'cv_mean': cv_scores.mean(),\n",
    "                    'cv_std': cv_scores.std(),\n",
    "                    'predictions': y_pred,\n",
    "                    'test_indices': X_test.index\n",
    "                }\n",
    "                \n",
    "                print(f\"   ‚úì {name}: R¬≤ {r2:.3f}, MSE {mse:.3f}, CV {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {name} failed: {e}\")\n",
    "        \n",
    "        # Select best model\n",
    "        if results:\n",
    "            best_model_name = max(results.keys(), key=lambda x: results[x]['r2'])\n",
    "            best_model = results[best_model_name]\n",
    "            \n",
    "            print(f\"   üèÜ Best model: {best_model_name}\")\n",
    "            \n",
    "            # Feature importance\n",
    "            if hasattr(best_model['model'], 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'importance': best_model['model'].feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                self.feature_importance['production_efficiency'] = importance_df\n",
    "                print(f\"   üìä Top features: {', '.join(importance_df.head(3)['feature'].tolist())}\")\n",
    "        \n",
    "        self.models['production_efficiency'] = results\n",
    "        return results\n",
    "    \n",
    "    def quality_clustering_analysis(self):\n",
    "        \"\"\"\n",
    "        Perform clustering analysis to identify quality patterns\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Performing Quality Clustering Analysis...\")\n",
    "        \n",
    "        # Select quality-related features\n",
    "        quality_features = []\n",
    "        for col in self.features.columns:\n",
    "            if any(keyword in col.upper() for keyword in ['QUALITY', 'DEFECT', 'CAUSE', 'NOTIF']):\n",
    "                quality_features.append(col)\n",
    "        \n",
    "        if len(quality_features) < 2:\n",
    "            print(\"   ‚ö†Ô∏è  Insufficient quality features for clustering\")\n",
    "            return None\n",
    "        \n",
    "        X = self.features[quality_features].fillna(0)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Determine optimal number of clusters\n",
    "        inertias = []\n",
    "        k_range = range(2, min(10, len(X)//10))\n",
    "        \n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            kmeans.fit(X_scaled)\n",
    "            inertias.append(kmeans.inertia_)\n",
    "        \n",
    "        # Use elbow method (simple implementation)\n",
    "        if len(inertias) >= 3:\n",
    "            # Find the point with maximum rate of decrease\n",
    "            rates = [inertias[i-1] - inertias[i] for i in range(1, len(inertias))]\n",
    "            optimal_k = k_range[rates.index(max(rates)) + 1]\n",
    "        else:\n",
    "            optimal_k = 3\n",
    "        \n",
    "        # Perform clustering with optimal k\n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "        clusters = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        # Analyze clusters\n",
    "        cluster_analysis = pd.DataFrame(X)\n",
    "        cluster_analysis['Cluster'] = clusters\n",
    "        \n",
    "        cluster_summary = cluster_analysis.groupby('Cluster').agg({\n",
    "            col: 'mean' for col in quality_features\n",
    "        }).round(2)\n",
    "        \n",
    "        # Add cluster interpretation\n",
    "        cluster_profiles = {}\n",
    "        for cluster_id in range(optimal_k):\n",
    "            cluster_data = cluster_summary.loc[cluster_id]\n",
    "            \n",
    "            # Determine cluster characteristics\n",
    "            high_quality_score = cluster_data.get('QUALITY_SCORE', 0)\n",
    "            high_defects = cluster_data.get('DEFECT_COUNT', 0)\n",
    "            high_notifications = cluster_data.get('QUALITY_NOTIF_COUNT', 0)\n",
    "            \n",
    "            if high_defects > cluster_summary['DEFECT_COUNT'].mean():\n",
    "                profile = \"High Defect Risk\"\n",
    "            elif high_notifications > cluster_summary['QUALITY_NOTIF_COUNT'].mean():\n",
    "                profile = \"Quality Issues Prone\"\n",
    "            elif high_quality_score > cluster_summary.get('QUALITY_SCORE', pd.Series([0])).mean():\n",
    "                profile = \"High Quality Performance\"\n",
    "            else:\n",
    "                profile = \"Standard Performance\"\n",
    "            \n",
    "            cluster_profiles[cluster_id] = profile\n",
    "        \n",
    "        results = {\n",
    "            'optimal_k': optimal_k,\n",
    "            'clusters': clusters,\n",
    "            'cluster_summary': cluster_summary,\n",
    "            'cluster_profiles': cluster_profiles,\n",
    "            'feature_names': quality_features\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úì Identified {optimal_k} quality clusters\")\n",
    "        for cluster_id, profile in cluster_profiles.items():\n",
    "            count = sum(clusters == cluster_id)\n",
    "            print(f\"   ‚Ä¢ Cluster {cluster_id}: {profile} ({count} orders)\")\n",
    "        \n",
    "        self.models['quality_clustering'] = results\n",
    "        return results\n",
    "    \n",
    "    def anomaly_detection(self):\n",
    "        \"\"\"\n",
    "        Detect anomalous orders using isolation forest and statistical methods\n",
    "        \"\"\"\n",
    "        print(\"\\nüö® Performing Anomaly Detection...\")\n",
    "        \n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        from scipy import stats\n",
    "        \n",
    "        # Select numeric features\n",
    "        X = self.features.select_dtypes(include=[np.number]).fillna(0)\n",
    "        \n",
    "        if len(X.columns) < 3:\n",
    "            print(\"   ‚ö†Ô∏è  Insufficient features for anomaly detection\")\n",
    "            return None\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Isolation Forest\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        anomaly_labels = iso_forest.fit_predict(X_scaled)\n",
    "        \n",
    "        # Statistical anomaly detection (Z-score based)\n",
    "        z_scores = np.abs(stats.zscore(X_scaled, axis=0))\n",
    "        statistical_anomalies = (z_scores > 3).any(axis=1)\n",
    "        \n",
    "        # Combine results\n",
    "        anomalies_df = pd.DataFrame({\n",
    "            'ORDER_INDEX': X.index,\n",
    "            'ISO_FOREST_ANOMALY': anomaly_labels == -1,\n",
    "            'STATISTICAL_ANOMALY': statistical_anomalies,\n",
    "            'COMBINED_ANOMALY': (anomaly_labels == -1) | statistical_anomalies\n",
    "        })\n",
    "        \n",
    "        # Add anomaly scores\n",
    "        anomalies_df['ANOMALY_SCORE'] = iso_forest.decision_function(X_scaled)\n",
    "        \n",
    "        # Analyze anomalies\n",
    "        total_anomalies = anomalies_df['COMBINED_ANOMALY'].sum()\n",
    "        iso_anomalies = anomalies_df['ISO_FOREST_ANOMALY'].sum()\n",
    "        stat_anomalies = anomalies_df['STATISTICAL_ANOMALY'].sum()\n",
    "        \n",
    "        print(f\"   ‚úì Detected {total_anomalies} anomalous orders\")\n",
    "        print(f\"   ‚Ä¢ Isolation Forest: {iso_anomalies}\")\n",
    "        print(f\"   ‚Ä¢ Statistical: {stat_anomalies}\")\n",
    "        \n",
    "        # Get most anomalous orders\n",
    "        top_anomalies = anomalies_df.nsmallest(10, 'ANOMALY_SCORE')\n",
    "        \n",
    "        results = {\n",
    "            'anomalies_df': anomalies_df,\n",
    "            'total_anomalies': total_anomalies,\n",
    "            'top_anomalies': top_anomalies,\n",
    "            'anomaly_rate': total_anomalies / len(X) * 100\n",
    "        }\n",
    "        \n",
    "        self.models['anomaly_detection'] = results\n",
    "        return results\n",
    "    \n",
    "    def feature_importance_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze feature importance across all models\n",
    "        \"\"\"\n",
    "        print(\"\\nüìä Analyzing Feature Importance...\")\n",
    "        \n",
    "        if not self.feature_importance:\n",
    "            print(\"   ‚ö†Ô∏è  No feature importance data available\")\n",
    "            return None\n",
    "        \n",
    "        # Combine feature importance from all models\n",
    "        combined_importance = pd.DataFrame()\n",
    "        \n",
    "        for model_name, importance_df in self.feature_importance.items():\n",
    "            importance_df = importance_df.copy()\n",
    "            importance_df['model'] = model_name\n",
    "            combined_importance = pd.concat([combined_importance, importance_df], ignore_index=True)\n",
    "        \n",
    "        # Calculate average importance across models\n",
    "        avg_importance = combined_importance.groupby('feature')['importance'].agg(['mean', 'std', 'count']).reset_index()\n",
    "        avg_importance = avg_importance.sort_values('mean', ascending=False)\n",
    "        \n",
    "        print(f\"   ‚úì Analyzed {len(avg_importance)} features across {len(self.feature_importance)} models\")\n",
    "        print(\"   üìà Top 5 most important features:\")\n",
    "        for i, row in avg_importance.head().iterrows():\n",
    "            print(f\"      {i+1}. {row['feature']}: {row['mean']:.3f} (¬±{row['std']:.3f})\")\n",
    "        \n",
    "        return avg_importance\n",
    "    \n",
    "    def generate_ml_predictions(self):\n",
    "        \"\"\"\n",
    "        Generate predictions for all orders using trained models\n",
    "        \"\"\"\n",
    "        print(\"\\nüîÆ Generating ML Predictions...\")\n",
    "        \n",
    "        predictions_df = pd.DataFrame(index=self.comprehensive_df.index)\n",
    "        \n",
    "        # Quality issue predictions\n",
    "        if 'quality_issues' in self.models and self.models['quality_issues']:\n",
    "            best_model_name = max(self.models['quality_issues'].keys(), \n",
    "                                key=lambda x: self.models['quality_issues'][x]['cv_mean'])\n",
    "            model_data = self.models['quality_issues'][best_model_name]\n",
    "            \n",
    "            # Get full dataset predictions\n",
    "            X_full = self.features.select_dtypes(include=[np.number]).fillna(0)\n",
    "            \n",
    "            try:\n",
    "                if best_model_name == 'Logistic Regression':\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(X_full)\n",
    "                    quality_pred = model_data['model'].predict_proba(X_scaled)[:, 1]\n",
    "                else:\n",
    "                    quality_pred = model_data['model'].predict_proba(X_full)[:, 1]\n",
    "                \n",
    "                predictions_df['QUALITY_ISSUE_PROBABILITY'] = quality_pred\n",
    "                predictions_df['QUALITY_ISSUE_PREDICTION'] = quality_pred > 0.5\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Quality prediction failed: {e}\")\n",
    "        \n",
    "        # Production efficiency predictions\n",
    "        if 'production_efficiency' in self.models and self.models['production_efficiency']:\n",
    "            best_model_name = max(self.models['production_efficiency'].keys(), \n",
    "                                key=lambda x: self.models['production_efficiency'][x]['r2'])\n",
    "            model_data = self.models['production_efficiency'][best_model_name]\n",
    "            \n",
    "            X_full = self.features.select_dtypes(include=[np.number]).fillna(0)\n",
    "            \n",
    "            try:\n",
    "                if best_model_name == 'Linear Regression':\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(X_full)\n",
    "                    efficiency_pred = model_data['model'].predict(X_scaled)\n",
    "                else:\n",
    "                    efficiency_pred = model_data['model'].predict(X_full)\n",
    "                \n",
    "                predictions_df['PREDICTED_EFFICIENCY'] = efficiency_pred\n",
    "                predictions_df['EFFICIENCY_IMPROVEMENT_POTENTIAL'] = np.maximum(0, 100 - efficiency_pred)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Efficiency prediction failed: {e}\")\n",
    "        \n",
    "        # Quality clusters\n",
    "        if 'quality_clustering' in self.models:\n",
    "            cluster_data = self.models['quality_clustering']\n",
    "            predictions_df['QUALITY_CLUSTER'] = cluster_data['clusters']\n",
    "            \n",
    "            # Map cluster profiles\n",
    "            cluster_mapping = cluster_data['cluster_profiles']\n",
    "            predictions_df['QUALITY_CLUSTER_PROFILE'] = predictions_df['QUALITY_CLUSTER'].map(cluster_mapping)\n",
    "        \n",
    "        # Anomaly flags\n",
    "        if 'anomaly_detection' in self.models:\n",
    "            anomaly_data = self.models['anomaly_detection']\n",
    "            anomaly_df = anomaly_data['anomalies_df'].set_index('ORDER_INDEX')\n",
    "            \n",
    "            for col in ['ISO_FOREST_ANOMALY', 'STATISTICAL_ANOMALY', 'COMBINED_ANOMALY', 'ANOMALY_SCORE']:\n",
    "                if col in anomaly_df.columns:\n",
    "                    predictions_df[col] = anomaly_df[col]\n",
    "        \n",
    "        print(f\"   ‚úì Generated {len(predictions_df.columns)} prediction columns\")\n",
    "        \n",
    "        self.predictions = predictions_df\n",
    "        return predictions_df\n",
    "    \n",
    "    def create_ml_insights(self):\n",
    "        \"\"\"\n",
    "        Create actionable insights from ML models\n",
    "        \"\"\"\n",
    "        print(\"\\nüí° Generating ML-Based Insights...\")\n",
    "        \n",
    "        insights = {\n",
    "            'high_risk_orders': [],\n",
    "            'improvement_opportunities': [],\n",
    "            'quality_patterns': [],\n",
    "            'efficiency_drivers': [],\n",
    "            'anomaly_alerts': []\n",
    "        }\n",
    "        \n",
    "        if self.predictions.empty:\n",
    "            print(\"   ‚ö†Ô∏è  No predictions available for insights\")\n",
    "            return insights\n",
    "        \n",
    "        # High-risk orders\n",
    "        if 'QUALITY_ISSUE_PROBABILITY' in self.predictions.columns:\n",
    "            high_risk = self.predictions[self.predictions['QUALITY_ISSUE_PROBABILITY'] > 0.7]\n",
    "            insights['high_risk_orders'] = {\n",
    "                'count': len(high_risk),\n",
    "                'percentage': len(high_risk) / len(self.predictions) * 100,\n",
    "                'order_indices': high_risk.index.tolist()[:10],  # Top 10\n",
    "                'avg_probability': high_risk['QUALITY_ISSUE_PROBABILITY'].mean()\n",
    "            }\n",
    "        \n",
    "        # Efficiency improvement opportunities\n",
    "        if 'EFFICIENCY_IMPROVEMENT_POTENTIAL' in self.predictions.columns:\n",
    "            high_potential = self.predictions[self.predictions['EFFICIENCY_IMPROVEMENT_POTENTIAL'] > 20]\n",
    "            insights['improvement_opportunities'] = {\n",
    "                'count': len(high_potential),\n",
    "                'avg_potential': high_potential['EFFICIENCY_IMPROVEMENT_POTENTIAL'].mean(),\n",
    "                'total_potential': high_potential['EFFICIENCY_IMPROVEMENT_POTENTIAL'].sum(),\n",
    "                'order_indices': high_potential.index.tolist()[:10]\n",
    "            }\n",
    "        \n",
    "        # Quality cluster patterns\n",
    "        if 'QUALITY_CLUSTER_PROFILE' in self.predictions.columns:\n",
    "            cluster_dist = self.predictions['QUALITY_CLUSTER_PROFILE'].value_counts()\n",
    "            insights['quality_patterns'] = {\n",
    "                'cluster_distribution': cluster_dist.to_dict(),\n",
    "                'dominant_pattern': cluster_dist.index[0] if len(cluster_dist) > 0 else None,\n",
    "                'pattern_percentage': cluster_dist.iloc[0] / len(self.predictions) * 100 if len(cluster_dist) > 0 else 0\n",
    "            }\n",
    "        \n",
    "        # Feature importance insights\n",
    "        if self.feature_importance:\n",
    "            top_features = []\n",
    "            for model_name, importance_df in self.feature_importance.items():\n",
    "                top_features.extend(importance_df.head(3)['feature'].tolist())\n",
    "            \n",
    "            insights['efficiency_drivers'] = {\n",
    "                'top_features': list(set(top_features)),\n",
    "                'models_analyzed': list(self.feature_importance.keys())\n",
    "            }\n",
    "        \n",
    "        # Anomaly alerts\n",
    "        if 'COMBINED_ANOMALY' in self.predictions.columns:\n",
    "            anomalies = self.predictions[self.predictions['COMBINED_ANOMALY'] == True]\n",
    "            insights['anomaly_alerts'] = {\n",
    "                'count': len(anomalies),\n",
    "                'percentage': len(anomalies) / len(self.predictions) * 100,\n",
    "                'order_indices': anomalies.index.tolist()[:5],  # Top 5\n",
    "                'requires_investigation': len(anomalies) > 0\n",
    "            }\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"   üìä High-risk orders identified: {insights['high_risk_orders'].get('count', 0)}\")\n",
    "        print(f\"   ‚ö° Efficiency improvement opportunities: {insights['improvement_opportunities'].get('count', 0)}\")\n",
    "        print(f\"   üö® Anomalies requiring investigation: {insights['anomaly_alerts'].get('count', 0)}\")\n",
    "        \n",
    "        return insights\n",
    "\n",
    "def run_complete_ml_analysis(comprehensive_df, result_folder=\"result\"):\n",
    "    \"\"\"\n",
    "    Run complete machine learning analysis on SAP data\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ RUNNING COMPLETE ML ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize ML analyzer\n",
    "    ml_analyzer = SAPMachineLearningAnalytics(comprehensive_df)\n",
    "    \n",
    "    # Prepare features\n",
    "    features, targets = ml_analyzer.prepare_ml_features()\n",
    "    \n",
    "    if features.empty:\n",
    "        print(\"‚ùå No features available for ML analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Run all ML analyses\n",
    "    ml_results = {}\n",
    "    \n",
    "    # 1. Quality issue prediction\n",
    "    quality_results = ml_analyzer.quality_issue_prediction()\n",
    "    if quality_results:\n",
    "        ml_results['quality_prediction'] = quality_results\n",
    "    \n",
    "    # 2. Production efficiency prediction  \n",
    "    efficiency_results = ml_analyzer.production_efficiency_prediction()\n",
    "    if efficiency_results:\n",
    "        ml_results['efficiency_prediction'] = efficiency_results\n",
    "    \n",
    "    # 3. Quality clustering\n",
    "    clustering_results = ml_analyzer.quality_clustering_analysis()\n",
    "    if clustering_results:\n",
    "        ml_results['quality_clustering'] = clustering_results\n",
    "    \n",
    "    # 4. Anomaly detection\n",
    "    anomaly_results = ml_analyzer.anomaly_detection()\n",
    "    if anomaly_results:\n",
    "        ml_results['anomaly_detection'] = anomaly_results\n",
    "    \n",
    "    # 5. Feature importance analysis\n",
    "    feature_importance = ml_analyzer.feature_importance_analysis()\n",
    "    if feature_importance is not None:\n",
    "        ml_results['feature_importance'] = feature_importance\n",
    "    \n",
    "    # 6. Generate predictions\n",
    "    predictions = ml_analyzer.generate_ml_predictions()\n",
    "    ml_results['predictions'] = predictions\n",
    "    \n",
    "    # 7. Create insights\n",
    "    insights = ml_analyzer.create_ml_insights()\n",
    "    ml_results['insights'] = insights\n",
    "    \n",
    "    print(f\"\\n‚úÖ ML Analysis Complete!\")\n",
    "    print(f\"   üîß Models trained: {len([k for k in ml_results.keys() if 'prediction' in k])}\")\n",
    "    print(f\"   üìä Predictions generated: {len(predictions.columns) if not predictions.empty else 0}\")\n",
    "    print(f\"   üí° Insights created: {len(insights)}\")\n",
    "    \n",
    "    return {\n",
    "        'ml_analyzer': ml_analyzer,\n",
    "        'ml_results': ml_results,\n",
    "        'features': features,\n",
    "        'targets': targets,\n",
    "        'predictions': predictions,\n",
    "        'insights': insights\n",
    "    }\n",
    "\n",
    "# Example usage for the assessment\n",
    "def tolaram_ml_assessment_workflow(comprehensive_df):\n",
    "    \"\"\"\n",
    "    Specific ML workflow for Tolaram assessment\n",
    "    \"\"\"\n",
    "    print(\"üéØ TOLARAM ASSESSMENT - ML WORKFLOW\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Run complete ML analysis\n",
    "    ml_analysis = run_complete_ml_analysis(comprehensive_df)\n",
    "    \n",
    "    if ml_analysis is None:\n",
    "        return None\n",
    "    \n",
    "    insights = ml_analysis['insights']\n",
    "    predictions = ml_analysis['predictions']\n",
    "    \n",
    "    # Create assessment-specific insights\n",
    "    assessment_insights = {\n",
    "        'executive_summary': {},\n",
    "        'operational_recommendations': [],\n",
    "        'predictive_capabilities': {},\n",
    "        'risk_assessment': {},\n",
    "        'business_impact': {}\n",
    "    }\n",
    "    \n",
    "    # Executive Summary for Assessment\n",
    "    total_orders = len(comprehensive_df)\n",
    "    \n",
    "    assessment_insights['executive_summary'] = {\n",
    "        'total_orders_analyzed': total_orders,\n",
    "        'ml_models_developed': len([k for k in ml_analysis['ml_results'].keys() if 'prediction' in k]),\n",
    "        'high_risk_orders_identified': insights.get('high_risk_orders', {}).get('count', 0),\n",
    "        'efficiency_opportunities': insights.get('improvement_opportunities', {}).get('count', 0),\n",
    "        'anomalies_detected': insights.get('anomaly_alerts', {}).get('count', 0)\n",
    "    }\n",
    "    \n",
    "    # Operational Recommendations\n",
    "    if insights.get('high_risk_orders', {}).get('count', 0) > 0:\n",
    "        risk_rate = insights['high_risk_orders']['percentage']\n",
    "        assessment_insights['operational_recommendations'].append({\n",
    "            'priority': 'HIGH',\n",
    "            'area': 'Quality Management',\n",
    "            'recommendation': f'Implement preventive measures for {insights[\"high_risk_orders\"][\"count\"]} high-risk orders ({risk_rate:.1f}% of total)',\n",
    "            'expected_impact': 'Reduce quality incidents by 30-50%'\n",
    "        })\n",
    "    \n",
    "    if insights.get('improvement_opportunities', {}).get('count', 0) > 0:\n",
    "        avg_potential = insights['improvement_opportunities']['avg_potential']\n",
    "        assessment_insights['operational_recommendations'].append({\n",
    "            'priority': 'MEDIUM',\n",
    "            'area': 'Production Efficiency', \n",
    "            'recommendation': f'Focus improvement efforts on {insights[\"improvement_opportunities\"][\"count\"]} orders with {avg_potential:.1f}% average efficiency gain potential',\n",
    "            'expected_impact': f'Increase overall efficiency by {avg_potential/4:.1f}%'\n",
    "        })\n",
    "    \n",
    "    if insights.get('anomaly_alerts', {}).get('count', 0) > 0:\n",
    "        assessment_insights['operational_recommendations'].append({\n",
    "            'priority': 'HIGH',\n",
    "            'area': 'Process Investigation',\n",
    "            'recommendation': f'Investigate {insights[\"anomaly_alerts\"][\"count\"]} anomalous orders for process deviations',\n",
    "            'expected_impact': 'Identify and eliminate root causes of process variations'\n",
    "        })\n",
    "    \n",
    "    # Predictive Capabilities Assessment\n",
    "    model_performance = {}\n",
    "    \n",
    "    if 'quality_prediction' in ml_analysis['ml_results']:\n",
    "        best_quality_model = max(ml_analysis['ml_results']['quality_prediction'].keys(),\n",
    "                               key=lambda x: ml_analysis['ml_results']['quality_prediction'][x]['cv_mean'])\n",
    "        quality_accuracy = ml_analysis['ml_results']['quality_prediction'][best_quality_model]['cv_mean']\n",
    "        model_performance['quality_prediction'] = {\n",
    "            'model_type': best_quality_model,\n",
    "            'accuracy': quality_accuracy,\n",
    "            'business_value': 'Predict quality issues before they occur'\n",
    "        }\n",
    "    \n",
    "    if 'efficiency_prediction' in ml_analysis['ml_results']:\n",
    "        best_efficiency_model = max(ml_analysis['ml_results']['efficiency_prediction'].keys(),\n",
    "                                  key=lambda x: ml_analysis['ml_results']['efficiency_prediction'][x]['r2'])\n",
    "        efficiency_r2 = ml_analysis['ml_results']['efficiency_prediction'][best_efficiency_model]['r2']\n",
    "        model_performance['efficiency_prediction'] = {\n",
    "            'model_type': best_efficiency_model,\n",
    "            'r2_score': efficiency_r2,\n",
    "            'business_value': 'Optimize production planning and resource allocation'\n",
    "        }\n",
    "    \n",
    "    assessment_insights['predictive_capabilities'] = model_performance\n",
    "    \n",
    "    # Risk Assessment\n",
    "    quality_risk_rate = insights.get('high_risk_orders', {}).get('percentage', 0)\n",
    "    anomaly_rate = insights.get('anomaly_alerts', {}).get('percentage', 0)\n",
    "    \n",
    "    assessment_insights['risk_assessment'] = {\n",
    "        'quality_risk_level': 'HIGH' if quality_risk_rate > 20 else 'MEDIUM' if quality_risk_rate > 10 else 'LOW',\n",
    "        'quality_risk_percentage': quality_risk_rate,\n",
    "        'process_stability': 'UNSTABLE' if anomaly_rate > 5 else 'STABLE',\n",
    "        'anomaly_percentage': anomaly_rate,\n",
    "        'overall_risk_score': min(100, (quality_risk_rate * 2) + (anomaly_rate * 3))\n",
    "    }\n",
    "    \n",
    "    # Business Impact Estimation\n",
    "    potential_savings = 0\n",
    "    \n",
    "    if insights.get('improvement_opportunities', {}).get('total_potential'):\n",
    "        # Assume each efficiency point is worth $1000 per order\n",
    "        potential_savings += insights['improvement_opportunities']['total_potential'] * 1000\n",
    "    \n",
    "    if insights.get('high_risk_orders', {}).get('count'):\n",
    "        # Assume preventing each quality issue saves $5000\n",
    "        potential_savings += insights['high_risk_orders']['count'] * 5000\n",
    "    \n",
    "    assessment_insights['business_impact'] = {\n",
    "        'estimated_annual_savings': potential_savings,\n",
    "        'efficiency_improvement_potential': insights.get('improvement_opportunities', {}).get('avg_potential', 0),\n",
    "        'quality_improvement_potential': insights.get('high_risk_orders', {}).get('percentage', 0),\n",
    "        'roi_timeline': '6-12 months'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìã Assessment Insights Generated:\")\n",
    "    print(f\"   ‚Ä¢ Risk Level: {assessment_insights['risk_assessment']['quality_risk_level']}\")\n",
    "    print(f\"   ‚Ä¢ Potential Savings: ${assessment_insights['business_impact']['estimated_annual_savings']:,.0f}\")\n",
    "    print(f\"   ‚Ä¢ Models Developed: {assessment_insights['executive_summary']['ml_models_developed']}\")\n",
    "    \n",
    "    return {\n",
    "        'ml_analysis': ml_analysis,\n",
    "        'assessment_insights': assessment_insights,\n",
    "        'comprehensive_predictions': predictions\n",
    "    }\n",
    "\n",
    "def save_ml_results_to_folder(ml_analysis, result_folder=\"result\"):\n",
    "    \"\"\"\n",
    "    Save ML analysis results to organized folder structure\n",
    "    \"\"\"\n",
    "    print(\"\\nüíæ Saving ML Results to Folder...\")\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Create ML-specific folder\n",
    "    ml_folder = os.path.join(result_folder, f\"09_machine_learning_{timestamp}\")\n",
    "    os.makedirs(ml_folder, exist_ok=True)\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    # 1. Save model performance report\n",
    "    performance_file = os.path.join(ml_folder, f\"ml_model_performance_{timestamp}.txt\")\n",
    "    \n",
    "    with open(performance_file, 'w') as f:\n",
    "        f.write(\"MACHINE LEARNING MODEL PERFORMANCE REPORT\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        # Model Performance Summary\n",
    "        if 'ml_results' in ml_analysis:\n",
    "            ml_results = ml_analysis['ml_results']\n",
    "            \n",
    "            f.write(\"MODEL PERFORMANCE SUMMARY\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            # Quality Prediction Models\n",
    "            if 'quality_prediction' in ml_results:\n",
    "                f.write(\"\\nQUALITY ISSUE PREDICTION:\\n\")\n",
    "                for model_name, results in ml_results['quality_prediction'].items():\n",
    "                    f.write(f\"  {model_name}:\\n\")\n",
    "                    f.write(f\"    Accuracy: {results['accuracy']:.3f}\\n\")\n",
    "                    f.write(f\"    Cross-validation: {results['cv_mean']:.3f} (¬±{results['cv_std']:.3f})\\n\")\n",
    "            \n",
    "            # Efficiency Prediction Models\n",
    "            if 'efficiency_prediction' in ml_results:\n",
    "                f.write(\"\\nPRODUCTION EFFICIENCY PREDICTION:\\n\")\n",
    "                for model_name, results in ml_results['efficiency_prediction'].items():\n",
    "                    f.write(f\"  {model_name}:\\n\")\n",
    "                    f.write(f\"    R¬≤ Score: {results['r2']:.3f}\\n\")\n",
    "                    f.write(f\"    MSE: {results['mse']:.3f}\\n\")\n",
    "                    f.write(f\"    Cross-validation: {results['cv_mean']:.3f} (¬±{results['cv_std']:.3f})\\n\")\n",
    "            \n",
    "            # Clustering Results\n",
    "            if 'quality_clustering' in ml_results:\n",
    "                f.write(\"\\nQUALITY CLUSTERING ANALYSIS:\\n\")\n",
    "                clustering = ml_results['quality_clustering']\n",
    "                f.write(f\"  Optimal clusters: {clustering['optimal_k']}\\n\")\n",
    "                f.write(\"  Cluster profiles:\\n\")\n",
    "                for cluster_id, profile in clustering['cluster_profiles'].items():\n",
    "                    cluster_size = sum(clustering['clusters'] == cluster_id)\n",
    "                    f.write(f\"    Cluster {cluster_id}: {profile} ({cluster_size} orders)\\n\")\n",
    "            \n",
    "            # Anomaly Detection\n",
    "            if 'anomaly_detection' in ml_results:\n",
    "                f.write(\"\\nANOMALY DETECTION:\\n\")\n",
    "                anomaly = ml_results['anomaly_detection']\n",
    "                f.write(f\"  Total anomalies: {anomaly['total_anomalies']}\\n\")\n",
    "                f.write(f\"  Anomaly rate: {anomaly['anomaly_rate']:.2f}%\\n\")\n",
    "        \n",
    "        # Feature Importance\n",
    "        if 'feature_importance' in ml_analysis['ml_results']:\n",
    "            f.write(\"\\nTOP FEATURE IMPORTANCE:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            importance_df = ml_analysis['ml_results']['feature_importance']\n",
    "            for i, row in importance_df.head(10).iterrows():\n",
    "                f.write(f\"  {i+1:2d}. {row['feature']}: {row['mean']:.3f}\\n\")\n",
    "    \n",
    "    created_files.append(performance_file)\n",
    "    print(f\"   ‚úì Model Performance Report: {os.path.basename(performance_file)}\")\n",
    "    \n",
    "    # 2. Save predictions to Excel\n",
    "    if 'predictions' in ml_analysis and not ml_analysis['predictions'].empty:\n",
    "        predictions_file = os.path.join(ml_folder, f\"ml_predictions_{timestamp}.xlsx\")\n",
    "        \n",
    "        with pd.ExcelWriter(predictions_file, engine='openpyxl') as writer:\n",
    "            # Main predictions\n",
    "            ml_analysis['predictions'].to_excel(writer, sheet_name='ML_Predictions', index=True)\n",
    "            \n",
    "            # Feature importance (if available)\n",
    "            if 'feature_importance' in ml_analysis['ml_results']:\n",
    "                ml_analysis['ml_results']['feature_importance'].to_excel(writer, sheet_name='Feature_Importance', index=False)\n",
    "            \n",
    "            # High-risk orders\n",
    "            if 'QUALITY_ISSUE_PROBABILITY' in ml_analysis['predictions'].columns:\n",
    "                high_risk = ml_analysis['predictions'][ml_analysis['predictions']['QUALITY_ISSUE_PROBABILITY'] > 0.7]\n",
    "                if not high_risk.empty:\n",
    "                    high_risk.to_excel(writer, sheet_name='High_Risk_Orders', index=True)\n",
    "            \n",
    "            # Efficiency opportunities\n",
    "            if 'EFFICIENCY_IMPROVEMENT_POTENTIAL' in ml_analysis['predictions'].columns:\n",
    "                opportunities = ml_analysis['predictions'][ml_analysis['predictions']['EFFICIENCY_IMPROVEMENT_POTENTIAL'] > 20]\n",
    "                if not opportunities.empty:\n",
    "                    opportunities.to_excel(writer, sheet_name='Efficiency_Opportunities', index=True)\n",
    "        \n",
    "        created_files.append(predictions_file)\n",
    "        print(f\"   ‚úì ML Predictions Excel: {os.path.basename(predictions_file)}\")\n",
    "    \n",
    "    # 3. Save insights as JSON\n",
    "    if 'insights' in ml_analysis:\n",
    "        insights_file = os.path.join(ml_folder, f\"ml_insights_{timestamp}.json\")\n",
    "        \n",
    "        with open(insights_file, 'w') as f:\n",
    "            json.dump(ml_analysis['insights'], f, indent=2, default=str)\n",
    "        \n",
    "        created_files.append(insights_file)\n",
    "        print(f\"   ‚úì ML Insights JSON: {os.path.basename(insights_file)}\")\n",
    "    \n",
    "    # 4. Save assessment-specific insights (if available)\n",
    "    if 'assessment_insights' in ml_analysis:\n",
    "        assessment_file = os.path.join(ml_folder, f\"assessment_insights_{timestamp}.txt\")\n",
    "        \n",
    "        with open(assessment_file, 'w') as f:\n",
    "            f.write(\"TOLARAM ASSESSMENT - ML INSIGHTS REPORT\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            insights = ml_analysis['assessment_insights']\n",
    "            \n",
    "            # Executive Summary\n",
    "            if 'executive_summary' in insights:\n",
    "                f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                exec_sum = insights['executive_summary']\n",
    "                f.write(f\"Total Orders Analyzed: {exec_sum['total_orders_analyzed']:,}\\n\")\n",
    "                f.write(f\"ML Models Developed: {exec_sum['ml_models_developed']}\\n\")\n",
    "                f.write(f\"High-Risk Orders: {exec_sum['high_risk_orders_identified']:,}\\n\")\n",
    "                f.write(f\"Efficiency Opportunities: {exec_sum['efficiency_opportunities']:,}\\n\")\n",
    "                f.write(f\"Anomalies Detected: {exec_sum['anomalies_detected']:,}\\n\\n\")\n",
    "            \n",
    "            # Risk Assessment\n",
    "            if 'risk_assessment' in insights:\n",
    "                f.write(\"RISK ASSESSMENT\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                risk = insights['risk_assessment']\n",
    "                f.write(f\"Quality Risk Level: {risk['quality_risk_level']}\\n\")\n",
    "                f.write(f\"Quality Risk Rate: {risk['quality_risk_percentage']:.1f}%\\n\")\n",
    "                f.write(f\"Process Stability: {risk['process_stability']}\\n\")\n",
    "                f.write(f\"Overall Risk Score: {risk['overall_risk_score']:.1f}/100\\n\\n\")\n",
    "            \n",
    "            # Business Impact\n",
    "            if 'business_impact' in insights:\n",
    "                f.write(\"BUSINESS IMPACT\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                impact = insights['business_impact']\n",
    "                f.write(f\"Estimated Annual Savings: ${impact['estimated_annual_savings']:,.0f}\\n\")\n",
    "                f.write(f\"Efficiency Improvement Potential: {impact['efficiency_improvement_potential']:.1f}%\\n\")\n",
    "                f.write(f\"Quality Improvement Potential: {impact['quality_improvement_potential']:.1f}%\\n\")\n",
    "                f.write(f\"ROI Timeline: {impact['roi_timeline']}\\n\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            if 'operational_recommendations' in insights:\n",
    "                f.write(\"OPERATIONAL RECOMMENDATIONS\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                for i, rec in enumerate(insights['operational_recommendations'], 1):\n",
    "                    f.write(f\"{i}. {rec['area']} (Priority: {rec['priority']})\\n\")\n",
    "                    f.write(f\"   Recommendation: {rec['recommendation']}\\n\")\n",
    "                    f.write(f\"   Expected Impact: {rec['expected_impact']}\\n\\n\")\n",
    "        \n",
    "        created_files.append(assessment_file)\n",
    "        print(f\"   ‚úì Assessment Insights: {os.path.basename(assessment_file)}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Saved {len(created_files)} ML analysis files\")\n",
    "    return created_files\n",
    "\n",
    "# Complete workflow for assessment\n",
    "def complete_tolaram_assessment_with_ml(df_aufk=None, df_afko=None, df_afpo=None, df_aufm=None,\n",
    "                                       df_qmel=None, df_qmfe=None, df_qmur=None, df_qmih=None,\n",
    "                                       df_qpcd=None, df_qpct=None, df_qpgt=None, \n",
    "                                       df_crhd_v1=None, df_jest=None, df_plant_description=None):\n",
    "    \"\"\"\n",
    "    Complete assessment workflow including ML analysis\n",
    "    \"\"\"\n",
    "    print(\"üéØ COMPLETE TOLARAM ASSESSMENT WITH ML\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Run basic integration (assuming functions are available)\n",
    "        print(\"\\nüîÑ Step 1: Running Core Integration...\")\n",
    "        \n",
    "        # This would need to import from your main integration script\n",
    "        # For the assessment, you'll have these functions available\n",
    "        comprehensive_df, summary_stats, quality_details = create_comprehensive_sap_view(\n",
    "            df_aufk, df_afko, df_afpo, df_aufm, df_qmel, df_qmfe, df_qmur, \n",
    "            df_qmih, df_qpcd, df_qpct, df_qpgt, df_crhd_v1, df_jest, df_plant_description\n",
    "        )\n",
    "        \n",
    "        # Step 2: Run ML analysis\n",
    "        print(\"\\nü§ñ Step 2: Running ML Analysis...\")\n",
    "        ml_assessment = tolaram_ml_assessment_workflow(comprehensive_df)\n",
    "        \n",
    "        if ml_assessment is None:\n",
    "            print(\"‚ùå ML analysis failed\")\n",
    "            return None\n",
    "        \n",
    "        # Step 3: Save results\n",
    "        print(\"\\nüíæ Step 3: Saving Results...\")\n",
    "        results = run_complete_analysis_with_results_folder(\n",
    "            df_aufk, df_afko, df_afpo, df_aufm, df_qmel, df_qmfe, df_qmur, \n",
    "            df_qmih, df_qpcd, df_qpct, df_qpgt, df_crhd_v1, df_jest, df_plant_description,\n",
    "            result_folder=\"tolaram_assessment\"\n",
    "        )\n",
    "        \n",
    "        # Step 4: Save ML-specific results\n",
    "        ml_files = save_ml_results_to_folder(ml_assessment, \"tolaram_assessment\")\n",
    "        \n",
    "        print(\"\\n‚úÖ ASSESSMENT COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"üìä DELIVERABLES CREATED:\")\n",
    "        print(\"1. Executive Summary with ML insights\")\n",
    "        print(\"2. Comprehensive data analysis\")\n",
    "        print(\"3. ML model performance reports\")\n",
    "        print(\"4. Predictive analytics results\")\n",
    "        print(\"5. Risk assessment with ML validation\")\n",
    "        print(\"6. Actionable recommendations\")\n",
    "        \n",
    "        return {\n",
    "            'comprehensive_results': results,\n",
    "            'ml_assessment': ml_assessment,\n",
    "            'ml_files': ml_files,\n",
    "            'session_folder': results['session_folder'] if results else None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Assessment failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Usage instructions for the assessment\n",
    "def assessment_usage_guide():\n",
    "    \"\"\"\n",
    "    Guide for using ML analysis in the Tolaram assessment\n",
    "    \"\"\"\n",
    "    print(\"üìö TOLARAM ASSESSMENT - ML USAGE GUIDE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n1. LOAD YOUR DATA:\")\n",
    "    print(\"```python\")\n",
    "    print(\"import pandas as pd\")\n",
    "    print(\"\")\n",
    "    print(\"# Load the Excel file\")\n",
    "    print(\"excel_file = 'Project_Assessment_Data.xlsx'\")\n",
    "    print(\"sheet_names = pd.ExcelFile(excel_file).sheet_names\")\n",
    "    print(\"\")\n",
    "    print(\"# Load each sheet\")\n",
    "    print(\"tables = {}\")\n",
    "    print(\"for sheet in sheet_names:\")\n",
    "    print(\"    tables[f'df_{sheet.lower()}'] = pd.read_excel(excel_file, sheet_name=sheet)\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n2. RUN COMPLETE ANALYSIS WITH ML:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Run everything including ML\")\n",
    "    print(\"assessment_results = complete_tolaram_assessment_with_ml(\")\n",
    "    print(\"    df_aufk=tables.get('df_aufk'),\")\n",
    "    print(\"    df_afko=tables.get('df_afko'),\")\n",
    "    print(\"    df_afpo=tables.get('df_afpo'),\")\n",
    "    print(\"    df_qmel=tables.get('df_qmel'),\")\n",
    "    print(\"    # ... other tables as available\")\n",
    "    print(\")\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n3. ACCESS ML INSIGHTS:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Get ML predictions\")\n",
    "    print(\"predictions = assessment_results['ml_assessment']['comprehensive_predictions']\")\n",
    "    print(\"\")\n",
    "    print(\"# Get business insights\")\n",
    "    print(\"insights = assessment_results['ml_assessment']['assessment_insights']\")\n",
    "    print(\"\")\n",
    "    print(\"# Print key findings\")\n",
    "    print(\"print('Risk Level:', insights['risk_assessment']['quality_risk_level'])\")\n",
    "    print(\"print('Potential Savings:', insights['business_impact']['estimated_annual_savings'])\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n4. ASSESSMENT REPORT SECTIONS:\")\n",
    "    print(\"‚Ä¢ Executive Summary: ML-driven KPIs and risk assessment\")\n",
    "    print(\"‚Ä¢ Predictive Models: Quality issue and efficiency prediction\")\n",
    "    print(\"‚Ä¢ Pattern Analysis: Clustering and anomaly detection\")\n",
    "    print(\"‚Ä¢ Business Impact: Quantified savings and ROI\")\n",
    "    print(\"‚Ä¢ Recommendations: ML-backed improvement actions\")\n",
    "    \n",
    "    print(\"\\n5. KEY ML CAPABILITIES:\")\n",
    "    print(\"‚Ä¢ Quality Issue Prediction (Classification)\")\n",
    "    print(\"‚Ä¢ Production Efficiency Prediction (Regression)\")\n",
    "    print(\"‚Ä¢ Quality Pattern Clustering (Unsupervised)\")\n",
    "    print(\"‚Ä¢ Anomaly Detection (Outlier Analysis)\")\n",
    "    print(\"‚Ä¢ Feature Importance Analysis\")\n",
    "    print(\"‚Ä¢ Risk Scoring and Prioritization\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    assessment_usage_guide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb00485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SAPBottleneckDetector:\n",
    "    \"\"\"\n",
    "    Advanced Bottleneck Detection and Downtime Analysis for SAP Manufacturing Data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, comprehensive_df):\n",
    "        self.comprehensive_df = comprehensive_df\n",
    "        self.bottleneck_analysis = {}\n",
    "        self.downtime_analysis = {}\n",
    "        self.throughput_analysis = {}\n",
    "        self.capacity_analysis = {}\n",
    "        \n",
    "    def detect_schedule_bottlenecks(self):\n",
    "        \"\"\"\n",
    "        Detect bottlenecks based on scheduling and timing data\n",
    "        \"\"\"\n",
    "        print(\"üïê Analyzing Schedule-Based Bottlenecks...\")\n",
    "        \n",
    "        df = self.comprehensive_df.copy()\n",
    "        bottlenecks = {}\n",
    "        \n",
    "        # Analyze planned vs actual dates\n",
    "        schedule_cols = {\n",
    "            'planned_start': ['GSTRP', 'PLANNED_START_DATE'],\n",
    "            'planned_end': ['GLTRP', 'PLANNED_END_DATE'], \n",
    "            'actual_start': ['GSTRS', 'ACTUAL_START_DATE'],\n",
    "            'actual_end': ['GLTRS', 'ACTUAL_END_DATE']\n",
    "        }\n",
    "        \n",
    "        date_analysis = {}\n",
    "        \n",
    "        for date_type, possible_cols in schedule_cols.items():\n",
    "            for col in possible_cols:\n",
    "                if col in df.columns:\n",
    "                    try:\n",
    "                        df[f'{date_type}_parsed'] = pd.to_datetime(df[col], errors='coerce')\n",
    "                        date_analysis[date_type] = f'{date_type}_parsed'\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        if len(date_analysis) >= 2:\n",
    "            # Calculate delays and lead times\n",
    "            if 'planned_start' in date_analysis and 'actual_start' in date_analysis:\n",
    "                df['START_DELAY_DAYS'] = (df[date_analysis['actual_start']] - df[date_analysis['planned_start']]).dt.days\n",
    "                \n",
    "            if 'planned_end' in date_analysis and 'actual_end' in date_analysis:\n",
    "                df['END_DELAY_DAYS'] = (df[date_analysis['actual_end']] - df[date_analysis['planned_end']]).dt.days\n",
    "                \n",
    "            if 'planned_start' in date_analysis and 'planned_end' in date_analysis:\n",
    "                df['PLANNED_CYCLE_TIME'] = (df[date_analysis['planned_end']] - df[date_analysis['planned_start']]).dt.days\n",
    "                \n",
    "            if 'actual_start' in date_analysis and 'actual_end' in date_analysis:\n",
    "                df['ACTUAL_CYCLE_TIME'] = (df[date_analysis['actual_end']] - df[date_analysis['actual_start']]).dt.days\n",
    "            \n",
    "            # Identify bottleneck patterns\n",
    "            bottlenecks['schedule_delays'] = self._analyze_schedule_delays(df)\n",
    "            \n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Insufficient date columns for schedule analysis\")\n",
    "            \n",
    "        return bottlenecks\n",
    "    \n",
    "    def _analyze_schedule_delays(self, df):\n",
    "        \"\"\"\n",
    "        Analyze schedule delays to identify bottlenecks\n",
    "        \"\"\"\n",
    "        delay_analysis = {}\n",
    "        \n",
    "        # Analyze start delays\n",
    "        if 'START_DELAY_DAYS' in df.columns:\n",
    "            start_delays = df['START_DELAY_DAYS'].dropna()\n",
    "            \n",
    "            delay_analysis['start_delays'] = {\n",
    "                'avg_delay': start_delays.mean(),\n",
    "                'median_delay': start_delays.median(),\n",
    "                'max_delay': start_delays.max(),\n",
    "                'delayed_orders_count': (start_delays > 0).sum(),\n",
    "                'delayed_orders_pct': (start_delays > 0).mean() * 100,\n",
    "                'severe_delays': (start_delays > 7).sum()  # More than 1 week\n",
    "            }\n",
    "        \n",
    "        # Analyze end delays\n",
    "        if 'END_DELAY_DAYS' in df.columns:\n",
    "            end_delays = df['END_DELAY_DAYS'].dropna()\n",
    "            \n",
    "            delay_analysis['end_delays'] = {\n",
    "                'avg_delay': end_delays.mean(),\n",
    "                'median_delay': end_delays.median(),\n",
    "                'max_delay': end_delays.max(),\n",
    "                'delayed_orders_count': (end_delays > 0).sum(),\n",
    "                'delayed_orders_pct': (end_delays > 0).mean() * 100,\n",
    "                'severe_delays': (end_delays > 7).sum()\n",
    "            }\n",
    "        \n",
    "        # Analyze cycle time variations\n",
    "        if 'PLANNED_CYCLE_TIME' in df.columns and 'ACTUAL_CYCLE_TIME' in df.columns:\n",
    "            cycle_variance = df['ACTUAL_CYCLE_TIME'] - df['PLANNED_CYCLE_TIME']\n",
    "            \n",
    "            delay_analysis['cycle_time_variance'] = {\n",
    "                'avg_variance': cycle_variance.mean(),\n",
    "                'std_variance': cycle_variance.std(),\n",
    "                'orders_over_planned': (cycle_variance > 0).sum(),\n",
    "                'avg_overrun': cycle_variance[cycle_variance > 0].mean() if (cycle_variance > 0).any() else 0\n",
    "            }\n",
    "        \n",
    "        return delay_analysis\n",
    "    \n",
    "    def detect_plant_bottlenecks(self):\n",
    "        \"\"\"\n",
    "        Detect bottlenecks by analyzing plant performance\n",
    "        \"\"\"\n",
    "        print(\"üè≠ Analyzing Plant-Level Bottlenecks...\")\n",
    "        \n",
    "        df = self.comprehensive_df.copy()\n",
    "        plant_bottlenecks = {}\n",
    "        \n",
    "        # Find plant column\n",
    "        plant_col = None\n",
    "        for col in ['Plant_Name', 'Plant_Code', 'WERKS', 'PWERK']:\n",
    "            if col in df.columns and df[col].notna().sum() > 0:\n",
    "                plant_col = col\n",
    "                break\n",
    "        \n",
    "        if not plant_col:\n",
    "            print(\"   ‚ö†Ô∏è  No plant column found\")\n",
    "            return {}\n",
    "        \n",
    "        # Plant performance metrics\n",
    "        plant_metrics = df.groupby(plant_col).agg({\n",
    "            'AUFNR': 'count',\n",
    "            'QUALITY_NOTIF_COUNT': ['sum', 'mean'],\n",
    "            'DEFECT_COUNT': ['sum', 'mean'],\n",
    "            'ORDER_ITEM_COUNT': ['sum', 'mean'],\n",
    "            'GOODS_MOVEMENT_COUNT': ['sum', 'mean'],\n",
    "            'QUALITY_SCORE': 'mean',\n",
    "            'PRODUCTION_EFFICIENCY': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        # Flatten column names\n",
    "        plant_metrics.columns = ['_'.join(col).strip() for col in plant_metrics.columns]\n",
    "        \n",
    "        # Calculate bottleneck indicators\n",
    "        plant_metrics['THROUGHPUT_SCORE'] = plant_metrics['AUFNR_count'] / plant_metrics['AUFNR_count'].max()\n",
    "        plant_metrics['QUALITY_BURDEN'] = (\n",
    "            plant_metrics['QUALITY_NOTIF_COUNT_sum'] + plant_metrics['DEFECT_COUNT_sum']\n",
    "        ) / plant_metrics['AUFNR_count']\n",
    "        \n",
    "        # Identify bottleneck plants\n",
    "        # Low throughput + high quality issues = bottleneck\n",
    "        plant_metrics['BOTTLENECK_SCORE'] = (\n",
    "            (1 - plant_metrics['THROUGHPUT_SCORE']) * 0.4 +\n",
    "            (plant_metrics['QUALITY_BURDEN'] / plant_metrics['QUALITY_BURDEN'].max()) * 0.3 +\n",
    "            (1 - plant_metrics.get('PRODUCTION_EFFICIENCY_mean', pd.Series([0])) / 100) * 0.3\n",
    "        )\n",
    "        \n",
    "        # Rank plants by bottleneck risk\n",
    "        bottleneck_ranking = plant_metrics.sort_values('BOTTLENECK_SCORE', ascending=False)\n",
    "        \n",
    "        plant_bottlenecks = {\n",
    "            'plant_metrics': plant_metrics,\n",
    "            'bottleneck_ranking': bottleneck_ranking,\n",
    "            'top_bottleneck_plants': bottleneck_ranking.head(3).index.tolist(),\n",
    "            'bottleneck_indicators': {\n",
    "                'low_throughput_plants': plant_metrics[plant_metrics['THROUGHPUT_SCORE'] < 0.7].index.tolist(),\n",
    "                'high_quality_burden_plants': plant_metrics[\n",
    "                    plant_metrics['QUALITY_BURDEN'] > plant_metrics['QUALITY_BURDEN'].quantile(0.8)\n",
    "                ].index.tolist(),\n",
    "                'low_efficiency_plants': plant_metrics[\n",
    "                    plant_metrics.get('PRODUCTION_EFFICIENCY_mean', pd.Series([100])) < 85\n",
    "                ].index.tolist()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úì Analyzed {len(plant_metrics)} plants\")\n",
    "        print(f\"   üö® Top bottleneck plants: {', '.join(plant_bottlenecks['top_bottleneck_plants'][:3])}\")\n",
    "        \n",
    "        return plant_bottlenecks\n",
    "    \n",
    "    def detect_material_bottlenecks(self):\n",
    "        \"\"\"\n",
    "        Detect bottlenecks related to materials and supply chain\n",
    "        \"\"\"\n",
    "        print(\"üì¶ Analyzing Material-Related Bottlenecks...\")\n",
    "        \n",
    "        df = self.comprehensive_df.copy()\n",
    "        material_bottlenecks = {}\n",
    "        \n",
    "        if 'ORDER_MATERIALS' not in df.columns:\n",
    "            print(\"   ‚ö†Ô∏è  No material information available\")\n",
    "            return {}\n",
    "        \n",
    "        # Extract individual materials from comma-separated lists\n",
    "        all_materials = []\n",
    "        material_orders = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if pd.notna(row.get('ORDER_MATERIALS')):\n",
    "                materials = str(row['ORDER_MATERIALS']).split(', ')\n",
    "                for material in materials[:3]:  # Top 3 materials per order\n",
    "                    all_materials.append(material.strip())\n",
    "                    material_orders.append({\n",
    "                        'MATERIAL': material.strip(),\n",
    "                        'ORDER_INDEX': idx,\n",
    "                        'QUALITY_NOTIF_COUNT': row.get('QUALITY_NOTIF_COUNT', 0),\n",
    "                        'DEFECT_COUNT': row.get('DEFECT_COUNT', 0),\n",
    "                        'PRODUCTION_EFFICIENCY': row.get('PRODUCTION_EFFICIENCY', 100),\n",
    "                        'GOODS_MOVEMENT_COUNT': row.get('GOODS_MOVEMENT_COUNT', 0)\n",
    "                    })\n",
    "        \n",
    "        if not material_orders:\n",
    "            print(\"   ‚ö†Ô∏è  No material data to analyze\")\n",
    "            return {}\n",
    "        \n",
    "        material_df = pd.DataFrame(material_orders)\n",
    "        \n",
    "        # Analyze material performance\n",
    "        material_analysis = material_df.groupby('MATERIAL').agg({\n",
    "            'ORDER_INDEX': 'count',\n",
    "            'QUALITY_NOTIF_COUNT': ['sum', 'mean'],\n",
    "            'DEFECT_COUNT': ['sum', 'mean'],\n",
    "            'PRODUCTION_EFFICIENCY': 'mean',\n",
    "            'GOODS_MOVEMENT_COUNT': ['sum', 'mean']\n",
    "        }).round(3)\n",
    "        \n",
    "        # Flatten columns\n",
    "        material_analysis.columns = ['_'.join(col).strip() for col in material_analysis.columns]\n",
    "        \n",
    "        # Calculate material bottleneck scores\n",
    "        material_analysis['USAGE_FREQUENCY'] = material_analysis['ORDER_INDEX_count']\n",
    "        material_analysis['QUALITY_ISSUES_PER_ORDER'] = (\n",
    "            material_analysis['QUALITY_NOTIF_COUNT_sum'] + material_analysis['DEFECT_COUNT_sum']\n",
    "        ) / material_analysis['ORDER_INDEX_count']\n",
    "        \n",
    "        material_analysis['MATERIAL_BOTTLENECK_SCORE'] = (\n",
    "            (material_analysis['QUALITY_ISSUES_PER_ORDER'] / material_analysis['QUALITY_ISSUES_PER_ORDER'].max()) * 0.5 +\n",
    "            (1 - material_analysis['PRODUCTION_EFFICIENCY_mean'] / 100) * 0.3 +\n",
    "            (material_analysis['USAGE_FREQUENCY'] / material_analysis['USAGE_FREQUENCY'].max()) * 0.2\n",
    "        )\n",
    "        \n",
    "        # Identify problematic materials\n",
    "        high_risk_materials = material_analysis[\n",
    "            material_analysis['MATERIAL_BOTTLENECK_SCORE'] > material_analysis['MATERIAL_BOTTLENECK_SCORE'].quantile(0.8)\n",
    "        ].sort_values('MATERIAL_BOTTLENECK_SCORE', ascending=False)\n",
    "        \n",
    "        material_bottlenecks = {\n",
    "            'material_analysis': material_analysis,\n",
    "            'high_risk_materials': high_risk_materials,\n",
    "            'top_problematic_materials': high_risk_materials.head(5).index.tolist(),\n",
    "            'material_quality_issues': material_analysis.sort_values('QUALITY_ISSUES_PER_ORDER', ascending=False).head(10),\n",
    "            'most_used_materials': material_analysis.sort_values('USAGE_FREQUENCY', ascending=False).head(10)\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úì Analyzed {len(material_analysis)} unique materials\")\n",
    "        print(f\"   üö® High-risk materials: {len(high_risk_materials)}\")\n",
    "        \n",
    "        return material_bottlenecks\n",
    "    \n",
    "    def detect_work_center_bottlenecks(self):\n",
    "        \"\"\"\n",
    "        Detect bottlenecks at work center level\n",
    "        \"\"\"\n",
    "        print(\"‚öôÔ∏è Analyzing Work Center Bottlenecks...\")\n",
    "        \n",
    "        df = self.comprehensive_df.copy()\n",
    "        wc_bottlenecks = {}\n",
    "        \n",
    "        # Find work center related columns\n",
    "        wc_cols = [col for col in df.columns if any(keyword in col.upper() for keyword in ['ARBPL', 'WORK_CENTER', 'WC'])]\n",
    "        \n",
    "        if not wc_cols:\n",
    "            print(\"   ‚ö†Ô∏è  No work center information available\")\n",
    "            return {}\n",
    "        \n",
    "        wc_col = wc_cols[0]  # Use first available work center column\n",
    "        \n",
    "        if df[wc_col].notna().sum() == 0:\n",
    "            print(\"   ‚ö†Ô∏è  Work center column is empty\")\n",
    "            return {}\n",
    "        \n",
    "        # Analyze work center performance\n",
    "        wc_analysis = df.groupby(wc_col).agg({\n",
    "            'AUFNR': 'count',\n",
    "            'QUALITY_NOTIF_COUNT': ['sum', 'mean'],\n",
    "            'DEFECT_COUNT': ['sum', 'mean'], \n",
    "            'PRODUCTION_EFFICIENCY': 'mean',\n",
    "            'ORDER_ITEM_COUNT': ['sum', 'mean'],\n",
    "            'GOODS_MOVEMENT_COUNT': ['sum', 'mean']\n",
    "        }).round(3)\n",
    "        \n",
    "        # Flatten columns\n",
    "        wc_analysis.columns = ['_'.join(col).strip() for col in wc_analysis.columns]\n",
    "        \n",
    "        # Calculate work center utilization and bottleneck indicators\n",
    "        wc_analysis['THROUGHPUT'] = wc_analysis['AUFNR_count']\n",
    "        wc_analysis['UTILIZATION_SCORE'] = wc_analysis['THROUGHPUT'] / wc_analysis['THROUGHPUT'].max()\n",
    "        wc_analysis['QUALITY_BURDEN'] = (\n",
    "            wc_analysis['QUALITY_NOTIF_COUNT_sum'] + wc_analysis['DEFECT_COUNT_sum']\n",
    "        ) / wc_analysis['AUFNR_count']\n",
    "        \n",
    "        # Work center bottleneck score\n",
    "        wc_analysis['WC_BOTTLENECK_SCORE'] = (\n",
    "            (1 - wc_analysis['UTILIZATION_SCORE']) * 0.3 +\n",
    "            (wc_analysis['QUALITY_BURDEN'] / wc_analysis['QUALITY_BURDEN'].max()) * 0.4 +\n",
    "            (1 - wc_analysis.get('PRODUCTION_EFFICIENCY_mean', pd.Series([100])) / 100) * 0.3\n",
    "        )\n",
    "        \n",
    "        # Identify bottleneck work centers\n",
    "        bottleneck_wcs = wc_analysis.sort_values('WC_BOTTLENECK_SCORE', ascending=False)\n",
    "        \n",
    "        wc_bottlenecks = {\n",
    "            'work_center_analysis': wc_analysis,\n",
    "            'bottleneck_ranking': bottleneck_wcs,\n",
    "            'top_bottleneck_work_centers': bottleneck_wcs.head(5).index.tolist(),\n",
    "            'underutilized_work_centers': wc_analysis[wc_analysis['UTILIZATION_SCORE'] < 0.5].index.tolist(),\n",
    "            'high_quality_burden_wcs': wc_analysis[\n",
    "                wc_analysis['QUALITY_BURDEN'] > wc_analysis['QUALITY_BURDEN'].quantile(0.8)\n",
    "            ].index.tolist()\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úì Analyzed {len(wc_analysis)} work centers\")\n",
    "        print(f\"   üö® Top bottleneck work centers: {', '.join(bottleneck_wcs.head(3).index.astype(str).tolist())}\")\n",
    "        \n",
    "        return wc_bottlenecks\n",
    "    \n",
    "    def detect_downtime_patterns(self):\n",
    "        \"\"\"\n",
    "        Detect downtime patterns and maintenance-related bottlenecks\n",
    "        \"\"\"\n",
    "        print(\"‚è∞ Analyzing Downtime Patterns...\")\n",
    "        \n",
    "        df = self.comprehensive_df.copy()\n",
    "        downtime_analysis = {}\n",
    "        \n",
    "        # Look for maintenance-related indicators\n",
    "        maintenance_indicators = []\n",
    "        \n",
    "        # Check for maintenance notifications\n",
    "        if 'HAS_MAINTENANCE' in df.columns:\n",
    "            maintenance_orders = df[df['HAS_MAINTENANCE'] == True]\n",
    "            maintenance_indicators.append(('maintenance_notifications', len(maintenance_orders)))\n",
    "        \n",
    "        # Check for quality issues that might indicate downtime\n",
    "        if 'QUALITY_NOTIF_COUNT' in df.columns:\n",
    "            high_quality_issues = df[df['QUALITY_NOTIF_COUNT'] > df['QUALITY_NOTIF_COUNT'].quantile(0.9)]\n",
    "            maintenance_indicators.append(('high_quality_issues', len(high_quality_issues)))\n",
    "        \n",
    "        # Analyze production gaps (if we have date information)\n",
    "        if 'ERDAT' in df.columns:\n",
    "            try:\n",
    "                df['ERDAT_parsed'] = pd.to_datetime(df['ERDAT'], errors='coerce')\n",
    "                df_sorted = df.sort_values('ERDAT_parsed')\n",
    "                \n",
    "                # Calculate gaps between orders\n",
    "                df_sorted['TIME_GAP'] = df_sorted['ERDAT_parsed'].diff().dt.days\n",
    "                \n",
    "                # Identify unusual gaps (potential downtime)\n",
    "                gap_threshold = df_sorted['TIME_GAP'].quantile(0.95)  # 95th percentile\n",
    "                unusual_gaps = df_sorted[df_sorted['TIME_GAP'] > gap_threshold]\n",
    "                \n",
    "                downtime_analysis['production_gaps'] = {\n",
    "                    'total_unusual_gaps': len(unusual_gaps),\n",
    "                    'avg_gap_days': unusual_gaps['TIME_GAP'].mean() if len(unusual_gaps) > 0 else 0,\n",
    "                    'max_gap_days': unusual_gaps['TIME_GAP'].max() if len(unusual_gaps) > 0 else 0,\n",
    "                    'gap_threshold': gap_threshold\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Date analysis failed: {e}\")\n",
    "        \n",
    "        # Analyze efficiency drops (potential downtime indicators)\n",
    "        if 'PRODUCTION_EFFICIENCY' in df.columns:\n",
    "            low_efficiency = df[df['PRODUCTION_EFFICIENCY'] < 70]  # Below 70% efficiency\n",
    "            \n",
    "            downtime_analysis['efficiency_drops'] = {\n",
    "                'low_efficiency_orders': len(low_efficiency),\n",
    "                'low_efficiency_percentage': len(low_efficiency) / len(df) * 100,\n",
    "                'avg_efficiency_drop': (100 - low_efficiency['PRODUCTION_EFFICIENCY'].mean()) if len(low_efficiency) > 0 else 0\n",
    "            }\n",
    "        \n",
    "        # Estimate downtime impact\n",
    "        if 'PLANNED_CYCLE_TIME' in df.columns and 'ACTUAL_CYCLE_TIME' in df.columns:\n",
    "            cycle_overruns = df[df['ACTUAL_CYCLE_TIME'] > df['PLANNED_CYCLE_TIME']]\n",
    "            total_overrun_days = (cycle_overruns['ACTUAL_CYCLE_TIME'] - cycle_overruns['PLANNED_CYCLE_TIME']).sum()\n",
    "            \n",
    "            downtime_analysis['cycle_time_analysis'] = {\n",
    "                'orders_with_overruns': len(cycle_overruns),\n",
    "                'total_overrun_days': total_overrun_days,\n",
    "                'avg_overrun_per_order': total_overrun_days / len(cycle_overruns) if len(cycle_overruns) > 0 else 0\n",
    "            }\n",
    "        \n",
    "        # Overall downtime risk assessment\n",
    "        risk_factors = []\n",
    "        \n",
    "        if downtime_analysis.get('production_gaps', {}).get('total_unusual_gaps', 0) > 0:\n",
    "            risk_factors.append('production_gaps')\n",
    "        \n",
    "        if downtime_analysis.get('efficiency_drops', {}).get('low_efficiency_percentage', 0) > 10:\n",
    "            risk_factors.append('efficiency_issues')\n",
    "        \n",
    "        if len([ind for ind_name, count in maintenance_indicators if count > 0]) > 0:\n",
    "            risk_factors.append('maintenance_issues')\n",
    "        \n",
    "        downtime_analysis['risk_assessment'] = {\n",
    "            'risk_factors': risk_factors,\n",
    "            'risk_level': 'HIGH' if len(risk_factors) >= 2 else 'MEDIUM' if len(risk_factors) == 1 else 'LOW',\n",
    "            'maintenance_indicators': dict(maintenance_indicators)\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úì Downtime risk level: {downtime_analysis.get('risk_assessment', {}).get('risk_level', 'UNKNOWN')}\")\n",
    "        print(f\"   üìä Risk factors identified: {len(risk_factors)}\")\n",
    "        \n",
    "        return downtime_analysis\n",
    "    \n",
    "    def analyze_throughput_bottlenecks(self):\n",
    "        \"\"\"\n",
    "        Analyze throughput bottlenecks across different dimensions\n",
    "        \"\"\"\n",
    "        print(\"üìà Analyzing Throughput Bottlenecks...\")\n",
    "        \n",
    "        df = self.comprehensive_df.copy()\n",
    "        throughput_analysis = {}\n",
    "        \n",
    "        # Time-based throughput analysis\n",
    "        if 'ERDAT' in df.columns:\n",
    "            try:\n",
    "                df['ERDAT_parsed'] = pd.to_datetime(df['ERDAT'], errors='coerce')\n",
    "                df['YEAR_MONTH'] = df['ERDAT_parsed'].dt.to_period('M')\n",
    "                \n",
    "                monthly_throughput = df.groupby('YEAR_MONTH').agg({\n",
    "                    'AUFNR': 'count',\n",
    "                    'ORDER_ITEM_COUNT': 'sum',\n",
    "                    'GOODS_MOVEMENT_COUNT': 'sum'\n",
    "                })\n",
    "                \n",
    "                # Identify throughput bottlenecks (low-throughput periods)\n",
    "                throughput_threshold = monthly_throughput['AUFNR'].quantile(0.25)  # Bottom 25%\n",
    "                low_throughput_periods = monthly_throughput[monthly_throughput['AUFNR'] < throughput_threshold]\n",
    "                \n",
    "                throughput_analysis['temporal_analysis'] = {\n",
    "                    'monthly_throughput': monthly_throughput,\n",
    "                    'low_throughput_periods': low_throughput_periods,\n",
    "                    'throughput_variability': monthly_throughput['AUFNR'].std() / monthly_throughput['AUFNR'].mean()\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Temporal analysis failed: {e}\")\n",
    "        \n",
    "        # Order type throughput analysis\n",
    "        if 'AUART' in df.columns:\n",
    "            order_type_throughput = df.groupby('AUART').agg({\n",
    "                'AUFNR': 'count',\n",
    "                'PRODUCTION_EFFICIENCY': 'mean',\n",
    "                'QUALITY_NOTIF_COUNT': 'mean'\n",
    "            })\n",
    "            \n",
    "            # Identify slow order types\n",
    "            slow_order_types = order_type_throughput[\n",
    "                order_type_throughput['PRODUCTION_EFFICIENCY'] < order_type_throughput['PRODUCTION_EFFICIENCY'].median()\n",
    "            ]\n",
    "            \n",
    "            throughput_analysis['order_type_analysis'] = {\n",
    "                'order_type_throughput': order_type_throughput,\n",
    "                'slow_order_types': slow_order_types.index.tolist(),\n",
    "                'fastest_order_type': order_type_throughput['PRODUCTION_EFFICIENCY'].idxmax(),\n",
    "                'slowest_order_type': order_type_throughput['PRODUCTION_EFFICIENCY'].idxmin()\n",
    "            }\n",
    "        \n",
    "        # Plant throughput analysis\n",
    "        plant_col = None\n",
    "        for col in ['Plant_Name', 'Plant_Code', 'WERKS']:\n",
    "            if col in df.columns and df[col].notna().sum() > 0:\n",
    "                plant_col = col\n",
    "                break\n",
    "        \n",
    "        if plant_col:\n",
    "            plant_throughput = df.groupby(plant_col).agg({\n",
    "                'AUFNR': 'count',\n",
    "                'ORDER_ITEM_COUNT': ['sum', 'mean'],\n",
    "                'PRODUCTION_EFFICIENCY': 'mean',\n",
    "                'GOODS_MOVEMENT_COUNT': ['sum', 'mean']\n",
    "            })\n",
    "            \n",
    "            # Flatten columns\n",
    "            plant_throughput.columns = ['_'.join(col).strip() for col in plant_throughput.columns]\n",
    "            \n",
    "            # Calculate throughput scores\n",
    "            plant_throughput['THROUGHPUT_SCORE'] = (\n",
    "                plant_throughput['AUFNR_count'] / plant_throughput['AUFNR_count'].max() * 0.4 +\n",
    "                plant_throughput.get('PRODUCTION_EFFICIENCY_mean', pd.Series([0])) / 100 * 0.4 +\n",
    "                plant_throughput['GOODS_MOVEMENT_COUNT_sum'] / plant_throughput['GOODS_MOVEMENT_COUNT_sum'].max() * 0.2\n",
    "            )\n",
    "            \n",
    "            bottleneck_plants = plant_throughput[plant_throughput['THROUGHPUT_SCORE'] < 0.6]\n",
    "            \n",
    "            throughput_analysis['plant_analysis'] = {\n",
    "                'plant_throughput': plant_throughput,\n",
    "                'bottleneck_plants': bottleneck_plants.index.tolist(),\n",
    "                'best_performing_plant': plant_throughput['THROUGHPUT_SCORE'].idxmax(),\n",
    "                'worst_performing_plant': plant_throughput['THROUGHPUT_SCORE'].idxmin()\n",
    "            }\n",
    "        \n",
    "        print(f\"   ‚úì Throughput analysis completed\")\n",
    "        \n",
    "        return throughput_analysis\n",
    "    \n",
    "    def generate_bottleneck_recommendations(self):\n",
    "        \"\"\"\n",
    "        Generate specific recommendations based on bottleneck analysis\n",
    "        \"\"\"\n",
    "        print(\"üí° Generating Bottleneck Recommendations...\")\n",
    "        \n",
    "        recommendations = {\n",
    "            'immediate_actions': [],\n",
    "            'process_improvements': [],\n",
    "            'capacity_optimizations': [],\n",
    "            'maintenance_actions': [],\n",
    "            'supply_chain_actions': []\n",
    "        }\n",
    "        \n",
    "        # Plant bottleneck recommendations\n",
    "        if 'plant_bottlenecks' in self.bottleneck_analysis:\n",
    "            plant_data = self.bottleneck_analysis['plant_bottlenecks']\n",
    "            \n",
    "            if plant_data.get('top_bottleneck_plants'):\n",
    "                top_bottleneck = plant_data['top_bottleneck_plants'][0]\n",
    "                recommendations['immediate_actions'].append({\n",
    "                    'action': f'Conduct detailed capacity analysis at {top_bottleneck}',\n",
    "                    'priority': 'HIGH',\n",
    "                    'timeline': '1 week',\n",
    "                    'category': 'Plant Optimization'\n",
    "                })\n",
    "            \n",
    "            if plant_data.get('bottleneck_indicators', {}).get('low_efficiency_plants'):\n",
    "                recommendations['process_improvements'].append({\n",
    "                    'action': f'Implement lean manufacturing at plants: {\", \".join(plant_data[\"bottleneck_indicators\"][\"low_efficiency_plants\"][:3])}',\n",
    "                    'priority': 'MEDIUM',\n",
    "                    'timeline': '2-3 months',\n",
    "                    'category': 'Efficiency Improvement'\n",
    "                })\n",
    "        \n",
    "        # Material bottleneck recommendations\n",
    "        if 'material_bottlenecks' in self.bottleneck_analysis:\n",
    "            material_data = self.bottleneck_analysis['material_bottlenecks']\n",
    "            \n",
    "            if material_data.get('top_problematic_materials'):\n",
    "                problematic_materials = material_data['top_problematic_materials'][:3]\n",
    "                recommendations['supply_chain_actions'].append({\n",
    "                    'action': f'Review supplier quality agreements for materials: {\", \".join(problematic_materials)}',\n",
    "                    'priority': 'HIGH',\n",
    "                    'timeline': '2 weeks',\n",
    "                    'category': 'Supply Chain'\n",
    "                })\n",
    "        \n",
    "        # Work center bottleneck recommendations\n",
    "        if 'work_center_bottlenecks' in self.bottleneck_analysis:\n",
    "            wc_data = self.bottleneck_analysis['work_center_bottlenecks']\n",
    "            \n",
    "            if wc_data.get('top_bottleneck_work_centers'):\n",
    "                bottleneck_wcs = wc_data['top_bottleneck_work_centers'][:2]\n",
    "                recommendations['capacity_optimizations'].append({\n",
    "                    'action': f'Increase capacity or redistribute workload for work centers: {\", \".join(map(str, bottleneck_wcs))}',\n",
    "                    'priority': 'HIGH',\n",
    "                    'timeline': '1 month',\n",
    "                    'category': 'Capacity Management'\n",
    "                })\n",
    "        \n",
    "        # Downtime recommendations\n",
    "        if 'downtime_analysis' in self.bottleneck_analysis:\n",
    "            downtime_data = self.bottleneck_analysis['downtime_analysis']\n",
    "            \n",
    "            if downtime_data.get('risk_assessment', {}).get('risk_level') == 'HIGH':\n",
    "                recommendations['maintenance_actions'].append({\n",
    "                    'action': 'Implement predictive maintenance program to reduce unplanned downtime',\n",
    "                    'priority': 'HIGH',\n",
    "                    'timeline': '3 months',\n",
    "                    'category': 'Maintenance'\n",
    "                })\n",
    "            \n",
    "            if downtime_data.get('efficiency_drops', {}).get('low_efficiency_percentage', 0) > 10:\n",
    "                recommendations['process_improvements'].append({\n",
    "                    'action': f'Investigate root causes of efficiency drops affecting {downtime_data[\"efficiency_drops\"][\"low_efficiency_percentage\"]:.1f}% of orders',\n",
    "                    'priority': 'MEDIUM',\n",
    "                    'timeline': '1 month',\n",
    "                    'category': 'Process Analysis'\n",
    "                })\n",
    "        \n",
    "        # Throughput recommendations\n",
    "        if 'throughput_analysis' in self.bottleneck_analysis:\n",
    "            throughput_data = self.bottleneck_analysis['throughput_analysis']\n",
    "            \n",
    "            if throughput_data.get('plant_analysis', {}).get('bottleneck_plants'):\n",
    "                recommendations['capacity_optimizations'].append({\n",
    "                    'action': f'Balance workload distribution across plants, reduce load on: {\", \".join(throughput_data[\"plant_analysis\"][\"bottleneck_plants\"][:2])}',\n",
    "                    'priority': 'MEDIUM',\n",
    "                    'timeline': '2 months',\n",
    "                    'category': 'Load Balancing'\n",
    "                })\n",
    "        \n",
    "        print(f\"   ‚úì Generated {sum(len(recs) for recs in recommendations.values())} recommendations\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def run_complete_bottleneck_analysis(self):\n",
    "        \"\"\"\n",
    "        Run complete bottleneck and downtime analysis\n",
    "        \"\"\"\n",
    "        print(\"üîç RUNNING COMPLETE BOTTLENECK ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Run all bottleneck detection methods\n",
    "        self.bottleneck_analysis['schedule_bottlenecks'] = self.detect_schedule_bottlenecks()\n",
    "        self.bottleneck_analysis['plant_bottlenecks'] = self.detect_plant_bottlenecks()\n",
    "        self.bottleneck_analysis['material_bottlenecks'] = self.detect_material_bottlenecks()\n",
    "        self.bottleneck_analysis['work_center_bottlenecks'] = self.detect_work_center_bottlenecks()\n",
    "        self.bottleneck_analysis['downtime_analysis'] = self.detect_downtime_patterns()\n",
    "        self.bottleneck_analysis['throughput_analysis'] = self.analyze_throughput_bottlenecks()\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = self.generate_bottleneck_recommendations()\n",
    "        self.bottleneck_analysis['recommendations'] = recommendations\n",
    "        \n",
    "        # Create summary\n",
    "        summary = self.create_bottleneck_summary()\n",
    "        self.bottleneck_analysis['summary'] = summary\n",
    "        \n",
    "        print(\"\\n‚úÖ Bottleneck Analysis Complete!\")\n",
    "        print(f\"   üéØ Total bottlenecks identified: {summary['total_bottlenecks']}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Critical issues: {summary['critical_issues']}\")\n",
    "        print(f\"   üí° Recommendations generated: {summary['total_recommendations']}\")\n",
    "        \n",
    "        return self.bottleneck_analysis\n",
    "    \n",
    "    def create_bottleneck_summary(self):\n",
    "        \"\"\"\n",
    "        Create executive summary of bottleneck analysis\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'total_bottlenecks': 0,\n",
    "            'critical_issues': 0,\n",
    "            'total_recommendations': 0,\n",
    "            'bottleneck_categories': {},\n",
    "            'priority_actions': [],\n",
    "            'estimated_impact': {}\n",
    "        }\n",
    "        \n",
    "        # Count bottlenecks by category\n",
    "        categories = ['plant_bottlenecks', 'material_bottlenecks', 'work_center_bottlenecks']\n",
    "        \n",
    "        for category in categories:\n",
    "            if category in self.bottleneck_analysis:\n",
    "                data = self.bottleneck_analysis[category]\n",
    "                \n",
    "                if category == 'plant_bottlenecks':\n",
    "                    count = len(data.get('top_bottleneck_plants', []))\n",
    "                elif category == 'material_bottlenecks':\n",
    "                    count = len(data.get('top_problematic_materials', []))\n",
    "                elif category == 'work_center_bottlenecks':\n",
    "                    count = len(data.get('top_bottleneck_work_centers', []))\n",
    "                else:\n",
    "                    count = 0\n",
    "                \n",
    "                summary['bottleneck_categories'][category] = count\n",
    "                summary['total_bottlenecks'] += count\n",
    "        \n",
    "        # Count critical issues\n",
    "        if 'downtime_analysis' in self.bottleneck_analysis:\n",
    "            downtime_data = self.bottleneck_analysis['downtime_analysis']\n",
    "            if downtime_data.get('risk_assessment', {}).get('risk_level') == 'HIGH':\n",
    "                summary['critical_issues'] += 1\n",
    "        \n",
    "        # Count recommendations\n",
    "        if 'recommendations' in self.bottleneck_analysis:\n",
    "            recs = self.bottleneck_analysis['recommendations']\n",
    "            summary['total_recommendations'] = sum(len(rec_list) for rec_list in recs.values())\n",
    "            \n",
    "            # Extract priority actions\n",
    "            for rec_list in recs.values():\n",
    "                for rec in rec_list:\n",
    "                    if rec.get('priority') == 'HIGH':\n",
    "                        summary['priority_actions'].append(rec['action'])\n",
    "        \n",
    "        # Estimate impact\n",
    "        summary['estimated_impact'] = {\n",
    "            'potential_efficiency_gain': '15-25%',\n",
    "            'downtime_reduction': '20-30%',\n",
    "            'quality_improvement': '10-20%',\n",
    "            'cost_savings_potential': 'High'\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "def create_downtime_prediction_model(comprehensive_df):\n",
    "    \"\"\"\n",
    "    Create ML model to predict potential downtime events\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Creating Downtime Prediction Model...\")\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    df = comprehensive_df.copy()\n",
    "    \n",
    "    # Create downtime indicators (target variable)\n",
    "    downtime_indicators = []\n",
    "    \n",
    "    # Low efficiency as downtime indicator\n",
    "    if 'PRODUCTION_EFFICIENCY' in df.columns:\n",
    "        downtime_indicators.append(df['PRODUCTION_EFFICIENCY'] < 70)\n",
    "    \n",
    "    # High quality issues as downtime indicator\n",
    "    if 'QUALITY_NOTIF_COUNT' in df.columns:\n",
    "        downtime_indicators.append(df['QUALITY_NOTIF_COUNT'] > df['QUALITY_NOTIF_COUNT'].quantile(0.8))\n",
    "    \n",
    "    # Maintenance issues as downtime indicator\n",
    "    if 'HAS_MAINTENANCE' in df.columns:\n",
    "        downtime_indicators.append(df['HAS_MAINTENANCE'] == True)\n",
    "    \n",
    "    if not downtime_indicators:\n",
    "        print(\"   ‚ö†Ô∏è  No downtime indicators available for modeling\")\n",
    "        return None\n",
    "    \n",
    "    # Combine indicators\n",
    "    df['POTENTIAL_DOWNTIME'] = pd.DataFrame(downtime_indicators).T.any(axis=1).astype(int)\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_cols = []\n",
    "    \n",
    "    # Numeric features\n",
    "    numeric_features = ['ORDER_ITEM_COUNT', 'GOODS_MOVEMENT_COUNT', 'QUALITY_NOTIF_COUNT', 'DEFECT_COUNT']\n",
    "    for col in numeric_features:\n",
    "        if col in df.columns:\n",
    "            feature_cols.append(col)\n",
    "    \n",
    "    # Categorical features (encoded)\n",
    "    if 'AUART' in df.columns:\n",
    "        df['AUART_ENCODED'] = pd.Categorical(df['AUART']).codes\n",
    "        feature_cols.append('AUART_ENCODED')\n",
    "    \n",
    "    # Plant features\n",
    "    plant_col = None\n",
    "    for col in ['Plant_Code', 'WERKS', 'PWERK']:\n",
    "        if col in df.columns and df[col].notna().sum() > 0:\n",
    "            df['PLANT_ENCODED'] = pd.Categorical(df[col]).codes\n",
    "            feature_cols.append('PLANT_ENCODED')\n",
    "            break\n",
    "    \n",
    "    if len(feature_cols) < 3:\n",
    "        print(\"   ‚ö†Ô∏è  Insufficient features for downtime modeling\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[feature_cols].fillna(0)\n",
    "    y = df['POTENTIAL_DOWNTIME']\n",
    "    \n",
    "    # Remove samples where target is NaN\n",
    "    mask = ~pd.isna(y)\n",
    "    X, y = X[mask], y[mask]\n",
    "    \n",
    "    if len(X) < 50 or y.sum() < 5:\n",
    "        print(f\"   ‚ö†Ô∏è  Insufficient data for modeling (samples: {len(X)}, positive cases: {y.sum()})\")\n",
    "        return None\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Predict for all orders\n",
    "    downtime_probabilities = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    results = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'feature_importance': importance_df,\n",
    "        'downtime_probabilities': downtime_probabilities,\n",
    "        'high_risk_orders': X.index[downtime_probabilities > 0.7].tolist(),\n",
    "        'prediction_summary': {\n",
    "            'total_orders': len(X),\n",
    "            'high_risk_count': (downtime_probabilities > 0.7).sum(),\n",
    "            'medium_risk_count': ((downtime_probabilities > 0.4) & (downtime_probabilities <= 0.7)).sum(),\n",
    "            'low_risk_count': (downtime_probabilities <= 0.4).sum()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Model trained with {accuracy:.3f} accuracy\")\n",
    "    print(f\"   üìä High-risk orders identified: {results['prediction_summary']['high_risk_count']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_capacity_optimization_analysis(comprehensive_df):\n",
    "    \"\"\"\n",
    "    Analyze capacity optimization opportunities\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating Capacity Optimization Analysis...\")\n",
    "    \n",
    "    df = comprehensive_df.copy()\n",
    "    optimization_analysis = {}\n",
    "    \n",
    "    # Plant capacity analysis\n",
    "    plant_col = None\n",
    "    for col in ['Plant_Name', 'Plant_Code', 'WERKS']:\n",
    "        if col in df.columns and df[col].notna().sum() > 0:\n",
    "            plant_col = col\n",
    "            break\n",
    "    \n",
    "    if plant_col:\n",
    "        # Current utilization\n",
    "        plant_utilization = df.groupby(plant_col).agg({\n",
    "            'AUFNR': 'count',\n",
    "            'ORDER_ITEM_COUNT': 'sum',\n",
    "            'PRODUCTION_EFFICIENCY': 'mean',\n",
    "            'QUALITY_NOTIF_COUNT': 'sum'\n",
    "        })\n",
    "        \n",
    "        # Calculate relative utilization\n",
    "        max_orders = plant_utilization['AUFNR'].max()\n",
    "        plant_utilization['UTILIZATION_RATE'] = plant_utilization['AUFNR'] / max_orders\n",
    "        plant_utilization['CAPACITY_AVAILABLE'] = 1 - plant_utilization['UTILIZATION_RATE']\n",
    "        \n",
    "        # Identify optimization opportunities\n",
    "        underutilized_plants = plant_utilization[plant_utilization['UTILIZATION_RATE'] < 0.6]\n",
    "        overutilized_plants = plant_utilization[plant_utilization['UTILIZATION_RATE'] > 0.9]\n",
    "        \n",
    "        optimization_analysis['plant_capacity'] = {\n",
    "            'utilization_analysis': plant_utilization,\n",
    "            'underutilized_plants': underutilized_plants.index.tolist(),\n",
    "            'overutilized_plants': overutilized_plants.index.tolist(),\n",
    "            'optimization_potential': underutilized_plants['CAPACITY_AVAILABLE'].sum() * 100\n",
    "        }\n",
    "    \n",
    "    # Load balancing opportunities\n",
    "    if 'AUART' in df.columns and plant_col:\n",
    "        order_distribution = df.groupby([plant_col, 'AUART']).size().unstack(fill_value=0)\n",
    "        \n",
    "        # Calculate distribution efficiency\n",
    "        distribution_variance = order_distribution.var(axis=0)\n",
    "        imbalanced_order_types = distribution_variance[distribution_variance > distribution_variance.quantile(0.8)].index.tolist()\n",
    "        \n",
    "        optimization_analysis['load_balancing'] = {\n",
    "            'order_distribution': order_distribution,\n",
    "            'imbalanced_order_types': imbalanced_order_types,\n",
    "            'rebalancing_opportunities': len(imbalanced_order_types)\n",
    "        }\n",
    "    \n",
    "    # Efficiency optimization\n",
    "    if 'PRODUCTION_EFFICIENCY' in df.columns:\n",
    "        low_efficiency_threshold = df['PRODUCTION_EFFICIENCY'].quantile(0.25)\n",
    "        improvement_opportunities = df[df['PRODUCTION_EFFICIENCY'] < low_efficiency_threshold]\n",
    "        \n",
    "        potential_gain = (85 - improvement_opportunities['PRODUCTION_EFFICIENCY']).sum()  # Target 85% efficiency\n",
    "        \n",
    "        optimization_analysis['efficiency_optimization'] = {\n",
    "            'low_efficiency_orders': len(improvement_opportunities),\n",
    "            'current_avg_efficiency': improvement_opportunities['PRODUCTION_EFFICIENCY'].mean(),\n",
    "            'potential_efficiency_gain': potential_gain,\n",
    "            'target_efficiency': 85\n",
    "        }\n",
    "    \n",
    "    # Resource allocation optimization\n",
    "    if plant_col and 'ORDER_ITEM_COUNT' in df.columns:\n",
    "        resource_allocation = df.groupby(plant_col).agg({\n",
    "            'ORDER_ITEM_COUNT': ['sum', 'mean'],\n",
    "            'AUFNR': 'count',\n",
    "            'PRODUCTION_EFFICIENCY': 'mean'\n",
    "        })\n",
    "        \n",
    "        # Calculate resource efficiency\n",
    "        resource_allocation.columns = ['_'.join(col).strip() for col in resource_allocation.columns]\n",
    "        resource_allocation['RESOURCE_EFFICIENCY'] = (\n",
    "            resource_allocation['ORDER_ITEM_COUNT_sum'] / resource_allocation['AUFNR_count'] *\n",
    "            resource_allocation['PRODUCTION_EFFICIENCY_mean'] / 100\n",
    "        )\n",
    "        \n",
    "        optimization_analysis['resource_allocation'] = {\n",
    "            'current_allocation': resource_allocation,\n",
    "            'most_efficient_plant': resource_allocation['RESOURCE_EFFICIENCY'].idxmax(),\n",
    "            'least_efficient_plant': resource_allocation['RESOURCE_EFFICIENCY'].idxmin(),\n",
    "            'efficiency_gap': resource_allocation['RESOURCE_EFFICIENCY'].max() - resource_allocation['RESOURCE_EFFICIENCY'].min()\n",
    "        }\n",
    "    \n",
    "    print(f\"   ‚úì Capacity optimization analysis completed\")\n",
    "    \n",
    "    return optimization_analysis\n",
    "\n",
    "def save_bottleneck_analysis_results(bottleneck_analysis, downtime_model, capacity_analysis, result_folder=\"result\"):\n",
    "    \"\"\"\n",
    "    Save bottleneck analysis results to organized folder\n",
    "    \"\"\"\n",
    "    print(\"\\nüíæ Saving Bottleneck Analysis Results...\")\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Create bottleneck-specific folder\n",
    "    bottleneck_folder = os.path.join(result_folder, f\"10_bottleneck_analysis_{timestamp}\")\n",
    "    os.makedirs(bottleneck_folder, exist_ok=True)\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    # 1. Bottleneck Summary Report\n",
    "    summary_file = os.path.join(bottleneck_folder, f\"bottleneck_summary_report_{timestamp}.txt\")\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"BOTTLENECK AND DOWNTIME ANALYSIS REPORT\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        # Executive Summary\n",
    "        if 'summary' in bottleneck_analysis:\n",
    "            summary = bottleneck_analysis['summary']\n",
    "            f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Total Bottlenecks Identified: {summary['total_bottlenecks']}\\n\")\n",
    "            f.write(f\"Critical Issues: {summary['critical_issues']}\\n\")\n",
    "            f.write(f\"Recommendations Generated: {summary['total_recommendations']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"BOTTLENECK BREAKDOWN:\\n\")\n",
    "            for category, count in summary['bottleneck_categories'].items():\n",
    "                f.write(f\"  {category.replace('_', ' ').title()}: {count}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Plant Bottlenecks\n",
    "        if 'plant_bottlenecks' in bottleneck_analysis:\n",
    "            plant_data = bottleneck_analysis['plant_bottlenecks']\n",
    "            f.write(\"PLANT BOTTLENECKS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            if plant_data.get('top_bottleneck_plants'):\n",
    "                f.write(f\"Top Bottleneck Plants: {', '.join(plant_data['top_bottleneck_plants'][:5])}\\n\")\n",
    "            \n",
    "            indicators = plant_data.get('bottleneck_indicators', {})\n",
    "            if indicators.get('low_efficiency_plants'):\n",
    "                f.write(f\"Low Efficiency Plants: {', '.join(indicators['low_efficiency_plants'][:5])}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Material Bottlenecks\n",
    "        if 'material_bottlenecks' in bottleneck_analysis:\n",
    "            material_data = bottleneck_analysis['material_bottlenecks']\n",
    "            f.write(\"MATERIAL BOTTLENECKS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            if material_data.get('top_problematic_materials'):\n",
    "                f.write(f\"High-Risk Materials: {', '.join(material_data['top_problematic_materials'][:5])}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Downtime Analysis\n",
    "        if 'downtime_analysis' in bottleneck_analysis:\n",
    "            downtime_data = bottleneck_analysis['downtime_analysis']\n",
    "            f.write(\"DOWNTIME ANALYSIS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            risk_assessment = downtime_data.get('risk_assessment', {})\n",
    "            f.write(f\"Downtime Risk Level: {risk_assessment.get('risk_level', 'UNKNOWN')}\\n\")\n",
    "            f.write(f\"Risk Factors: {', '.join(risk_assessment.get('risk_factors', []))}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Recommendations\n",
    "        if 'recommendations' in bottleneck_analysis:\n",
    "            f.write(\"KEY RECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            recs = bottleneck_analysis['recommendations']\n",
    "            \n",
    "            priority_order = ['immediate_actions', 'process_improvements', 'capacity_optimizations', 'maintenance_actions']\n",
    "            \n",
    "            for category in priority_order:\n",
    "                if category in recs and recs[category]:\n",
    "                    f.write(f\"\\n{category.replace('_', ' ').upper()}:\\n\")\n",
    "                    for i, rec in enumerate(recs[category][:3], 1):  # Top 3 per category\n",
    "                        f.write(f\"  {i}. {rec.get('action', rec)}\\n\")\n",
    "                        if isinstance(rec, dict):\n",
    "                            f.write(f\"     Priority: {rec.get('priority', 'N/A')}, Timeline: {rec.get('timeline', 'N/A')}\\n\")\n",
    "    \n",
    "    created_files.append(summary_file)\n",
    "    print(f\"   ‚úì Bottleneck Summary: {os.path.basename(summary_file)}\")\n",
    "    \n",
    "    # 2. Detailed Analysis Excel\n",
    "    excel_file = os.path.join(bottleneck_folder, f\"bottleneck_detailed_analysis_{timestamp}.xlsx\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "        # Plant analysis\n",
    "        if 'plant_bottlenecks' in bottleneck_analysis:\n",
    "            plant_data = bottleneck_analysis['plant_bottlenecks']\n",
    "            if 'plant_metrics' in plant_data:\n",
    "                plant_data['plant_metrics'].to_excel(writer, sheet_name='Plant_Bottlenecks')\n",
    "        \n",
    "        # Material analysis\n",
    "        if 'material_bottlenecks' in bottleneck_analysis:\n",
    "            material_data = bottleneck_analysis['material_bottlenecks']\n",
    "            if 'material_analysis' in material_data:\n",
    "                material_data['material_analysis'].to_excel(writer, sheet_name='Material_Bottlenecks')\n",
    "        \n",
    "        # Work center analysis\n",
    "        if 'work_center_bottlenecks' in bottleneck_analysis:\n",
    "            wc_data = bottleneck_analysis['work_center_bottlenecks']\n",
    "            if 'work_center_analysis' in wc_data:\n",
    "                wc_data['work_center_analysis'].to_excel(writer, sheet_name='WorkCenter_Bottlenecks')\n",
    "        \n",
    "        # Downtime prediction results\n",
    "        if downtime_model:\n",
    "            downtime_df = pd.DataFrame({\n",
    "                'Order_Index': range(len(downtime_model['downtime_probabilities'])),\n",
    "                'Downtime_Probability': downtime_model['downtime_probabilities'],\n",
    "                'Risk_Level': pd.cut(downtime_model['downtime_probabilities'], \n",
    "                                   bins=[0, 0.4, 0.7, 1.0], \n",
    "                                   labels=['Low', 'Medium', 'High'])\n",
    "            })\n",
    "            downtime_df.to_excel(writer, sheet_name='Downtime_Predictions', index=False)\n",
    "            \n",
    "            # Feature importance\n",
    "            if 'feature_importance' in downtime_model:\n",
    "                downtime_model['feature_importance'].to_excel(writer, sheet_name='Downtime_Features', index=False)\n",
    "        \n",
    "        # Capacity optimization\n",
    "        if capacity_analysis and 'plant_capacity' in capacity_analysis:\n",
    "            capacity_data = capacity_analysis['plant_capacity']['utilization_analysis']\n",
    "            capacity_data.to_excel(writer, sheet_name='Capacity_Analysis')\n",
    "    \n",
    "    created_files.append(excel_file)\n",
    "    print(f\"   ‚úì Detailed Analysis Excel: {os.path.basename(excel_file)}\")\n",
    "    \n",
    "    # 3. Recommendations Action Plan\n",
    "    action_plan_file = os.path.join(bottleneck_folder, f\"bottleneck_action_plan_{timestamp}.txt\")\n",
    "    \n",
    "    with open(action_plan_file, 'w') as f:\n",
    "        f.write(\"BOTTLENECK RESOLUTION ACTION PLAN\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        if 'recommendations' in bottleneck_analysis:\n",
    "            recs = bottleneck_analysis['recommendations']\n",
    "            \n",
    "            # Immediate Actions\n",
    "            f.write(\"IMMEDIATE ACTIONS (1-30 DAYS)\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for i, action in enumerate(recs.get('immediate_actions', []), 1):\n",
    "                f.write(f\"{i}. {action.get('action', action)}\\n\")\n",
    "                if isinstance(action, dict):\n",
    "                    f.write(f\"   Timeline: {action.get('timeline', 'TBD')}\\n\")\n",
    "                    f.write(f\"   Priority: {action.get('priority', 'Medium')}\\n\")\n",
    "                    f.write(f\"   Category: {action.get('category', 'General')}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Process Improvements\n",
    "            f.write(\"PROCESS IMPROVEMENTS (1-3 MONTHS)\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for i, action in enumerate(recs.get('process_improvements', []), 1):\n",
    "                f.write(f\"{i}. {action.get('action', action)}\\n\")\n",
    "                if isinstance(action, dict):\n",
    "                    f.write(f\"   Timeline: {action.get('timeline', 'TBD')}\\n\")\n",
    "                    f.write(f\"   Priority: {action.get('priority', 'Medium')}\\n\")\n",
    "                    f.write(f\"   Category: {action.get('category', 'General')}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Capacity Optimizations\n",
    "            f.write(\"CAPACITY OPTIMIZATIONS (1-6 MONTHS)\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for i, action in enumerate(recs.get('capacity_optimizations', []), 1):\n",
    "                f.write(f\"{i}. {action.get('action', action)}\\n\")\n",
    "                if isinstance(action, dict):\n",
    "                    f.write(f\"   Timeline: {action.get('timeline', 'TBD')}\\n\")\n",
    "                    f.write(f\"   Priority: {action.get('priority', 'Medium')}\\n\")\n",
    "                    f.write(f\"   Category: {action.get('category', 'General')}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "    \n",
    "    created_files.append(action_plan_file)\n",
    "    print(f\"   ‚úì Action Plan: {os.path.basename(action_plan_file)}\")\n",
    "    \n",
    "    # 4. JSON export for APIs/dashboards\n",
    "    json_file = os.path.join(bottleneck_folder, f\"bottleneck_analysis_{timestamp}.json\")\n",
    "    \n",
    "    # Prepare data for JSON serialization\n",
    "    json_data = {\n",
    "        'bottleneck_analysis': bottleneck_analysis,\n",
    "        'downtime_model_summary': {\n",
    "            'accuracy': downtime_model['accuracy'] if downtime_model else None,\n",
    "            'high_risk_orders': len(downtime_model['high_risk_orders']) if downtime_model else 0,\n",
    "            'prediction_summary': downtime_model['prediction_summary'] if downtime_model else {}\n",
    "        },\n",
    "        'capacity_analysis': capacity_analysis\n",
    "    }\n",
    "    \n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2, default=str)\n",
    "    \n",
    "    created_files.append(json_file)\n",
    "    print(f\"   ‚úì JSON Export: {os.path.basename(json_file)}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Saved {len(created_files)} bottleneck analysis files\")\n",
    "    return created_files\n",
    "\n",
    "# Main integration function\n",
    "def run_complete_bottleneck_analysis(comprehensive_df, result_folder=\"result\"):\n",
    "    \"\"\"\n",
    "    Run complete bottleneck and downtime analysis\n",
    "    \"\"\"\n",
    "    print(\"üîç RUNNING COMPLETE BOTTLENECK & DOWNTIME ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize bottleneck detector\n",
    "    bottleneck_detector = SAPBottleneckDetector(comprehensive_df)\n",
    "    \n",
    "    # Run bottleneck analysis\n",
    "    bottleneck_analysis = bottleneck_detector.run_complete_bottleneck_analysis()\n",
    "    \n",
    "    # Create downtime prediction model\n",
    "    downtime_model = create_downtime_prediction_model(comprehensive_df)\n",
    "    \n",
    "    # Create capacity optimization analysis\n",
    "    capacity_analysis = create_capacity_optimization_analysis(comprehensive_df)\n",
    "    \n",
    "    # Save results\n",
    "    created_files = save_bottleneck_analysis_results(\n",
    "        bottleneck_analysis, downtime_model, capacity_analysis, result_folder\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ BOTTLENECK ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üìä ANALYSIS RESULTS:\")\n",
    "    print(f\"   ‚Ä¢ Bottlenecks Identified: {bottleneck_analysis.get('summary', {}).get('total_bottlenecks', 0)}\")\n",
    "    print(f\"   ‚Ä¢ Critical Issues: {bottleneck_analysis.get('summary', {}).get('critical_issues', 0)}\")\n",
    "    print(f\"   ‚Ä¢ High-Risk Orders: {downtime_model['prediction_summary']['high_risk_count'] if downtime_model else 0}\")\n",
    "    print(f\"   ‚Ä¢ Action Items: {bottleneck_analysis.get('summary', {}).get('total_recommendations', 0)}\")\n",
    "    \n",
    "    return {\n",
    "        'bottleneck_detector': bottleneck_detector,\n",
    "        'bottleneck_analysis': bottleneck_analysis,\n",
    "        'downtime_model': downtime_model,\n",
    "        'capacity_analysis': capacity_analysis,\n",
    "        'created_files': created_files\n",
    "    }\n",
    "\n",
    "# Usage example for Tolaram assessment\n",
    "def tolaram_bottleneck_assessment_guide():\n",
    "    \"\"\"\n",
    "    Guide for using bottleneck analysis in Tolaram assessment\n",
    "    \"\"\"\n",
    "    print(\"üìö TOLARAM ASSESSMENT - BOTTLENECK ANALYSIS GUIDE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n1. INTEGRATION WITH MAIN ANALYSIS:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# After running main SAP integration\")\n",
    "    print(\"comprehensive_df, summary_stats, quality_details = create_comprehensive_sap_view(...)\")\n",
    "    print(\"\")\n",
    "    print(\"# Run bottleneck analysis\")\n",
    "    print(\"bottleneck_results = run_complete_bottleneck_analysis(\")\n",
    "    print(\"    comprehensive_df, result_folder='tolaram_assessment'\")\n",
    "    print(\")\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n2. KEY BOTTLENECK INSIGHTS FOR ASSESSMENT:\")\n",
    "    print(\"‚Ä¢ Plant Performance Bottlenecks\")\n",
    "    print(\"‚Ä¢ Material Supply Chain Issues\")\n",
    "    print(\"‚Ä¢ Work Center Capacity Constraints\")\n",
    "    print(\"‚Ä¢ Downtime Risk Prediction\")\n",
    "    print(\"‚Ä¢ Schedule Delay Analysis\")\n",
    "    print(\"‚Ä¢ Throughput Optimization Opportunities\")\n",
    "    \n",
    "    print(\"\\n3. BUSINESS VALUE DEMONSTRATION:\")\n",
    "    print(\"‚Ä¢ Quantified efficiency improvement potential\")\n",
    "    print(\"‚Ä¢ Predictive downtime prevention\")\n",
    "    print(\"‚Ä¢ Capacity optimization recommendations\")\n",
    "    print(\"‚Ä¢ Cost reduction through bottleneck elimination\")\n",
    "    print(\"‚Ä¢ Data-driven resource allocation\")\n",
    "    \n",
    "    print(\"\\n4. ASSESSMENT REPORT SECTIONS:\")\n",
    "    print(\"‚Ä¢ Executive Summary: Bottleneck impact on operations\")\n",
    "    print(\"‚Ä¢ Predictive Analytics: Downtime risk modeling\")\n",
    "    print(\"‚Ä¢ Capacity Analysis: Optimization opportunities\")\n",
    "    print(\"‚Ä¢ Action Plan: Prioritized improvement initiatives\")\n",
    "    print(\"‚Ä¢ ROI Estimation: Expected benefits of improvements\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tolaram_bottleneck_assessment_guide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce1770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for matplotlib\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class SAPVisualizationSuite:\n",
    "    \"\"\"\n",
    "    Comprehensive Visualization Suite for SAP Manufacturing and Quality Analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, comprehensive_df, ml_results=None, bottleneck_results=None, result_folder=\"result\"):\n",
    "        self.comprehensive_df = comprehensive_df\n",
    "        self.ml_results = ml_results\n",
    "        self.bottleneck_results = bottleneck_results\n",
    "        self.result_folder = result_folder\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Create visualization folder\n",
    "        self.viz_folder = os.path.join(result_folder, f\"11_visualizations_{self.timestamp}\")\n",
    "        os.makedirs(self.viz_folder, exist_ok=True)\n",
    "        \n",
    "        # Color schemes\n",
    "        self.colors = {\n",
    "            'primary': '#1f77b4',\n",
    "            'secondary': '#ff7f0e', \n",
    "            'success': '#2ca02c',\n",
    "            'warning': '#d62728',\n",
    "            'info': '#9467bd',\n",
    "            'quality_good': '#2ca02c',\n",
    "            'quality_poor': '#d62728',\n",
    "            'quality_medium': '#ff7f0e'\n",
    "        }\n",
    "        \n",
    "        self.created_files = []\n",
    "    \n",
    "    def create_executive_dashboard(self):\n",
    "        \"\"\"\n",
    "        Create executive-level dashboard with key KPIs\n",
    "        \"\"\"\n",
    "        print(\"üìä Creating Executive Dashboard...\")\n",
    "        \n",
    "        # Create subplot layout\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=3,\n",
    "            subplot_titles=[\n",
    "                'Quality Score Distribution', 'Plant Performance Overview', 'Production Efficiency Trends',\n",
    "                'Quality Issues by Plant', 'Order Volume Trends', 'Bottleneck Analysis',\n",
    "                'ML Predictions Summary', 'Downtime Risk Assessment', 'Key Performance Metrics'\n",
    "            ],\n",
    "            specs=[\n",
    "                [{\"type\": \"xy\"}, {\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "                [{\"type\": \"xy\"}, {\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "                [{\"type\": \"xy\"}, {\"type\": \"xy\"}, {\"type\": \"xy\"}]\n",
    "            ],\n",
    "            vertical_spacing=0.12,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        df = self.comprehensive_df\n",
    "        \n",
    "        # 1. Quality Score Distribution\n",
    "        if 'QUALITY_SCORE' in df.columns:\n",
    "            quality_scores = df['QUALITY_SCORE'].dropna()\n",
    "            fig.add_trace(\n",
    "                go.Histogram(x=quality_scores, nbinsx=20, name=\"Quality Scores\", \n",
    "                           marker_color=self.colors['primary'], showlegend=False),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # 2. Plant Performance Overview\n",
    "        plant_col = self._get_plant_column()\n",
    "        if plant_col:\n",
    "            plant_performance = df.groupby(plant_col).agg({\n",
    "                'QUALITY_SCORE': 'mean',\n",
    "                'AUFNR': 'count'\n",
    "            }).reset_index()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=plant_performance[plant_col], \n",
    "                          y=plant_performance['QUALITY_SCORE'],\n",
    "                          mode='markers',\n",
    "                          marker=dict(size=plant_performance['AUFNR']/10, \n",
    "                                    color=self.colors['secondary']),\n",
    "                          name=\"Plant Performance\", showlegend=False),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # 3. Production Efficiency Trends\n",
    "        if 'PRODUCTION_EFFICIENCY' in df.columns and 'ERDAT' in df.columns:\n",
    "            try:\n",
    "                df_temp = df.copy()\n",
    "                df_temp['ERDAT'] = pd.to_datetime(df_temp['ERDAT'], errors='coerce')\n",
    "                df_temp['MONTH'] = df_temp['ERDAT'].dt.to_period('M')\n",
    "                \n",
    "                monthly_efficiency = df_temp.groupby('MONTH')['PRODUCTION_EFFICIENCY'].mean().reset_index()\n",
    "                monthly_efficiency['MONTH_STR'] = monthly_efficiency['MONTH'].astype(str)\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=monthly_efficiency['MONTH_STR'], \n",
    "                              y=monthly_efficiency['PRODUCTION_EFFICIENCY'],\n",
    "                              mode='lines+markers',\n",
    "                              line=dict(color=self.colors['success']),\n",
    "                              name=\"Efficiency Trend\", showlegend=False),\n",
    "                    row=1, col=3\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # 4. Quality Issues by Plant\n",
    "        if plant_col and 'QUALITY_NOTIF_COUNT' in df.columns:\n",
    "            plant_quality = df.groupby(plant_col)['QUALITY_NOTIF_COUNT'].sum().reset_index()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=plant_quality[plant_col], \n",
    "                       y=plant_quality['QUALITY_NOTIF_COUNT'],\n",
    "                       marker_color=self.colors['warning'],\n",
    "                       name=\"Quality Issues\", showlegend=False),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # 5. Order Volume Trends\n",
    "        if 'ERDAT' in df.columns:\n",
    "            try:\n",
    "                df_temp = df.copy()\n",
    "                df_temp['ERDAT'] = pd.to_datetime(df_temp['ERDAT'], errors='coerce')\n",
    "                df_temp['MONTH'] = df_temp['ERDAT'].dt.to_period('M')\n",
    "                \n",
    "                monthly_orders = df_temp.groupby('MONTH').size().reset_index(name='Order_Count')\n",
    "                monthly_orders['MONTH_STR'] = monthly_orders['MONTH'].astype(str)\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Bar(x=monthly_orders['MONTH_STR'], \n",
    "                           y=monthly_orders['Order_Count'],\n",
    "                           marker_color=self.colors['info'],\n",
    "                           name=\"Order Volume\", showlegend=False),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # 6. Bottleneck Analysis\n",
    "        if self.bottleneck_results and 'bottleneck_analysis' in self.bottleneck_results:\n",
    "            summary = self.bottleneck_results['bottleneck_analysis'].get('summary', {})\n",
    "            categories = summary.get('bottleneck_categories', {})\n",
    "            \n",
    "            if categories:\n",
    "                fig.add_trace(\n",
    "                    go.Bar(x=list(categories.keys()), \n",
    "                           y=list(categories.values()),\n",
    "                           marker_color=self.colors['warning'],\n",
    "                           name=\"Bottlenecks\", showlegend=False),\n",
    "                    row=2, col=3\n",
    "                )\n",
    "        \n",
    "        # 7. ML Predictions Summary\n",
    "        if self.ml_results and 'predictions' in self.ml_results:\n",
    "            predictions = self.ml_results['predictions']\n",
    "            \n",
    "            if 'QUALITY_ISSUE_PROBABILITY' in predictions.columns:\n",
    "                risk_levels = pd.cut(predictions['QUALITY_ISSUE_PROBABILITY'], \n",
    "                                   bins=[0, 0.3, 0.7, 1.0], \n",
    "                                   labels=['Low', 'Medium', 'High'])\n",
    "                risk_counts = risk_levels.value_counts()\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Pie(labels=risk_counts.index, \n",
    "                           values=risk_counts.values,\n",
    "                           hole=0.4,\n",
    "                           marker=dict(colors=[self.colors['success'], \n",
    "                                             self.colors['warning'], \n",
    "                                             self.colors['warning']]),\n",
    "                           showlegend=False),\n",
    "                    row=3, col=1\n",
    "                )\n",
    "        \n",
    "        # 8. Downtime Risk Assessment\n",
    "        if self.bottleneck_results and 'downtime_model' in self.bottleneck_results:\n",
    "            downtime_model = self.bottleneck_results['downtime_model']\n",
    "            if downtime_model and 'prediction_summary' in downtime_model:\n",
    "                summary = downtime_model['prediction_summary']\n",
    "                \n",
    "                risk_data = {\n",
    "                    'Risk Level': ['Low', 'Medium', 'High'],\n",
    "                    'Count': [summary.get('low_risk_count', 0),\n",
    "                             summary.get('medium_risk_count', 0), \n",
    "                             summary.get('high_risk_count', 0)]\n",
    "                }\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Bar(x=risk_data['Risk Level'], \n",
    "                           y=risk_data['Count'],\n",
    "                           marker_color=[self.colors['success'], \n",
    "                                       self.colors['warning'], \n",
    "                                       self.colors['warning']],\n",
    "                           name=\"Downtime Risk\", showlegend=False),\n",
    "                    row=3, col=2\n",
    "                )\n",
    "        \n",
    "        # 9. Key Performance Metrics\n",
    "        metrics = self._calculate_key_metrics()\n",
    "        if metrics:\n",
    "            fig.add_trace(\n",
    "                go.Indicator(\n",
    "                    mode=\"gauge+number+delta\",\n",
    "                    value=metrics.get('overall_score', 75),\n",
    "                    domain={'x': [0, 1], 'y': [0, 1]},\n",
    "                    title={'text': \"Overall Performance\"},\n",
    "                    gauge={\n",
    "                        'axis': {'range': [None, 100]},\n",
    "                        'bar': {'color': self.colors['primary']},\n",
    "                        'steps': [\n",
    "                            {'range': [0, 50], 'color': \"lightgray\"},\n",
    "                            {'range': [50, 80], 'color': \"gray\"}\n",
    "                        ],\n",
    "                        'threshold': {\n",
    "                            'line': {'color': \"red\", 'width': 4},\n",
    "                            'thickness': 0.75,\n",
    "                            'value': 90\n",
    "                        }\n",
    "                    }\n",
    "                ),\n",
    "                row=3, col=3\n",
    "            )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': \"SAP Manufacturing Analytics - Executive Dashboard\",\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'font': {'size': 24}\n",
    "            },\n",
    "            height=1200,\n",
    "            showlegend=False,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        # Save dashboard\n",
    "        dashboard_file = os.path.join(self.viz_folder, f\"executive_dashboard_{self.timestamp}.html\")\n",
    "        fig.write_html(dashboard_file)\n",
    "        self.created_files.append(dashboard_file)\n",
    "        \n",
    "        print(f\"   ‚úì Executive Dashboard: {os.path.basename(dashboard_file)}\")\n",
    "        return fig\n",
    "    \n",
    "    def create_quality_analysis_charts(self):\n",
    "        \"\"\"\n",
    "        Create detailed quality analysis visualizations\n",
    "        \"\"\"\n",
    "        print(\"üîç Creating Quality Analysis Charts...\")\n",
    "        \n",
    "        df = self.comprehensive_df\n",
    "        \n",
    "        # Quality Score Distribution with Statistics\n",
    "        fig1 = go.Figure()\n",
    "        \n",
    "        if 'QUALITY_SCORE' in df.columns:\n",
    "            quality_scores = df['QUALITY_SCORE'].dropna()\n",
    "            \n",
    "            # Histogram\n",
    "            fig1.add_trace(go.Histogram(\n",
    "                x=quality_scores,\n",
    "                nbinsx=30,\n",
    "                name=\"Quality Score Distribution\",\n",
    "                marker_color=self.colors['primary'],\n",
    "                opacity=0.7\n",
    "            ))\n",
    "            \n",
    "            # Add mean line\n",
    "            mean_score = quality_scores.mean()\n",
    "            fig1.add_vline(x=mean_score, line_dash=\"dash\", line_color=\"red\",\n",
    "                          annotation_text=f\"Mean: {mean_score:.1f}\")\n",
    "            \n",
    "            fig1.update_layout(\n",
    "                title=\"Quality Score Distribution Analysis\",\n",
    "                xaxis_title=\"Quality Score\",\n",
    "                yaxis_title=\"Number of Orders\",\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "        \n",
    "        # Quality by Plant Comparison\n",
    "        fig2 = go.Figure()\n",
    "        plant_col = self._get_plant_column()\n",
    "        \n",
    "        if plant_col and 'QUALITY_SCORE' in df.columns:\n",
    "            plant_quality = df.groupby(plant_col).agg({\n",
    "                'QUALITY_SCORE': ['mean', 'std', 'count'],\n",
    "                'QUALITY_NOTIF_COUNT': 'sum'\n",
    "            }).round(2)\n",
    "            \n",
    "            plant_quality.columns = ['_'.join(col).strip() for col in plant_quality.columns]\n",
    "            plant_quality = plant_quality.reset_index()\n",
    "            \n",
    "            # Box plot for quality scores by plant\n",
    "            for plant in plant_quality[plant_col].unique():\n",
    "                plant_scores = df[df[plant_col] == plant]['QUALITY_SCORE'].dropna()\n",
    "                fig2.add_trace(go.Box(\n",
    "                    y=plant_scores,\n",
    "                    name=str(plant),\n",
    "                    boxpoints='outliers'\n",
    "                ))\n",
    "            \n",
    "            fig2.update_layout(\n",
    "                title=\"Quality Score Distribution by Plant\",\n",
    "                xaxis_title=\"Plant\",\n",
    "                yaxis_title=\"Quality Score\",\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "        \n",
    "        # Quality Issues Over Time\n",
    "        fig3 = go.Figure()\n",
    "        \n",
    "        if 'ERDAT' in df.columns and 'QUALITY_NOTIF_COUNT' in df.columns:\n",
    "            try:\n",
    "                df_temp = df.copy()\n",
    "                df_temp['ERDAT'] = pd.to_datetime(df_temp['ERDAT'], errors='coerce')\n",
    "                df_temp['WEEK'] = df_temp['ERDAT'].dt.to_period('W')\n",
    "                \n",
    "                weekly_quality = df_temp.groupby('WEEK').agg({\n",
    "                    'QUALITY_NOTIF_COUNT': 'sum',\n",
    "                    'AUFNR': 'count'\n",
    "                }).reset_index()\n",
    "                weekly_quality['QUALITY_RATE'] = weekly_quality['QUALITY_NOTIF_COUNT'] / weekly_quality['AUFNR'] * 100\n",
    "                weekly_quality['WEEK_STR'] = weekly_quality['WEEK'].astype(str)\n",
    "                \n",
    "                fig3.add_trace(go.Scatter(\n",
    "                    x=weekly_quality['WEEK_STR'],\n",
    "                    y=weekly_quality['QUALITY_RATE'],\n",
    "                    mode='lines+markers',\n",
    "                    name=\"Quality Issue Rate\",\n",
    "                    line=dict(color=self.colors['warning'])\n",
    "                ))\n",
    "                \n",
    "                fig3.update_layout(\n",
    "                    title=\"Quality Issue Rate Trend Over Time\",\n",
    "                    xaxis_title=\"Week\",\n",
    "                    yaxis_title=\"Quality Issues per 100 Orders\",\n",
    "                    template=\"plotly_white\"\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Save quality charts\n",
    "        quality_file1 = os.path.join(self.viz_folder, f\"quality_distribution_{self.timestamp}.html\")\n",
    "        quality_file2 = os.path.join(self.viz_folder, f\"quality_by_plant_{self.timestamp}.html\")\n",
    "        quality_file3 = os.path.join(self.viz_folder, f\"quality_trends_{self.timestamp}.html\")\n",
    "        \n",
    "        fig1.write_html(quality_file1)\n",
    "        fig2.write_html(quality_file2)\n",
    "        fig3.write_html(quality_file3)\n",
    "        \n",
    "        self.created_files.extend([quality_file1, quality_file2, quality_file3])\n",
    "        \n",
    "        print(f\"   ‚úì Quality Analysis Charts: 3 files created\")\n",
    "        return [fig1, fig2, fig3]\n",
    "    \n",
    "    def create_production_efficiency_charts(self):\n",
    "        \"\"\"\n",
    "        Create production efficiency analysis visualizations\n",
    "        \"\"\"\n",
    "        print(\"‚ö° Creating Production Efficiency Charts...\")\n",
    "        \n",
    "        df = self.comprehensive_df\n",
    "        \n",
    "        # Efficiency Distribution and Benchmarking\n",
    "        fig1 = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=['Efficiency Distribution', 'Efficiency by Plant', \n",
    "                           'Efficiency vs Quality Score', 'Monthly Efficiency Trends'],\n",
    "            specs=[[{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "                   [{\"type\": \"xy\"}, {\"type\": \"xy\"}]]\n",
    "        )\n",
    "        \n",
    "        if 'PRODUCTION_EFFICIENCY' in df.columns:\n",
    "            efficiency = df['PRODUCTION_EFFICIENCY'].dropna()\n",
    "            \n",
    "            # 1. Efficiency Distribution\n",
    "            fig1.add_trace(\n",
    "                go.Histogram(x=efficiency, nbinsx=25, name=\"Efficiency Distribution\",\n",
    "                           marker_color=self.colors['success'], showlegend=False),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Add benchmark lines\n",
    "            fig1.add_vline(x=85, line_dash=\"dash\", line_color=\"orange\",\n",
    "                          annotation_text=\"Target: 85%\", row=1, col=1)\n",
    "            fig1.add_vline(x=efficiency.mean(), line_dash=\"dash\", line_color=\"red\",\n",
    "                          annotation_text=f\"Avg: {efficiency.mean():.1f}%\", row=1, col=1)\n",
    "        \n",
    "        # 2. Efficiency by Plant\n",
    "        plant_col = self._get_plant_column()\n",
    "        if plant_col and 'PRODUCTION_EFFICIENCY' in df.columns:\n",
    "            plant_efficiency = df.groupby(plant_col)['PRODUCTION_EFFICIENCY'].agg(['mean', 'count']).reset_index()\n",
    "            \n",
    "            fig1.add_trace(\n",
    "                go.Bar(x=plant_efficiency[plant_col], \n",
    "                       y=plant_efficiency['mean'],\n",
    "                       marker_color=self.colors['info'],\n",
    "                       name=\"Plant Efficiency\", showlegend=False),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # 3. Efficiency vs Quality Score\n",
    "        if 'PRODUCTION_EFFICIENCY' in df.columns and 'QUALITY_SCORE' in df.columns:\n",
    "            fig1.add_trace(\n",
    "                go.Scatter(x=df['PRODUCTION_EFFICIENCY'], \n",
    "                          y=df['QUALITY_SCORE'],\n",
    "                          mode='markers',\n",
    "                          marker=dict(color=self.colors['primary'], opacity=0.6),\n",
    "                          name=\"Efficiency vs Quality\", showlegend=False),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # 4. Monthly Efficiency Trends\n",
    "        if 'ERDAT' in df.columns and 'PRODUCTION_EFFICIENCY' in df.columns:\n",
    "            try:\n",
    "                df_temp = df.copy()\n",
    "                df_temp['ERDAT'] = pd.to_datetime(df_temp['ERDAT'], errors='coerce')\n",
    "                df_temp['MONTH'] = df_temp['ERDAT'].dt.to_period('M')\n",
    "                \n",
    "                monthly_eff = df_temp.groupby('MONTH')['PRODUCTION_EFFICIENCY'].mean().reset_index()\n",
    "                monthly_eff['MONTH_STR'] = monthly_eff['MONTH'].astype(str)\n",
    "                \n",
    "                fig1.add_trace(\n",
    "                    go.Scatter(x=monthly_eff['MONTH_STR'], \n",
    "                              y=monthly_eff['PRODUCTION_EFFICIENCY'],\n",
    "                              mode='lines+markers',\n",
    "                              line=dict(color=self.colors['success']),\n",
    "                              name=\"Monthly Trend\", showlegend=False),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        fig1.update_layout(\n",
    "            title=\"Production Efficiency Analysis\",\n",
    "            height=800,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        # Efficiency Improvement Opportunities\n",
    "        fig2 = go.Figure()\n",
    "        \n",
    "        if 'PRODUCTION_EFFICIENCY' in df.columns:\n",
    "            # Create efficiency categories\n",
    "            df_temp = df.copy()\n",
    "            df_temp['EFFICIENCY_CATEGORY'] = pd.cut(\n",
    "                df_temp['PRODUCTION_EFFICIENCY'],\n",
    "                bins=[0, 70, 85, 95, 100],\n",
    "                labels=['Poor (<70%)', 'Fair (70-85%)', 'Good (85-95%)', 'Excellent (95%+)']\n",
    "            )\n",
    "            \n",
    "            efficiency_dist = df_temp['EFFICIENCY_CATEGORY'].value_counts()\n",
    "            \n",
    "            colors = [self.colors['warning'], self.colors['secondary'], \n",
    "                     self.colors['success'], self.colors['primary']]\n",
    "            \n",
    "            fig2.add_trace(go.Pie(\n",
    "                labels=efficiency_dist.index,\n",
    "                values=efficiency_dist.values,\n",
    "                marker=dict(colors=colors),\n",
    "                hole=0.4,\n",
    "                textinfo='label+percent'\n",
    "            ))\n",
    "            \n",
    "            fig2.update_layout(\n",
    "                title=\"Production Efficiency Categories\",\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "        \n",
    "        # Save efficiency charts\n",
    "        efficiency_file1 = os.path.join(self.viz_folder, f\"production_efficiency_analysis_{self.timestamp}.html\")\n",
    "        efficiency_file2 = os.path.join(self.viz_folder, f\"efficiency_categories_{self.timestamp}.html\")\n",
    "        \n",
    "        fig1.write_html(efficiency_file1)\n",
    "        fig2.write_html(efficiency_file2)\n",
    "        \n",
    "        self.created_files.extend([efficiency_file1, efficiency_file2])\n",
    "        \n",
    "        print(f\"   ‚úì Production Efficiency Charts: 2 files created\")\n",
    "        return [fig1, fig2]\n",
    "    \n",
    "    def create_bottleneck_visualizations(self):\n",
    "        \"\"\"\n",
    "        Create bottleneck analysis visualizations\n",
    "        \"\"\"\n",
    "        print(\"üîç Creating Bottleneck Analysis Charts...\")\n",
    "        \n",
    "        if not self.bottleneck_results:\n",
    "            print(\"   ‚ö†Ô∏è  No bottleneck analysis data available\")\n",
    "            return []\n",
    "        \n",
    "        bottleneck_data = self.bottleneck_results['bottleneck_analysis']\n",
    "        \n",
    "        # Bottleneck Summary Dashboard\n",
    "        fig1 = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=['Bottleneck Categories', 'Plant Bottleneck Scores', \n",
    "                           'Work Center Utilization', 'Material Risk Analysis'],\n",
    "            specs=[[{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "                   [{\"type\": \"xy\"}, {\"type\": \"xy\"}]]\n",
    "        )\n",
    "        \n",
    "        # 1. Bottleneck Categories\n",
    "        if 'summary' in bottleneck_data:\n",
    "            categories = bottleneck_data['summary'].get('bottleneck_categories', {})\n",
    "            if categories:\n",
    "                fig1.add_trace(\n",
    "                    go.Bar(x=list(categories.keys()), \n",
    "                           y=list(categories.values()),\n",
    "                           marker_color=self.colors['warning'],\n",
    "                           name=\"Bottleneck Count\", showlegend=False),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "        \n",
    "        # 2. Plant Bottleneck Scores\n",
    "        if 'plant_bottlenecks' in bottleneck_data:\n",
    "            plant_data = bottleneck_data['plant_bottlenecks']\n",
    "            if 'bottleneck_ranking' in plant_data:\n",
    "                ranking = plant_data['bottleneck_ranking'].head(10)\n",
    "                \n",
    "                fig1.add_trace(\n",
    "                    go.Bar(x=ranking.index.astype(str), \n",
    "                           y=ranking['BOTTLENECK_SCORE'],\n",
    "                           marker_color=self.colors['warning'],\n",
    "                           name=\"Bottleneck Score\", showlegend=False),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "        \n",
    "        # 3. Work Center Utilization\n",
    "        if 'work_center_bottlenecks' in bottleneck_data:\n",
    "            wc_data = bottleneck_data['work_center_bottlenecks']\n",
    "            if 'work_center_analysis' in wc_data:\n",
    "                wc_analysis = wc_data['work_center_analysis'].head(10)\n",
    "                \n",
    "                fig1.add_trace(\n",
    "                    go.Scatter(x=wc_analysis.index.astype(str), \n",
    "                              y=wc_analysis['UTILIZATION_SCORE'],\n",
    "                              mode='markers',\n",
    "                              marker=dict(size=10, color=self.colors['info']),\n",
    "                              name=\"Utilization Score\", showlegend=False),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "        \n",
    "        # 4. Material Risk Analysis\n",
    "        if 'material_bottlenecks' in bottleneck_data:\n",
    "            material_data = bottleneck_data['material_bottlenecks']\n",
    "            if 'high_risk_materials' in material_data:\n",
    "                risk_materials = material_data['high_risk_materials'].head(10)\n",
    "                \n",
    "                fig1.add_trace(\n",
    "                    go.Bar(x=risk_materials.index, \n",
    "                           y=risk_materials['MATERIAL_BOTTLENECK_SCORE'],\n",
    "                           marker_color=self.colors['warning'],\n",
    "                           name=\"Risk Score\", showlegend=False),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "        \n",
    "        fig1.update_layout(\n",
    "            title=\"Bottleneck Analysis Dashboard\",\n",
    "            height=800,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        # Downtime Risk Visualization\n",
    "        fig2 = go.Figure()\n",
    "        \n",
    "        if 'downtime_model' in self.bottleneck_results:\n",
    "            downtime_model = self.bottleneck_results['downtime_model']\n",
    "            if downtime_model and 'prediction_summary' in downtime_model:\n",
    "                summary = downtime_model['prediction_summary']\n",
    "                \n",
    "                risk_levels = ['Low Risk', 'Medium Risk', 'High Risk']\n",
    "                counts = [summary.get('low_risk_count', 0),\n",
    "                         summary.get('medium_risk_count', 0),\n",
    "                         summary.get('high_risk_count', 0)]\n",
    "                \n",
    "                colors = [self.colors['success'], self.colors['warning'], self.colors['warning']]\n",
    "                \n",
    "                fig2.add_trace(go.Bar(\n",
    "                    x=risk_levels,\n",
    "                    y=counts,\n",
    "                    marker_color=colors,\n",
    "                    text=counts,\n",
    "                    textposition='auto'\n",
    "                ))\n",
    "                \n",
    "                fig2.update_layout(\n",
    "                    title=\"Downtime Risk Assessment\",\n",
    "                    xaxis_title=\"Risk Level\",\n",
    "                    yaxis_title=\"Number of Orders\",\n",
    "                    template=\"plotly_white\"\n",
    "                )\n",
    "        \n",
    "        # Throughput Analysis\n",
    "        fig3 = go.Figure()\n",
    "        \n",
    "        if 'throughput_analysis' in bottleneck_data:\n",
    "            throughput_data = bottleneck_data['throughput_analysis']\n",
    "            \n",
    "            if 'plant_analysis' in throughput_data:\n",
    "                plant_throughput = throughput_data['plant_analysis']['plant_throughput']\n",
    "                \n",
    "                fig3.add_trace(go.Scatter(\n",
    "                    x=plant_throughput.index,\n",
    "                    y=plant_throughput['THROUGHPUT_SCORE'],\n",
    "                    mode='markers+lines',\n",
    "                    marker=dict(size=12, color=self.colors['primary']),\n",
    "                    line=dict(color=self.colors['primary'])\n",
    "                ))\n",
    "                \n",
    "                fig3.update_layout(\n",
    "                    title=\"Plant Throughput Performance\",\n",
    "                    xaxis_title=\"Plant\",\n",
    "                    yaxis_title=\"Throughput Score\",\n",
    "                    template=\"plotly_white\"\n",
    "                )\n",
    "        \n",
    "        # Save bottleneck charts\n",
    "        bottleneck_file1 = os.path.join(self.viz_folder, f\"bottleneck_dashboard_{self.timestamp}.html\")\n",
    "        bottleneck_file2 = os.path.join(self.viz_folder, f\"downtime_risk_{self.timestamp}.html\")\n",
    "        bottleneck_file3 = os.path.join(self.viz_folder, f\"throughput_analysis_{self.timestamp}.html\")\n",
    "        \n",
    "        fig1.write_html(bottleneck_file1)\n",
    "        fig2.write_html(bottleneck_file2)\n",
    "        fig3.write_html(bottleneck_file3)\n",
    "        \n",
    "        self.created_files.extend([bottleneck_file1, bottleneck_file2, bottleneck_file3])\n",
    "        \n",
    "        print(f\"   ‚úì Bottleneck Analysis Charts: 3 files created\")\n",
    "        return [fig1, fig2, fig3]\n",
    "    \n",
    "    def create_ml_model_visualizations(self):\n",
    "        \"\"\"\n",
    "        Create machine learning model performance visualizations\n",
    "        \"\"\"\n",
    "        print(\"ü§ñ Creating ML Model Visualizations...\")\n",
    "        \n",
    "        if not self.ml_results:\n",
    "            print(\"   ‚ö†Ô∏è  No ML results available\")\n",
    "            return []\n",
    "        \n",
    "        # Model Performance Comparison\n",
    "        fig1 = go.Figure()\n",
    "        \n",
    "        if 'ml_results' in self.ml_results:\n",
    "            ml_data = self.ml_results['ml_results']\n",
    "            \n",
    "            # Quality prediction models\n",
    "            if 'quality_prediction' in ml_data:\n",
    "                models = []\n",
    "                accuracies = []\n",
    "                cv_scores = []\n",
    "                \n",
    "                for model_name, results in ml_data['quality_prediction'].items():\n",
    "                    models.append(model_name)\n",
    "                    accuracies.append(results['accuracy'])\n",
    "                    cv_scores.append(results['cv_mean'])\n",
    "                \n",
    "                fig1.add_trace(go.Bar(\n",
    "                    x=models,\n",
    "                    y=accuracies,\n",
    "                    name=\"Test Accuracy\",\n",
    "                    marker_color=self.colors['primary']\n",
    "                ))\n",
    "                \n",
    "                fig1.add_trace(go.Bar(\n",
    "                    x=models,\n",
    "                    y=cv_scores,\n",
    "                    name=\"CV Score\",\n",
    "                    marker_color=self.colors['secondary']\n",
    "                ))\n",
    "                \n",
    "                fig1.update_layout(\n",
    "                    title=\"Quality Prediction Model Performance\",\n",
    "                    xaxis_title=\"Model\",\n",
    "                    yaxis_title=\"Accuracy\",\n",
    "                    template=\"plotly_white\",\n",
    "                    barmode='group'\n",
    "                )\n",
    "        \n",
    "        # Feature Importance Visualization\n",
    "        fig2 = go.Figure()\n",
    "        \n",
    "        if 'feature_importance' in self.ml_results.get('ml_results', {}):\n",
    "            importance_df = self.ml_results['ml_results']['feature_importance'].head(10)\n",
    "            \n",
    "            fig2.add_trace(go.Bar(\n",
    "                x=importance_df['mean'],\n",
    "                y=importance_df['feature'],\n",
    "                orientation='h',\n",
    "                marker_color=self.colors['info'],\n",
    "                error_x=dict(type='data', array=importance_df['std'])\n",
    "            ))\n",
    "            \n",
    "            fig2.update_layout(\n",
    "                title=\"Feature Importance Analysis\",\n",
    "                xaxis_title=\"Importance Score\",\n",
    "                yaxis_title=\"Features\",\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "        \n",
    "        # Prediction Distribution\n",
    "        fig3 = go.Figure()\n",
    "        \n",
    "        if 'predictions' in self.ml_results:\n",
    "            predictions = self.ml_results['predictions']\n",
    "            \n",
    "            if 'QUALITY_ISSUE_PROBABILITY' in predictions.columns:\n",
    "                probabilities = predictions['QUALITY_ISSUE_PROBABILITY'].dropna()\n",
    "                \n",
    "                fig3.add_trace(go.Histogram(\n",
    "                    x=probabilities,\n",
    "                    nbinsx=30,\n",
    "                    marker_color=self.colors['warning'],\n",
    "                    opacity=0.7\n",
    "                ))\n",
    "                \n",
    "                # Add risk thresholds\n",
    "                fig3.add_vline(x=0.3, line_dash=\"dash\", line_color=\"orange\",\n",
    "                              annotation_text=\"Low Risk Threshold\")\n",
    "                fig3.add_vline(x=0.7, line_dash=\"dash\", line_color=\"red\",\n",
    "                              annotation_text=\"High Risk Threshold\")\n",
    "                \n",
    "                fig3.update_layout(\n",
    "                    title=\"Quality Issue Probability Distribution\",\n",
    "                    xaxis_title=\"Probability\",\n",
    "                    yaxis_title=\"Number of Orders\",\n",
    "                    template=\"plotly_white\"\n",
    "                )\n",
    "        \n",
    "        # Save ML visualization charts\n",
    "        ml_file1 = os.path.join(self.viz_folder, f\"ml_model_performance_{self.timestamp}.html\")\n",
    "        ml_file2 = os.path.join(self.viz_folder, f\"feature_importance_{self.timestamp}.html\")\n",
    "        ml_file3 = os.path.join(self.viz_folder, f\"prediction_distribution_{self.timestamp}.html\")\n",
    "        \n",
    "        fig1.write_html(ml_file1)\n",
    "        fig2.write_html(ml_file2)\n",
    "        fig3.write_html(ml_file3)\n",
    "        \n",
    "        self.created_files.extend([ml_file1, ml_file2, ml_file3])\n",
    "        \n",
    "        print(f\"   ‚úì ML Model Visualizations: 3 files created\")\n",
    "        return [fig1, fig2, fig3]\n",
    "    \n",
    "    def create_plant_comparison_dashboard(self):\n",
    "        \"\"\"\n",
    "        Create comprehensive plant comparison dashboard\n",
    "        \"\"\"\n",
    "        print(\"üè≠ Creating Plant Comparison Dashboard...\")\n",
    "        \n",
    "        df = self.comprehensive_df\n",
    "        plant_col = self._get_plant_column()\n",
    "        \n",
    "        if not plant_col:\n",
    "            print(\"   ‚ö†Ô∏è  No plant information available\")\n",
    "            return None\n",
    "        \n",
    "        # Plant Performance Radar Chart\n",
    "        fig1 = go.Figure()\n",
    "        \n",
    "        # Calculate plant metrics\n",
    "        plant_metrics = df.groupby(plant_col).agg({\n",
    "            'QUALITY_SCORE': 'mean',\n",
    "            'PRODUCTION_EFFICIENCY': 'mean',\n",
    "            'QUALITY_NOTIF_COUNT': 'mean',\n",
    "            'DEFECT_COUNT': 'mean',\n",
    "            'ORDER_ITEM_COUNT': 'mean',\n",
    "            'AUFNR': 'count'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Normalize metrics for radar chart (0-100 scale)\n",
    "        normalized_metrics = plant_metrics.copy()\n",
    "        normalized_metrics['QUALITY_SCORE_NORM'] = normalized_metrics['QUALITY_SCORE']\n",
    "        normalized_metrics['EFFICIENCY_NORM'] = normalized_metrics['PRODUCTION_EFFICIENCY']\n",
    "        normalized_metrics['QUALITY_ISSUES_NORM'] = 100 - (normalized_metrics['QUALITY_NOTIF_COUNT'] / normalized_metrics['QUALITY_NOTIF_COUNT'].max() * 100)\n",
    "        normalized_metrics['DEFECTS_NORM'] = 100 - (normalized_metrics['DEFECT_COUNT'] / normalized_metrics['DEFECT_COUNT'].max() * 100)\n",
    "        normalized_metrics['THROUGHPUT_NORM'] = normalized_metrics['AUFNR'] / normalized_metrics['AUFNR'].max() * 100\n",
    "        \n",
    "        categories = ['Quality Score', 'Efficiency', 'Quality Issues (Inv)', 'Defects (Inv)', 'Throughput']\n",
    "        \n",
    "        # Create radar chart for each plant\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
    "        \n",
    "        for i, (plant, row) in enumerate(normalized_metrics.head(5).iterrows()):\n",
    "            values = [\n",
    "                row['QUALITY_SCORE_NORM'],\n",
    "                row['EFFICIENCY_NORM'],\n",
    "                row['QUALITY_ISSUES_NORM'],\n",
    "                row['DEFECTS_NORM'],\n",
    "                row['THROUGHPUT_NORM']\n",
    "            ]\n",
    "            \n",
    "            fig1.add_trace(go.Scatterpolar(\n",
    "                r=values + [values[0]],  # Close the polygon\n",
    "                theta=categories + [categories[0]],\n",
    "                fill='toself',\n",
    "                name=str(plant),\n",
    "                line_color=colors[i % len(colors)]\n",
    "            ))\n",
    "        \n",
    "        fig1.update_layout(\n",
    "            polar=dict(\n",
    "                radialaxis=dict(\n",
    "                    visible=True,\n",
    "                    range=[0, 100]\n",
    "                )),\n",
    "            showlegend=True,\n",
    "            title=\"Plant Performance Comparison (Radar Chart)\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "        \n",
    "        # Plant Ranking Matrix\n",
    "        fig2 = go.Figure()\n",
    "        \n",
    "        # Create ranking matrix\n",
    "        ranking_metrics = ['QUALITY_SCORE', 'PRODUCTION_EFFICIENCY', 'AUFNR']\n",
    "        plant_rankings = pd.DataFrame()\n",
    "        \n",
    "        for metric in ranking_metrics:\n",
    "            if metric in plant_metrics.columns:\n",
    "                rankings = plant_metrics[metric].rank(ascending=False)\n",
    "                plant_rankings[metric] = rankings\n",
    "        \n",
    "        if not plant_rankings.empty:\n",
    "            fig2.add_trace(go.Heatmap(\n",
    "                z=plant_rankings.T.values,\n",
    "                x=plant_rankings.index,\n",
    "                y=plant_rankings.columns,\n",
    "                colorscale='RdYlGn_r',\n",
    "                text=plant_rankings.T.values,\n",
    "                texttemplate=\"%{text}\",\n",
    "                textfont={\"size\": 12},\n",
    "                colorbar=dict(title=\"Ranking\")\n",
    "            ))\n",
    "            \n",
    "            fig2.update_layout(\n",
    "                title=\"Plant Ranking Matrix (1 = Best)\",\n",
    "                xaxis_title=\"Plant\",\n",
    "                yaxis_title=\"Metric\",\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "        \n",
    "        # Plant Efficiency vs Quality Scatter\n",
    "        fig3 = go.Figure()\n",
    "        \n",
    "        if 'QUALITY_SCORE' in plant_metrics.columns and 'PRODUCTION_EFFICIENCY' in plant_metrics.columns:\n",
    "            fig3.add_trace(go.Scatter(\n",
    "                x=plant_metrics['PRODUCTION_EFFICIENCY'],\n",
    "                y=plant_metrics['QUALITY_SCORE'],\n",
    "                mode='markers+text',\n",
    "                text=plant_metrics.index,\n",
    "                textposition=\"top center\",\n",
    "                marker=dict(\n",
    "                    size=plant_metrics['AUFNR'] / 10,  # Size by order count\n",
    "                    color=plant_metrics['QUALITY_NOTIF_COUNT'],\n",
    "                    colorscale='RdYlGn_r',\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(title=\"Quality Issues\")\n",
    "                )\n",
    "            ))\n",
    "            \n",
    "            # Add benchmark lines\n",
    "            fig3.add_hline(y=85, line_dash=\"dash\", line_color=\"orange\",\n",
    "                          annotation_text=\"Quality Target: 85\")\n",
    "            fig3.add_vline(x=85, line_dash=\"dash\", line_color=\"orange\",\n",
    "                          annotation_text=\"Efficiency Target: 85%\")\n",
    "            \n",
    "            fig3.update_layout(\n",
    "                title=\"Plant Performance Matrix: Efficiency vs Quality\",\n",
    "                xaxis_title=\"Production Efficiency (%)\",\n",
    "                yaxis_title=\"Quality Score\",\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "        \n",
    "        # Save plant comparison charts\n",
    "        plant_file1 = os.path.join(self.viz_folder, f\"plant_radar_comparison_{self.timestamp}.html\")\n",
    "        plant_file2 = os.path.join(self.viz_folder, f\"plant_ranking_matrix_{self.timestamp}.html\")\n",
    "        plant_file3 = os.path.join(self.viz_folder, f\"plant_performance_matrix_{self.timestamp}.html\")\n",
    "        \n",
    "        fig1.write_html(plant_file1)\n",
    "        fig2.write_html(plant_file2)\n",
    "        fig3.write_html(plant_file3)\n",
    "        \n",
    "        self.created_files.extend([plant_file1, plant_file2, plant_file3])\n",
    "        \n",
    "        print(f\"   ‚úì Plant Comparison Dashboard: 3 files created\")\n",
    "        return [fig1, fig2, fig3]\n",
    "    \n",
    "    def create_time_series_analysis(self):\n",
    "        \"\"\"\n",
    "        Create time series analysis visualizations\n",
    "        \"\"\"\n",
    "        print(\"üìà Creating Time Series Analysis...\")\n",
    "        \n",
    "        df = self.comprehensive_df\n",
    "        \n",
    "        if 'ERDAT' not in df.columns:\n",
    "            print(\"   ‚ö†Ô∏è  No date information available for time series\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            df_temp = df.copy()\n",
    "            df_temp['ERDAT'] = pd.to_datetime(df_temp['ERDAT'], errors='coerce')\n",
    "            df_temp = df_temp.dropna(subset=['ERDAT'])\n",
    "            \n",
    "            # Time Series Dashboard\n",
    "            fig1 = make_subplots(\n",
    "                rows=3, cols=1,\n",
    "                subplot_titles=['Order Volume Over Time', 'Quality Metrics Trends', 'Efficiency Trends'],\n",
    "                shared_xaxes=True,\n",
    "                vertical_spacing=0.1\n",
    "            )\n",
    "            \n",
    "            # 1. Order Volume Over Time\n",
    "            df_temp['WEEK'] = df_temp['ERDAT'].dt.to_period('W')\n",
    "            weekly_orders = df_temp.groupby('WEEK').size().reset_index(name='Order_Count')\n",
    "            weekly_orders['WEEK_STR'] = weekly_orders['WEEK'].astype(str)\n",
    "            \n",
    "            fig1.add_trace(\n",
    "                go.Scatter(x=weekly_orders['WEEK_STR'], \n",
    "                          y=weekly_orders['Order_Count'],\n",
    "                          mode='lines+markers',\n",
    "                          name=\"Order Volume\",\n",
    "                          line=dict(color=self.colors['primary'])),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # 2. Quality Metrics Trends\n",
    "            if 'QUALITY_SCORE' in df_temp.columns:\n",
    "                weekly_quality = df_temp.groupby('WEEK').agg({\n",
    "                    'QUALITY_SCORE': 'mean',\n",
    "                    'QUALITY_NOTIF_COUNT': 'sum'\n",
    "                }).reset_index()\n",
    "                weekly_quality['WEEK_STR'] = weekly_quality['WEEK'].astype(str)\n",
    "                \n",
    "                fig1.add_trace(\n",
    "                    go.Scatter(x=weekly_quality['WEEK_STR'], \n",
    "                              y=weekly_quality['QUALITY_SCORE'],\n",
    "                              mode='lines+markers',\n",
    "                              name=\"Quality Score\",\n",
    "                              line=dict(color=self.colors['success'])),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "            \n",
    "            # 3. Efficiency Trends\n",
    "            if 'PRODUCTION_EFFICIENCY' in df_temp.columns:\n",
    "                weekly_efficiency = df_temp.groupby('WEEK')['PRODUCTION_EFFICIENCY'].mean().reset_index()\n",
    "                weekly_efficiency['WEEK_STR'] = weekly_efficiency['WEEK'].astype(str)\n",
    "                \n",
    "                fig1.add_trace(\n",
    "                    go.Scatter(x=weekly_efficiency['WEEK_STR'], \n",
    "                              y=weekly_efficiency['PRODUCTION_EFFICIENCY'],\n",
    "                              mode='lines+markers',\n",
    "                              name=\"Production Efficiency\",\n",
    "                              line=dict(color=self.colors['warning'])),\n",
    "                    row=3, col=1\n",
    "                )\n",
    "            \n",
    "            fig1.update_layout(\n",
    "                title=\"Manufacturing Performance Time Series\",\n",
    "                height=900,\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "            \n",
    "            # Seasonal Analysis\n",
    "            fig2 = go.Figure()\n",
    "            \n",
    "            df_temp['MONTH'] = df_temp['ERDAT'].dt.month\n",
    "            df_temp['DAY_OF_WEEK'] = df_temp['ERDAT'].dt.dayofweek\n",
    "            \n",
    "            # Monthly seasonality\n",
    "            monthly_pattern = df_temp.groupby('MONTH').agg({\n",
    "                'AUFNR': 'count',\n",
    "                'QUALITY_SCORE': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            fig2.add_trace(go.Bar(\n",
    "                x=monthly_pattern['MONTH'],\n",
    "                y=monthly_pattern['AUFNR'],\n",
    "                name=\"Order Count\",\n",
    "                marker_color=self.colors['primary']\n",
    "            ))\n",
    "            \n",
    "            fig2.update_layout(\n",
    "                title=\"Monthly Order Volume Pattern\",\n",
    "                xaxis_title=\"Month\",\n",
    "                yaxis_title=\"Order Count\",\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "            \n",
    "            # Save time series charts\n",
    "            ts_file1 = os.path.join(self.viz_folder, f\"time_series_dashboard_{self.timestamp}.html\")\n",
    "            ts_file2 = os.path.join(self.viz_folder, f\"seasonal_analysis_{self.timestamp}.html\")\n",
    "            \n",
    "            fig1.write_html(ts_file1)\n",
    "            fig2.write_html(ts_file2)\n",
    "            \n",
    "            self.created_files.extend([ts_file1, ts_file2])\n",
    "            \n",
    "            print(f\"   ‚úì Time Series Analysis: 2 files created\")\n",
    "            return [fig1, fig2]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Time series analysis failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_static_summary_charts(self):\n",
    "        \"\"\"\n",
    "        Create static matplotlib charts for reports\n",
    "        \"\"\"\n",
    "        print(\"üìä Creating Static Summary Charts...\")\n",
    "        \n",
    "        df = self.comprehensive_df\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('SAP Manufacturing Analytics - Summary Report', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Quality Score Distribution\n",
    "        if 'QUALITY_SCORE' in df.columns:\n",
    "            axes[0, 0].hist(df['QUALITY_SCORE'].dropna(), bins=20, alpha=0.7, color=self.colors['primary'])\n",
    "            axes[0, 0].axvline(df['QUALITY_SCORE'].mean(), color='red', linestyle='--', \n",
    "                              label=f'Mean: {df[\"QUALITY_SCORE\"].mean():.1f}')\n",
    "            axes[0, 0].set_title('Quality Score Distribution')\n",
    "            axes[0, 0].set_xlabel('Quality Score')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].legend()\n",
    "        \n",
    "        # 2. Plant Performance Comparison\n",
    "        plant_col = self._get_plant_column()\n",
    "        if plant_col and 'QUALITY_SCORE' in df.columns:\n",
    "            plant_quality = df.groupby(plant_col)['QUALITY_SCORE'].mean().sort_values(ascending=True)\n",
    "            plant_quality.plot(kind='barh', ax=axes[0, 1], color=self.colors['secondary'])\n",
    "            axes[0, 1].set_title('Plant Quality Performance')\n",
    "            axes[0, 1].set_xlabel('Average Quality Score')\n",
    "        \n",
    "        # 3. Production Efficiency vs Quality\n",
    "        if 'PRODUCTION_EFFICIENCY' in df.columns and 'QUALITY_SCORE' in df.columns:\n",
    "            axes[0, 2].scatter(df['PRODUCTION_EFFICIENCY'], df['QUALITY_SCORE'], \n",
    "                              alpha=0.6, color=self.colors['info'])\n",
    "            axes[0, 2].set_title('Efficiency vs Quality')\n",
    "            axes[0, 2].set_xlabel('Production Efficiency (%)')\n",
    "            axes[0, 2].set_ylabel('Quality Score')\n",
    "            \n",
    "            # Add correlation coefficient\n",
    "            corr = df[['PRODUCTION_EFFICIENCY', 'QUALITY_SCORE']].corr().iloc[0, 1]\n",
    "            axes[0, 2].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                           transform=axes[0, 2].transAxes, fontsize=10,\n",
    "                           bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        # 4. Quality Issues by Plant\n",
    "        if plant_col and 'QUALITY_NOTIF_COUNT' in df.columns:\n",
    "            plant_issues = df.groupby(plant_col)['QUALITY_NOTIF_COUNT'].sum().sort_values(ascending=True)\n",
    "            plant_issues.plot(kind='barh', ax=axes[1, 0], color=self.colors['warning'])\n",
    "            axes[1, 0].set_title('Quality Issues by Plant')\n",
    "            axes[1, 0].set_xlabel('Total Quality Notifications')\n",
    "        \n",
    "        # 5. Monthly Trends\n",
    "        if 'ERDAT' in df.columns:\n",
    "            try:\n",
    "                df_temp = df.copy()\n",
    "                df_temp['ERDAT'] = pd.to_datetime(df_temp['ERDAT'], errors='coerce')\n",
    "                df_temp['MONTH'] = df_temp['ERDAT'].dt.to_period('M')\n",
    "                \n",
    "                monthly_orders = df_temp.groupby('MONTH').size()\n",
    "                monthly_orders.plot(kind='line', ax=axes[1, 1], color=self.colors['success'], marker='o')\n",
    "                axes[1, 1].set_title('Monthly Order Volume')\n",
    "                axes[1, 1].set_xlabel('Month')\n",
    "                axes[1, 1].set_ylabel('Order Count')\n",
    "                plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "            except:\n",
    "                axes[1, 1].text(0.5, 0.5, 'Date data not available', \n",
    "                               ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "                axes[1, 1].set_title('Monthly Trends')\n",
    "        \n",
    "        # 6. Key Metrics Summary\n",
    "        metrics = self._calculate_key_metrics()\n",
    "        if metrics:\n",
    "            metrics_data = {\n",
    "                'Total Orders': metrics.get('total_orders', 0),\n",
    "                'Avg Quality Score': metrics.get('avg_quality_score', 0),\n",
    "                'Avg Efficiency': metrics.get('avg_efficiency', 0),\n",
    "                'Quality Issues': metrics.get('total_quality_issues', 0)\n",
    "            }\n",
    "            \n",
    "            bars = axes[1, 2].bar(range(len(metrics_data)), list(metrics_data.values()), \n",
    "                                 color=[self.colors['primary'], self.colors['success'], \n",
    "                                       self.colors['info'], self.colors['warning']])\n",
    "            axes[1, 2].set_title('Key Performance Metrics')\n",
    "            axes[1, 2].set_xticks(range(len(metrics_data)))\n",
    "            axes[1, 2].set_xticklabels(list(metrics_data.keys()), rotation=45)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, metrics_data.values()):\n",
    "                height = bar.get_height()\n",
    "                axes[1, 2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                               f'{value:.0f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save static chart\n",
    "        static_file = os.path.join(self.viz_folder, f\"summary_report_charts_{self.timestamp}.png\")\n",
    "        plt.savefig(static_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        self.created_files.append(static_file)\n",
    "        \n",
    "        print(f\"   ‚úì Static Summary Charts: {os.path.basename(static_file)}\")\n",
    "        return static_file\n",
    "    \n",
    "    def create_visualization_index(self):\n",
    "        \"\"\"\n",
    "        Create an HTML index page linking all visualizations\n",
    "        \"\"\"\n",
    "        print(\"üìë Creating Visualization Index...\")\n",
    "        \n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>SAP Manufacturing Analytics - Visualization Index</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "                h1 {{ color: #1f77b4; }}\n",
    "                h2 {{ color: #ff7f0e; }}\n",
    "                .section {{ margin-bottom: 30px; }}\n",
    "                .viz-link {{ \n",
    "                    display: inline-block; \n",
    "                    margin: 10px; \n",
    "                    padding: 10px 15px; \n",
    "                    background-color: #f0f0f0; \n",
    "                    text-decoration: none; \n",
    "                    border-radius: 5px;\n",
    "                    color: #333;\n",
    "                }}\n",
    "                .viz-link:hover {{ background-color: #e0e0e0; }}\n",
    "                .summary {{ \n",
    "                    background-color: #f9f9f9; \n",
    "                    padding: 20px; \n",
    "                    border-left: 4px solid #1f77b4; \n",
    "                    margin-bottom: 30px;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>SAP Manufacturing Analytics - Visualization Dashboard</h1>\n",
    "            \n",
    "            <div class=\"summary\">\n",
    "                <h3>Analysis Summary</h3>\n",
    "                <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "                <p><strong>Total Orders Analyzed:</strong> {len(self.comprehensive_df):,}</p>\n",
    "                <p><strong>Visualizations Created:</strong> {len(self.created_files)}</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>üìä Executive Dashboard</h2>\n",
    "                <p>High-level overview of key performance indicators and trends.</p>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add links to visualizations by category\n",
    "        viz_categories = {\n",
    "            'executive_dashboard': 'üìä Executive Dashboards',\n",
    "            'quality': 'üîç Quality Analysis',\n",
    "            'production_efficiency': '‚ö° Production Efficiency',\n",
    "            'bottleneck': 'üîç Bottleneck Analysis', \n",
    "            'ml': 'ü§ñ Machine Learning Models',\n",
    "            'plant': 'üè≠ Plant Comparisons',\n",
    "            'time_series': 'üìà Time Series Analysis',\n",
    "            'summary': 'üìä Summary Reports'\n",
    "        }\n",
    "        \n",
    "        for category, title in viz_categories.items():\n",
    "            category_files = [f for f in self.created_files if category in os.path.basename(f)]\n",
    "            \n",
    "            if category_files:\n",
    "                html_content += f\"\"\"\n",
    "                <div class=\"section\">\n",
    "                    <h2>{title}</h2>\n",
    "                \"\"\"\n",
    "                \n",
    "                for file_path in category_files:\n",
    "                    filename = os.path.basename(file_path)\n",
    "                    display_name = filename.replace(f'_{self.timestamp}', '').replace('_', ' ').title()\n",
    "                    \n",
    "                    if filename.endswith('.html'):\n",
    "                        html_content += f'<a href=\"{filename}\" class=\"viz-link\">{display_name}</a>\\n'\n",
    "                    elif filename.endswith('.png'):\n",
    "                        html_content += f'<a href=\"{filename}\" class=\"viz-link\">{display_name} (Image)</a>\\n'\n",
    "                \n",
    "                html_content += \"</div>\\n\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>üìã How to Use These Visualizations</h2>\n",
    "                <ul>\n",
    "                    <li><strong>Executive Dashboard:</strong> Start here for overall performance overview</li>\n",
    "                    <li><strong>Quality Analysis:</strong> Deep dive into quality metrics and trends</li>\n",
    "                    <li><strong>Production Efficiency:</strong> Analyze operational efficiency patterns</li>\n",
    "                    <li><strong>Bottleneck Analysis:</strong> Identify operational constraints and improvements</li>\n",
    "                    <li><strong>ML Models:</strong> Review predictive model performance and insights</li>\n",
    "                    <li><strong>Plant Comparisons:</strong> Compare performance across facilities</li>\n",
    "                    <li><strong>Time Series:</strong> Understand trends and seasonal patterns</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>üìû Support</h2>\n",
    "                <p>For questions about these visualizations or the underlying data analysis, \n",
    "                refer to the comprehensive analysis reports in the parent folder.</p>\n",
    "            </div>\n",
    "            \n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save index file\n",
    "        index_file = os.path.join(self.viz_folder, f\"index_{self.timestamp}.html\")\n",
    "        with open(index_file, 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        self.created_files.append(index_file)\n",
    "        \n",
    "        print(f\"   ‚úì Visualization Index: {os.path.basename(index_file)}\")\n",
    "        return index_file\n",
    "    \n",
    "    def _get_plant_column(self):\n",
    "        \"\"\"Helper method to find the best plant column\"\"\"\n",
    "        for col in ['Plant_Name', 'Plant_Code', 'WERKS', 'PWERK']:\n",
    "            if col in self.comprehensive_df.columns and self.comprehensive_df[col].notna().sum() > 0:\n",
    "                return col\n",
    "        return None\n",
    "    \n",
    "    def _calculate_key_metrics(self):\n",
    "        \"\"\"Calculate key performance metrics\"\"\"\n",
    "        df = self.comprehensive_df\n",
    "        \n",
    "        metrics = {\n",
    "            'total_orders': len(df),\n",
    "            'avg_quality_score': df.get('QUALITY_SCORE', pd.Series([0])).mean(),\n",
    "            'avg_efficiency': df.get('PRODUCTION_EFFICIENCY', pd.Series([0])).mean(),\n",
    "            'total_quality_issues': df.get('QUALITY_NOTIF_COUNT', pd.Series([0])).sum(),\n",
    "            'overall_score': 0\n",
    "        }\n",
    "        \n",
    "        # Calculate overall performance score\n",
    "        quality_component = min(metrics['avg_quality_score'], 100) * 0.4\n",
    "        efficiency_component = min(metrics['avg_efficiency'], 100) * 0.4\n",
    "        issue_penalty = min(metrics['total_quality_issues'] / metrics['total_orders'] * 100, 50) * 0.2\n",
    "        \n",
    "        metrics['overall_score'] = quality_component + efficiency_component - issue_penalty\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def generate_all_visualizations(self):\n",
    "        \"\"\"\n",
    "        Generate all visualization types\n",
    "        \"\"\"\n",
    "        print(\"üé® GENERATING COMPLETE VISUALIZATION SUITE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        visualization_results = {}\n",
    "        \n",
    "        # Generate all visualization types\n",
    "        try:\n",
    "            visualization_results['executive_dashboard'] = self.create_executive_dashboard()\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Executive dashboard failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            visualization_results['quality_charts'] = self.create_quality_analysis_charts()\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Quality charts failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            visualization_results['efficiency_charts'] = self.create_production_efficiency_charts()\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Efficiency charts failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            visualization_results['bottleneck_charts'] = self.create_bottleneck_visualizations()\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Bottleneck charts failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            visualization_results['ml_charts'] = self.create_ml_model_visualizations()\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå ML charts failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            visualization_results['plant_comparison'] = self.create_plant_comparison_dashboard()\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Plant comparison failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            visualization_results['time_series'] = self.create_time_series_analysis()\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Time series failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            visualization_results['static_charts'] = self.create_static_summary_charts()\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Static charts failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            visualization_results['index_page'] = self.create_visualization_index()\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Index page failed: {e}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ VISUALIZATION SUITE COMPLETE!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üìÅ Visualization folder: {self.viz_folder}\")\n",
    "        print(f\"üìä Total files created: {len(self.created_files)}\")\n",
    "        print(f\"üåê Start with: index_{self.timestamp}.html\")\n",
    "        \n",
    "        return {\n",
    "            'visualization_results': visualization_results,\n",
    "            'created_files': self.created_files,\n",
    "            'viz_folder': self.viz_folder\n",
    "        }\n",
    "\n",
    "# Integration function for complete workflow\n",
    "def create_complete_visualization_suite(comprehensive_df, ml_results=None, bottleneck_results=None, result_folder=\"result\"):\n",
    "    \"\"\"\n",
    "    Create complete visualization suite for SAP analysis\n",
    "    \"\"\"\n",
    "    print(\"üé® CREATING COMPLETE VISUALIZATION SUITE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize visualization suite\n",
    "    viz_suite = SAPVisualizationSuite(comprehensive_df, ml_results, bottleneck_results, result_folder)\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    results = viz_suite.generate_all_visualizations()\n",
    "    \n",
    "    return {\n",
    "        'viz_suite': viz_suite,\n",
    "        'results': results,\n",
    "        'created_files': results['created_files'],\n",
    "        'viz_folder': results['viz_folder']\n",
    "    }\n",
    "\n",
    "# Usage guide for Tolaram assessment\n",
    "def tolaram_visualization_guide():\n",
    "    \"\"\"\n",
    "    Guide for using visualizations in Tolaram assessment\n",
    "    \"\"\"\n",
    "    print(\"üìö TOLARAM ASSESSMENT - VISUALIZATION GUIDE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n1. INTEGRATION WITH ANALYSIS:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# After running complete analysis\")\n",
    "    print(\"comprehensive_df, summary, quality_details = create_comprehensive_sap_view(...)\")\n",
    "    print(\"ml_results = run_complete_ml_analysis(comprehensive_df)\")\n",
    "    print(\"bottleneck_results = run_complete_bottleneck_analysis(comprehensive_df)\")\n",
    "    print(\"\")\n",
    "    print(\"# Create all visualizations\")\n",
    "    print(\"viz_results = create_complete_visualization_suite(\")\n",
    "    print(\"    comprehensive_df, ml_results, bottleneck_results, 'tolaram_assessment'\")\n",
    "    print(\")\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    print(\"\\n2. VISUALIZATION CATEGORIES:\")\n",
    "    print(\"‚Ä¢ Executive Dashboard: KPI overview for management\")\n",
    "    print(\"‚Ä¢ Quality Analysis: Detailed quality performance charts\")\n",
    "    print(\"‚Ä¢ Production Efficiency: Operational performance metrics\")\n",
    "    print(\"‚Ä¢ Bottleneck Analysis: Constraint identification visuals\")\n",
    "    print(\"‚Ä¢ ML Model Performance: Predictive analytics results\")\n",
    "    print(\"‚Ä¢ Plant Comparisons: Multi-facility benchmarking\")\n",
    "    print(\"‚Ä¢ Time Series: Trend analysis and seasonality\")\n",
    "    print(\"‚Ä¢ Static Reports: Print-ready summary charts\")\n",
    "    \n",
    "    print(\"\\n3. ASSESSMENT REPORT INTEGRATION:\")\n",
    "    print(\"‚Ä¢ Use executive dashboard for presentation slides\")\n",
    "    print(\"‚Ä¢ Include static charts in written report\")\n",
    "    print(\"‚Ä¢ Reference interactive charts for detailed analysis\")\n",
    "    print(\"‚Ä¢ Plant comparison radar charts for benchmarking\")\n",
    "    print(\"‚Ä¢ ML model visualizations for technical validation\")\n",
    "    \n",
    "    print(\"\\n4. BUSINESS VALUE DEMONSTRATION:\")\n",
    "    print(\"‚Ä¢ Visual KPI dashboards show immediate impact\")\n",
    "    print(\"‚Ä¢ Trend analysis demonstrates data-driven insights\")\n",
    "    print(\"‚Ä¢ Comparative charts highlight improvement opportunities\")\n",
    "    print(\"‚Ä¢ Professional presentation-ready outputs\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tolaram_visualization_guide()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
