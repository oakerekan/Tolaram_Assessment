{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e540342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For modeling (optional, if needed later)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Date and time handling\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7142ec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading Excel file: data\\Project Assessment Data.xlsx\n",
      "✅ Successfully loaded 14 sheets\n",
      "\n",
      "================================================================================\n",
      "📊 SHEET PREVIEW AND VARIABLE CREATION\n",
      "================================================================================\n",
      "\n",
      "📄 Sheet: 'plant description'\n",
      "   Shape: (13 rows, 2 columns)\n",
      "   Columns: ['Plant Code', 'Name']\n",
      "   Preview (first 3 rows):\n",
      "Plant Code   Name\n",
      "      A110 Plant1\n",
      "      A111 Plant2\n",
      "      A112 Plant3\n",
      "   ✅ Created variable: df_plant_description\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'afko'\n",
      "   Shape: (200 rows, 183 columns)\n",
      "   Columns: ['MANDT', 'AUFNR 2 Order Number', 'GLTRP', 'GSTRP', 'FTRMS'] ... ['QPGT.LTEXTV', 'QPGT.INAKTIV']\n",
      "   Preview (first 3 rows):\n",
      " MANDT  AUFNR 2 Order Number    GLTRP    GSTRP  ...  QPGT.SPRACHE  QPGT.KURZTEXT  QPGT.LTEXTV  QPGT.INAKTIV\n",
      "   600             340011104 20241005 20241005  ...           NaN            NaN          NaN           NaN\n",
      "   600             340011105 20241005 20241005  ...           NaN            NaN          NaN           NaN\n",
      "   600             340011106 20241005 20241005  ...           NaN            NaN          NaN           NaN\n",
      "   ✅ Created variable: df_afko\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'afpo'\n",
      "   Shape: (1,000 rows, 100 columns)\n",
      "   Columns: ['MANDT', 'AUFNR', 'POSNR', 'PSOBS', 'QUNUM'] ... ['MILL_OC_RUMNG', 'MILL_OC_SORT']\n",
      "   Preview (first 3 rows):\n",
      " MANDT    AUFNR  POSNR PSOBS  ...  FSH_SALLOC_QTY  MILL_OC_AUFNR_U  MILL_OC_RUMNG  MILL_OC_SORT\n",
      "   600 10000550      1     E  ...               0              NaN              0             0\n",
      "   600 10001074      1     E  ...               0              NaN              0             0\n",
      "   600 10001071      1     E  ...               0              NaN              0             0\n",
      "   ✅ Created variable: df_afpo\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'aufm'\n",
      "   Shape: (1,000 rows, 46 columns)\n",
      "   Columns: ['MANDT', 'MBLNR', 'MJAHR', 'ZEILE', 'BLDAT'] ... ['/CWM/ERFME', 'WTY_IND']\n",
      "   Preview (first 3 rows):\n",
      " MANDT      MBLNR  MJAHR  ZEILE  ...  /CWM/MEINS  /CWM/ERFMG  /CWM/ERFME  WTY_IND\n",
      "   600 5000713450   2024      1  ...         NaN           0         NaN      NaN\n",
      "   600 5000713436   2024      1  ...         NaN           0         NaN      NaN\n",
      "   600 5000713437   2024      1  ...         NaN           0         NaN      NaN\n",
      "   ✅ Created variable: df_aufm\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'aufk'\n",
      "   Shape: (1,000 rows, 125 columns)\n",
      "   Columns: ['MANDT', 'AUFNR', 'AUART', 'AUTYP', 'REFNR'] ... ['Z_TRAILER_NO_EXIT', 'ZZODO_METER_EXIT']\n",
      "   Preview (first 3 rows):\n",
      " MANDT     AUFNR AUART  AUTYP  ...  Z_TRAILER_NO Z_TRUCK_NO_EXIT  Z_TRAILER_NO_EXIT ZZODO_METER_EXIT\n",
      "   600 220002709  ZIS1     10  ...           NaN             NaN                NaN                0\n",
      "   600 210000458  ZIF1     10  ...           NaN             NaN                NaN                0\n",
      "   600 210000459  ZIF1     10  ...           NaN             NaN                NaN                0\n",
      "   ✅ Created variable: df_aufk\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'qmel'\n",
      "   Shape: (200 rows, 165 columns)\n",
      "   Columns: ['MANDT', 'QMNUM', 'QMART', 'QMTXT', 'ARTPR'] ... ['SHN_ORIGIN', 'UII']\n",
      "   Preview (first 3 rows):\n",
      " MANDT    QMNUM QMART QMTXT  ... SHN_FUNCT_LOC  SHN_EQUIPMENT SHN_ORIGIN  UII\n",
      "   600 10003076    Z1   NaN  ...           NaN            NaN          0  NaN\n",
      "   600 10003906    Z1   NaN  ...           NaN            NaN          0  NaN\n",
      "   600 10003909    Z1   NaN  ...           NaN            NaN          0  NaN\n",
      "   ✅ Created variable: df_qmel\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'qmfe'\n",
      "   Shape: (1,000 rows, 90 columns)\n",
      "   Columns: ['MANDT', 'QMNUM', 'FENUM', 'CATEGORY', 'ERNAM'] ... ['PROD_MATNR', 'PROD_SERNR']\n",
      "   Preview (first 3 rows):\n",
      " MANDT    QMNUM  FENUM  CATEGORY  ... LGNUM  VLTYP  PROD_MATNR  PROD_SERNR\n",
      "   600 10004225      1       NaN  ...   NaN    NaN         NaN         NaN\n",
      "   600 10004226      1       NaN  ...   NaN    NaN         NaN         NaN\n",
      "   600 10004253      1       NaN  ...   NaN    NaN         NaN         NaN\n",
      "   ✅ Created variable: df_qmfe\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'qmih'\n",
      "   Shape: (1,000 rows, 60 columns)\n",
      "   Columns: ['MANDT', 'QMNUM', 'IWERK', 'ILOAN', 'ILOAI'] ... ['PAMS_PROID', 'PAMS_KOKRS']\n",
      "   Preview (first 3 rows):\n",
      " MANDT    QMNUM IWERK  ILOAN  ... PAMS_KOSTL PAMS_AUFNR  PAMS_PROID  PAMS_KOKRS\n",
      "   600 10000024  A113   1692  ...        NaN        NaN           0         NaN\n",
      "   600 10000092  A113   1810  ...        NaN        NaN           0         NaN\n",
      "   600 10000043  A113   1639  ...        NaN        NaN           0         NaN\n",
      "   ✅ Created variable: df_qmih\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'qmur'\n",
      "   Shape: (1,000 rows, 32 columns)\n",
      "   Columns: ['MANDT', 'QMNUM', 'FENUM', 'URNUM', 'ERNAM'] ... ['CHANGEDDATETIME', 'INVOLVPERC']\n",
      "   Preview (first 3 rows):\n",
      " MANDT    QMNUM  FENUM  URNUM  ... DUMMY_QMUR_INCL_EEW_PS  ROOTCAUSE CHANGEDDATETIME  INVOLVPERC\n",
      "   600 10000004      1      1  ...                    NaN        NaN    2.022080e+13           0\n",
      "   600 10000095      1      1  ...                    NaN        NaN    2.022080e+13           0\n",
      "   600 10000317      1      1  ...                    NaN        NaN    2.022080e+13           0\n",
      "   ✅ Created variable: df_qmur\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'qpcd'\n",
      "   Shape: (1,000 rows, 16 columns)\n",
      "   Columns: ['MANDT', 'KATALOGART', 'CODEGRUPPE', 'CODE', 'VERSION'] ... ['FOLGEAKTI', 'CODE4ACTIONBOX_REL']\n",
      "   Preview (first 3 rows):\n",
      " MANDT KATALOGART CODEGRUPPE CODE  ...  VERWENDUNG  GELOESCHT  FOLGEAKTI CODE4ACTIONBOX_REL\n",
      "     0          1      COLOR    1  ...         NaN        NaN        NaN                NaN\n",
      "     0          1      COLOR   10  ...         NaN        NaN        NaN                NaN\n",
      "     0          1      COLOR   11  ...         NaN        NaN        NaN                NaN\n",
      "   ✅ Created variable: df_qpcd\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'qpct'\n",
      "   Shape: (1,000 rows, 11 columns)\n",
      "   Columns: ['MANDT', 'KATALOGART', 'CODEGRUPPE', 'CODE', 'SPRACHE'] ... ['INAKTIV', 'GELOESCHT']\n",
      "   Preview (first 3 rows):\n",
      " MANDT KATALOGART CODEGRUPPE CODE  ... KURZTEXT  LTEXTV  INAKTIV GELOESCHT\n",
      "     0          1      COLOR    1  ...      Rot     NaN      NaN       NaN\n",
      "     0          1      COLOR    1  ...      Red     NaN      NaN       NaN\n",
      "     0          1      COLOR   10  ...    Braun     NaN      NaN       NaN\n",
      "   ✅ Created variable: df_qpct\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'crhd_v1'\n",
      "   Shape: (525 rows, 7 columns)\n",
      "   Columns: ['MANDT', 'OBJTY', 'OBJID', 'SPRAS', 'ARBPL', 'WERKS', 'KTEXT']\n",
      "   Preview (first 3 rows):\n",
      " MANDT OBJTY    OBJID SPRAS  ARBPL WERKS      KTEXT\n",
      "   600     A 10000000     E NSPXM1  A113 Preventive\n",
      "   600     A 10000001     E NSPXM2  A113 Corrective\n",
      "   600     A 10000002     E NSPXM3  A113    Utility\n",
      "   ✅ Created variable: df_crhd_v1\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'jest'\n",
      "   Shape: (1,000 rows, 6 columns)\n",
      "   Columns: ['MANDT', 'OBJNR', 'STAT', 'INACT', 'CHGNR', '_DATAAGING']\n",
      "   Preview (first 3 rows):\n",
      " MANDT          OBJNR  STAT  INACT  CHGNR  _DATAAGING\n",
      "   600 QM000010000006 I0072    NaN      1           0\n",
      "   600 QM000010001496 I0072    NaN      1           0\n",
      "   600 QM000010002074 I0072    NaN      1           0\n",
      "   ✅ Created variable: df_jest\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Sheet: 'qpgt'\n",
      "   Shape: (651 rows, 7 columns)\n",
      "   Columns: ['MANDANT', 'KATALOGART', 'CODEGRUPPE', 'SPRACHE', 'KURZTEXT', 'LTEXTV', 'INAKTIV']\n",
      "   Preview (first 3 rows):\n",
      " MANDANT KATALOGART CODEGRUPPE SPRACHE     KURZTEXT  LTEXTV  INAKTIV\n",
      "       0          1      COLOR       D        Farbe     NaN      NaN\n",
      "       0          1      COLOR       E        Color     NaN      NaN\n",
      "       0          2   $$OSS001       D Zu senden: &     NaN      NaN\n",
      "   ✅ Created variable: df_qpgt\n",
      "------------------------------------------------------------\n",
      "\n",
      "📋 SUMMARY:\n",
      "   • Total sheets loaded: 14\n",
      "   • Total rows across all sheets: 10,589\n",
      "   • Created variables: ['df_plant_description', 'df_afko', 'df_afpo', 'df_aufm', 'df_aufk', 'df_qmel', 'df_qmfe', 'df_qmih', 'df_qmur', 'df_qpcd', 'df_qpct', 'df_crhd_v1', 'df_jest', 'df_qpgt']\n",
      "\n",
      "================================================================================\n",
      "📈 DETAILED SHEET SUMMARY\n",
      "================================================================================\n",
      "       Sheet Name  Rows  Columns  Memory Usage (KB)  Has Missing Values  Numeric Columns  Text Columns  Date Columns\n",
      "             aufm  1000       46             707.16                True               38             8             0\n",
      "             afpo  1000      100            1292.06                True               87            13             0\n",
      "             aufk  1000      125            1475.34                True              114            11             0\n",
      "             qmur  1000       32             518.64                True               24             8             0\n",
      "             qmih  1000       60             730.88                True               52             8             0\n",
      "             qmfe  1000       90            1043.10                True               83             7             0\n",
      "             qpct  1000       11             326.14                True                5             6             0\n",
      "             jest  1000        6             145.64                True                4             2             0\n",
      "             qpcd  1000       16             368.98                True                9             7             0\n",
      "             qpgt   651        7             155.98                True                3             4             0\n",
      "          crhd_v1   525        7             152.06               False                2             5             0\n",
      "             afko   200      183             471.13                True              158            25             0\n",
      "             qmel   200      165             371.75                True              152            13             0\n",
      "plant description    13        2               1.50               False                0             2             0\n",
      "\n",
      "🎯 Available DataFrame variables: ['df_plant_description', 'df_afko', 'df_afpo', 'df_aufm', 'df_aufk', 'df_qmel', 'df_qmfe', 'df_qmih', 'df_qmur', 'df_qpcd', 'df_qpct', 'df_crhd_v1', 'df_jest', 'df_qpgt']\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Excel Data Loader with Error Handling and Logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def load_excel_sheets(excel_path, preview_rows=2, create_globals=True):\n",
    "    \"\"\"\n",
    "    Load all sheets from an Excel file with enhanced error handling and logging.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    excel_path : str\n",
    "        Path to the Excel file\n",
    "    preview_rows : int\n",
    "        Number of rows to preview for each sheet\n",
    "    create_globals : bool\n",
    "        Whether to create global DataFrame variables\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with lowercase sheet names as keys and DataFrames as values\n",
    "    \"\"\"\n",
    "    \n",
    "    df_lowercase = {}\n",
    "    created_variables = []\n",
    "    \n",
    "    # Convert to Path object for better path handling\n",
    "    file_path = Path(excel_path)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not file_path.exists():\n",
    "        print(f\"❌ Error: File not found at: {excel_path}\")\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        return df_lowercase\n",
    "    \n",
    "    try:\n",
    "        print(f\"📂 Loading Excel file: {excel_path}\")\n",
    "        \n",
    "        # Read all sheets\n",
    "        df_dict = pd.read_excel(excel_path, sheet_name=None)\n",
    "        \n",
    "        # Convert sheet names to lowercase and clean them\n",
    "        df_lowercase = {}\n",
    "        for sheet_name, df in df_dict.items():\n",
    "            clean_sheet_name = sheet_name.lower().strip()\n",
    "            df_lowercase[clean_sheet_name] = df\n",
    "        \n",
    "        print(f\"✅ Successfully loaded {len(df_lowercase)} sheets\")\n",
    "        \n",
    "        # Preview and create variables\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📊 SHEET PREVIEW AND VARIABLE CREATION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for sheet_key, df_value in df_lowercase.items():\n",
    "            print(f\"\\n📄 Sheet: '{sheet_key}'\")\n",
    "            print(f\"   Shape: ({df_value.shape[0]:,} rows, {df_value.shape[1]:,} columns)\")\n",
    "            \n",
    "            # Show column names\n",
    "            if len(df_value.columns) <= 10:\n",
    "                print(f\"   Columns: {list(df_value.columns)}\")\n",
    "            else:\n",
    "                print(f\"   Columns: {list(df_value.columns[:5])} ... {list(df_value.columns[-2:])}\")\n",
    "            \n",
    "            # Preview data\n",
    "            if not df_value.empty:\n",
    "                print(f\"   Preview (first {preview_rows} rows):\")\n",
    "                print(df_value.head(preview_rows).to_string(index=False, max_cols=8))\n",
    "            else:\n",
    "                print(\"   ⚠️  Sheet is empty\")\n",
    "            \n",
    "            # Create global variable if requested\n",
    "            if create_globals:\n",
    "                variable_name = create_safe_variable_name(sheet_key)\n",
    "                globals()[variable_name] = df_value\n",
    "                created_variables.append(variable_name)\n",
    "                print(f\"   ✅ Created variable: {variable_name}\")\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n📋 SUMMARY:\")\n",
    "        print(f\"   • Total sheets loaded: {len(df_lowercase)}\")\n",
    "        print(f\"   • Total rows across all sheets: {sum(df.shape[0] for df in df_lowercase.values()):,}\")\n",
    "        print(f\"   • Created variables: {created_variables}\")\n",
    "        \n",
    "        return df_lowercase\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading Excel file: {str(e)}\")\n",
    "        return df_lowercase\n",
    "\n",
    "def create_safe_variable_name(sheet_name):\n",
    "    \"\"\"\n",
    "    Create a safe Python variable name from a sheet name.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sheet_name : str\n",
    "        Original sheet name\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Safe variable name\n",
    "    \"\"\"\n",
    "    # Replace spaces and special characters with underscores\n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9_]', '_', sheet_name)\n",
    "    \n",
    "    # Remove consecutive underscores\n",
    "    safe_name = re.sub(r'_+', '_', safe_name)\n",
    "    \n",
    "    # Remove leading/trailing underscores\n",
    "    safe_name = safe_name.strip('_')\n",
    "    \n",
    "    # Ensure it starts with a letter or underscore (not a number)\n",
    "    if safe_name and safe_name[0].isdigit():\n",
    "        safe_name = 'sheet_' + safe_name\n",
    "    \n",
    "    # Add df_ prefix\n",
    "    return f'df_{safe_name}' if safe_name else 'df_unnamed_sheet'\n",
    "\n",
    "def get_sheet_info(df_dict):\n",
    "    \"\"\"\n",
    "    Get summary information about all loaded sheets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_dict : dict\n",
    "        Dictionary of DataFrames\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Summary information\n",
    "    \"\"\"\n",
    "    if not df_dict:\n",
    "        print(\"No sheets to summarize.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    summary_data = []\n",
    "    for sheet_name, df in df_dict.items():\n",
    "        info = {\n",
    "            'Sheet Name': sheet_name,\n",
    "            'Rows': df.shape[0],\n",
    "            'Columns': df.shape[1],\n",
    "            'Memory Usage (KB)': round(df.memory_usage(deep=True).sum() / 1024, 2),\n",
    "            'Has Missing Values': df.isnull().any().any(),\n",
    "            'Numeric Columns': df.select_dtypes(include=['number']).shape[1],\n",
    "            'Text Columns': df.select_dtypes(include=['object']).shape[1],\n",
    "            'Date Columns': df.select_dtypes(include=['datetime']).shape[1]\n",
    "        }\n",
    "        summary_data.append(info)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    return summary_df.sort_values('Rows', ascending=False)\n",
    "\n",
    "# Usage Example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the Excel path\n",
    "    excel_path = r'data\\Project Assessment Data.xlsx'\n",
    "    \n",
    "    # Load the sheets\n",
    "    df_lowercase = load_excel_sheets(excel_path, preview_rows=3, create_globals=True)\n",
    "    \n",
    "    # Get summary information\n",
    "    if df_lowercase:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"📈 DETAILED SHEET SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        summary = get_sheet_info(df_lowercase)\n",
    "        print(summary.to_string(index=False))\n",
    "        \n",
    "        # Show available DataFrames in globals\n",
    "        df_variables = [var for var in globals() if var.startswith('df_') and isinstance(globals()[var], pd.DataFrame)]\n",
    "        print(f\"\\n🎯 Available DataFrame variables: {df_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1feec4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting QMEL Data Cleaning...\n",
      "🧹 STARTING QMEL DATA CLEANING PIPELINE\n",
      "============================================================\n",
      "Initial dataset shape: (200, 165)\n",
      "\n",
      "📊 1. INITIAL DATA ASSESSMENT\n",
      "Shape: (200, 165)\n",
      "Total missing values: 19,833\n",
      "Memory usage: 0.36 MB\n",
      "\n",
      "🔍 2. MISSING VALUES ANALYSIS\n",
      "                        Missing_Count  Missing_Percentage\n",
      "AENAM                             200               100.0\n",
      "AUFNR                             200               100.0\n",
      "WAERS                             200               100.0\n",
      "VERID                             200               100.0\n",
      "SA_AUFNR                          200               100.0\n",
      "RM_WERKS                          200               100.0\n",
      "RM_MATNR                          200               100.0\n",
      "QMCOD                             200               100.0\n",
      "AUSWIRK                           200               100.0\n",
      "VKORG                             200               100.0\n",
      "MATNR                             200               100.0\n",
      "REVLV                             200               100.0\n",
      "MATKL                             200               100.0\n",
      "PRDHA                             200               100.0\n",
      "KZKRI                             200               100.0\n",
      "KZDKZ                             200               100.0\n",
      "KUNUM                             200               100.0\n",
      "MAKNZ                             200               100.0\n",
      "LIFNUM                            200               100.0\n",
      "VBELN                             200               100.0\n",
      "BUNAME                            200               100.0\n",
      "SPART                             200               100.0\n",
      "BSTNK                             200               100.0\n",
      "LGORTCHARG                        200               100.0\n",
      "LICHN                             200               100.0\n",
      "TEILEV                            200               100.0\n",
      "VTWEG                             200               100.0\n",
      "ADRNR                             200               100.0\n",
      "MAWERK                            200               100.0\n",
      "QMKAT                             200               100.0\n",
      "QMGRP                             200               100.0\n",
      "HERSTELLER                        200               100.0\n",
      "EMATNR                            200               100.0\n",
      "CHARG                             200               100.0\n",
      "EBELN                             200               100.0\n",
      "FERTAUFNR                         200               100.0\n",
      "LGORTVORG                         200               100.0\n",
      "BKGRP                             200               100.0\n",
      "EKORG                             200               100.0\n",
      "/SAPSMOSS/STATUS                  200               100.0\n",
      "/SAPSMOSS/SYSID                   200               100.0\n",
      "ZZAUFNR6                          200               100.0\n",
      "ZZAUFNR7                          200               100.0\n",
      "ZZAUFNR8                          200               100.0\n",
      "MBLNR                             200               100.0\n",
      "LS_KDAUF                          200               100.0\n",
      "LS_VBELN                          200               100.0\n",
      "MGEIN                             200               100.0\n",
      "FEART                             200               100.0\n",
      "QWRNUM                            200               100.0\n",
      "COAUFNR                           200               100.0\n",
      "REFNUM                            200               100.0\n",
      "KDMAT                             200               100.0\n",
      "SERIALNR                          200               100.0\n",
      "IDNLF                             200               100.0\n",
      "PROFIL_TYP                        200               100.0\n",
      "TSEGFL                            200               100.0\n",
      "KZLOESCH                          200               100.0\n",
      "CVP_XBLCK                         200               100.0\n",
      "DEVICEID                          200               100.0\n",
      "VKBUR                             200               100.0\n",
      "VKGRP                             200               100.0\n",
      "AUTKZ                             200               100.0\n",
      "FUNKTION                          200               100.0\n",
      "ZZAUFNR1                          200               100.0\n",
      "TSEGTP                            200               100.0\n",
      "TZONID                            200               100.0\n",
      "ZZAUFNR2                          200               100.0\n",
      "ZZAUFNR3                          200               100.0\n",
      "ZZAUFNR5                          200               100.0\n",
      "ZZAUFNR4                          200               100.0\n",
      "/ISDFPS/USERMODE                  200               100.0\n",
      "/ISDFPS/OBJNR                     200               100.0\n",
      "/SAPSMOSS/MANDT                   200               100.0\n",
      "OPPONENT                          200               100.0\n",
      "KALVAR                            200               100.0\n",
      "ZZAUFNR9                          200               100.0\n",
      "ZZAUFNR10                         200               100.0\n",
      "/SAPSMOSS/INSTN                   200               100.0\n",
      "/SAPSMOSS/OSSYS                   200               100.0\n",
      "/SAPSMOSS/DBSYS                   200               100.0\n",
      "/SAPSMOSS/REL                     200               100.0\n",
      "/SAPSMOSS/COMP                    200               100.0\n",
      "/SAPSMOSS/FRONT                   200               100.0\n",
      "/SAPSMOSS/SYSTYP                  200               100.0\n",
      "/SAPSMOSS/ADDID                   200               100.0\n",
      "/SAPSMOSS/ADDREL                  200               100.0\n",
      "SHN_OBJTY                         200               100.0\n",
      "SHN_FUNCT_LOC                     200               100.0\n",
      "LOGSYSTEM                         200               100.0\n",
      "/ISDFPS/MEQUI                     200               100.0\n",
      "SHN_EQUIPMENT                     200               100.0\n",
      "OBJNR_REAL                        200               100.0\n",
      "OBJNR_STAT                        200               100.0\n",
      "DUMMY_QMEL_INCL_EEW_PS            200               100.0\n",
      "UII                               200               100.0\n",
      "INDTX                             192                96.0\n",
      "PRIOK                             176                88.0\n",
      "FEKNZ                             151                75.5\n",
      "QMNAM                              32                16.0\n",
      "CROBJTY                            29                14.5\n",
      "ARBPLWERK                          29                14.5\n",
      "QMTXT                              24                12.0\n",
      "\n",
      "📅 3. DATE COLUMN CONVERSION\n",
      "   📅 'ERDAT': int64 → datetime64[ns]\n",
      "      📊 Valid dates: 200, Missing: 0\n",
      "   📅 'AEDAT': int64 → datetime64[ns]\n",
      "      ⚠️  200 values couldn't be converted\n",
      "      📊 Valid dates: 0, Missing: 200\n",
      "   ⚠️  Column 'PSTER' (Period From) not found\n",
      "   ⚠️  Column 'PETRI' (Period To) not found\n",
      "   📅 'BEZDT': int64 → datetime64[ns]\n",
      "      📊 Valid dates: 200, Missing: 0\n",
      "   📅 'QMDAT': int64 → datetime64[ns]\n",
      "      📊 Valid dates: 200, Missing: 0\n",
      "   📅 'LTRMN': int64 → datetime64[ns]\n",
      "      ⚠️  200 values couldn't be converted\n",
      "      📊 Valid dates: 0, Missing: 200\n",
      "\n",
      "🔧 4. DATA TYPE OPTIMIZATION\n",
      "\n",
      "📝 5. TEXT DATA CLEANING\n",
      "   📝 Cleaned 'BZMNG': 200 empty values converted to NaN\n",
      "\n",
      "🏷️  6. CATEGORICAL DATA ANALYSIS\n",
      "   📊 'QMART': 1 unique values\n",
      "      Values: {'Z1': 200}\n",
      "   📊 'PRIOK': 3 unique values\n",
      "      Values: {nan: 176, 1.0: 14, 2.0: 6, 3.0: 4}\n",
      "   📊 'QMGRP': 0 unique values\n",
      "      Values: {nan: 200}\n",
      "   📊 'QMCOD': 0 unique values\n",
      "      Values: {nan: 200}\n",
      "   📊 'MAWERK': 0 unique values\n",
      "      Values: {nan: 200}\n",
      "\n",
      "🗑️  7. DUPLICATE REMOVAL\n",
      "   ✅ No duplicate rows found\n",
      "\n",
      "📈 8. CLEANING SUMMARY\n",
      "========================================\n",
      "Rows: 200 → 200 (+0)\n",
      "Missing values: 19,833 → 20,433\n",
      "Date columns converted: 5\n",
      "Memory usage: 0.38 MB\n",
      "Duplicates removed: 0\n",
      "\n",
      "============================================================\n",
      "📋 QMEL DATA QUALITY REPORT\n",
      "============================================================\n",
      "📊 Total Notifications: 200\n",
      "📅 Date Range: 2023-03-09 to 2023-09-18\n",
      "   Span: 193 days\n",
      "🏷️  Top Notification Types:\n",
      "   Z1: 200\n",
      "✅ Data Completeness: 38.08%\n",
      "\n",
      "🎯 Cleaned DataFrame 'df_qmel_clean' is ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "def clean_qmel_data(df_qmel, verbose=True):\n",
    "    \"\"\"\n",
    "    Comprehensive cleaning pipeline for QMEL (Quality Notifications) data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_qmel : pd.DataFrame\n",
    "        Raw QMEL DataFrame\n",
    "    verbose : bool\n",
    "        Whether to print detailed progress information\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Cleaned QMEL DataFrame\n",
    "    dict : Cleaning summary statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    if df_qmel is None or df_qmel.empty:\n",
    "        print(\"❌ DataFrame is None or empty. Cannot proceed with cleaning.\")\n",
    "        return df_qmel, {}\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_clean = df_qmel.copy()\n",
    "    initial_shape = df_clean.shape\n",
    "    cleaning_stats = {\n",
    "        'initial_rows': initial_shape[0],\n",
    "        'initial_columns': initial_shape[1],\n",
    "        'date_columns_converted': 0,\n",
    "        'missing_values_before': df_clean.isnull().sum().sum(),\n",
    "        'duplicates_removed': 0\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"🧹 STARTING QMEL DATA CLEANING PIPELINE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Initial dataset shape: {initial_shape}\")\n",
    "    \n",
    "    # 1. Initial Data Assessment\n",
    "    if verbose:\n",
    "        print(f\"\\n📊 1. INITIAL DATA ASSESSMENT\")\n",
    "        print(f\"Shape: {df_clean.shape}\")\n",
    "        print(f\"Total missing values: {df_clean.isnull().sum().sum():,}\")\n",
    "        print(f\"Memory usage: {df_clean.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # 2. Handle Missing Values Analysis\n",
    "    if verbose:\n",
    "        print(f\"\\n🔍 2. MISSING VALUES ANALYSIS\")\n",
    "        missing_analysis = analyze_missing_values(df_clean)\n",
    "        if not missing_analysis.empty:\n",
    "            print(missing_analysis.to_string())\n",
    "        else:\n",
    "            print(\"✅ No missing values found!\")\n",
    "    \n",
    "    # 3. Date Column Conversion\n",
    "    if verbose:\n",
    "        print(f\"\\n📅 3. DATE COLUMN CONVERSION\")\n",
    "    \n",
    "    # SAP date columns in QMEL\n",
    "    date_columns = {\n",
    "        'ERDAT': 'Creation Date',\n",
    "        'AEDAT': 'Changed Date', \n",
    "        'PSTER': 'Period From',\n",
    "        'PETRI': 'Period To',\n",
    "        'BEZDT': 'Reference Date',\n",
    "        'QMDAT': 'Notification Date',\n",
    "        'LTRMN': 'Delivery Date'\n",
    "    }\n",
    "    \n",
    "    date_conversion_results = {}\n",
    "    \n",
    "    for col, description in date_columns.items():\n",
    "        if col in df_clean.columns:\n",
    "            result = convert_sap_date_column(df_clean, col, verbose=verbose)\n",
    "            date_conversion_results[col] = result\n",
    "            if result['converted']:\n",
    "                cleaning_stats['date_columns_converted'] += 1\n",
    "        elif verbose:\n",
    "            print(f\"   ⚠️  Column '{col}' ({description}) not found\")\n",
    "    \n",
    "    # 4. Data Type Optimization\n",
    "    if verbose:\n",
    "        print(f\"\\n🔧 4. DATA TYPE OPTIMIZATION\")\n",
    "    \n",
    "    # Convert numeric columns that might be stored as strings\n",
    "    numeric_candidates = ['PRIOK', 'QMNUM', 'MATNR', 'MENGE', 'MEINS']\n",
    "    for col in numeric_candidates:\n",
    "        if col in df_clean.columns:\n",
    "            original_dtype = df_clean[col].dtype\n",
    "            if df_clean[col].dtype == 'object':\n",
    "                try:\n",
    "                    # Try to convert to numeric\n",
    "                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "                    if verbose:\n",
    "                        print(f\"   ✅ Converted '{col}' from {original_dtype} to {df_clean[col].dtype}\")\n",
    "                except:\n",
    "                    if verbose:\n",
    "                        print(f\"   ❌ Failed to convert '{col}' to numeric\")\n",
    "    \n",
    "    # 5. Text Data Cleaning\n",
    "    if verbose:\n",
    "        print(f\"\\n📝 5. TEXT DATA CLEANING\")\n",
    "    \n",
    "    text_columns = ['QMTXT', 'KURZTEXT', 'LTXTM', 'BZMNG']\n",
    "    for col in text_columns:\n",
    "        if col in df_clean.columns:\n",
    "            original_nulls = df_clean[col].isnull().sum()\n",
    "            # Strip whitespace and convert empty strings to NaN\n",
    "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "            df_clean[col] = df_clean[col].replace(['', 'nan', 'None', '0'], pd.NaT)\n",
    "            new_nulls = df_clean[col].isnull().sum()\n",
    "            if verbose and new_nulls != original_nulls:\n",
    "                print(f\"   📝 Cleaned '{col}': {new_nulls - original_nulls} empty values converted to NaN\")\n",
    "    \n",
    "    # 6. Categorical Data Analysis\n",
    "    if verbose:\n",
    "        print(f\"\\n🏷️  6. CATEGORICAL DATA ANALYSIS\")\n",
    "    \n",
    "    categorical_columns = ['QMART', 'PRIOK', 'QMGRP', 'QMCOD', 'MAWERK', 'LIFNR']\n",
    "    for col in categorical_columns:\n",
    "        if col in df_clean.columns:\n",
    "            unique_count = df_clean[col].nunique()\n",
    "            if verbose:\n",
    "                print(f\"   📊 '{col}': {unique_count} unique values\")\n",
    "                if unique_count <= 20:  # Show value counts for small categorical vars\n",
    "                    print(f\"      Values: {df_clean[col].value_counts(dropna=False).head().to_dict()}\")\n",
    "    \n",
    "    # 7. Remove Duplicates\n",
    "    if verbose:\n",
    "        print(f\"\\n🗑️  7. DUPLICATE REMOVAL\")\n",
    "    \n",
    "    initial_rows = len(df_clean)\n",
    "    # Check for complete duplicates\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df_clean)\n",
    "    cleaning_stats['duplicates_removed'] = duplicates_removed\n",
    "    \n",
    "    if verbose:\n",
    "        if duplicates_removed > 0:\n",
    "            print(f\"   🗑️  Removed {duplicates_removed} duplicate rows\")\n",
    "        else:\n",
    "            print(f\"   ✅ No duplicate rows found\")\n",
    "    \n",
    "    # 8. Final Statistics\n",
    "    cleaning_stats.update({\n",
    "        'final_rows': len(df_clean),\n",
    "        'final_columns': len(df_clean.columns),\n",
    "        'missing_values_after': df_clean.isnull().sum().sum(),\n",
    "        'memory_mb': df_clean.memory_usage(deep=True).sum() / (1024**2)\n",
    "    })\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n📈 8. CLEANING SUMMARY\")\n",
    "        print(f\"=\" * 40)\n",
    "        print(f\"Rows: {cleaning_stats['initial_rows']:,} → {cleaning_stats['final_rows']:,} \"\n",
    "              f\"({cleaning_stats['final_rows'] - cleaning_stats['initial_rows']:+,})\")\n",
    "        print(f\"Missing values: {cleaning_stats['missing_values_before']:,} → {cleaning_stats['missing_values_after']:,}\")\n",
    "        print(f\"Date columns converted: {cleaning_stats['date_columns_converted']}\")\n",
    "        print(f\"Memory usage: {cleaning_stats['memory_mb']:.2f} MB\")\n",
    "        print(f\"Duplicates removed: {cleaning_stats['duplicates_removed']:,}\")\n",
    "    \n",
    "    return df_clean, cleaning_stats\n",
    "\n",
    "def convert_sap_date_column(df, column_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Convert SAP date column from YYYYMMDD format to datetime.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the column\n",
    "    column_name : str\n",
    "        Name of the column to convert\n",
    "    verbose : bool\n",
    "        Whether to print conversion details\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Conversion results and statistics\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        return {'converted': False, 'reason': 'Column not found'}\n",
    "    \n",
    "    original_dtype = df[column_name].dtype\n",
    "    original_nulls = df[column_name].isnull().sum()\n",
    "    \n",
    "    # Convert to string and handle SAP null representations\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    \n",
    "    # Replace SAP null representations\n",
    "    sap_nulls = ['0', '0.0', '00000000', 'nan', 'None', '']\n",
    "    df[column_name] = df[column_name].replace(sap_nulls, pd.NaT)\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df[column_name] = pd.to_datetime(df[column_name], format='%Y%m%d', errors='coerce')\n",
    "    \n",
    "    new_nulls = df[column_name].isnull().sum()\n",
    "    conversion_failures = new_nulls - original_nulls\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   📅 '{column_name}': {original_dtype} → {df[column_name].dtype}\")\n",
    "        if conversion_failures > 0:\n",
    "            print(f\"      ⚠️  {conversion_failures} values couldn't be converted\")\n",
    "        print(f\"      📊 Valid dates: {len(df) - new_nulls:,}, Missing: {new_nulls:,}\")\n",
    "    \n",
    "    return {\n",
    "        'converted': True,\n",
    "        'original_dtype': str(original_dtype),\n",
    "        'new_dtype': str(df[column_name].dtype),\n",
    "        'conversion_failures': conversion_failures,\n",
    "        'total_nulls': new_nulls\n",
    "    }\n",
    "\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"\n",
    "    Analyze missing values in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to analyze\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Missing value analysis\n",
    "    \"\"\"\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if missing_data.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    missing_percent = (missing_data / len(df) * 100).round(2)\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_data,\n",
    "        'Missing_Percentage': missing_percent\n",
    "    })\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "def get_qmel_quality_report(df_qmel_clean):\n",
    "    \"\"\"\n",
    "    Generate a data quality report for cleaned QMEL data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_qmel_clean : pd.DataFrame\n",
    "        Cleaned QMEL DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Quality report\n",
    "    \"\"\"\n",
    "    if df_qmel_clean.empty:\n",
    "        return {'error': 'DataFrame is empty'}\n",
    "    \n",
    "    report = {\n",
    "        'total_notifications': len(df_qmel_clean),\n",
    "        'date_range': {},\n",
    "        'notification_types': {},\n",
    "        'priority_distribution': {},\n",
    "        'data_completeness': {}\n",
    "    }\n",
    "    \n",
    "    # Date range analysis\n",
    "    if 'ERDAT' in df_qmel_clean.columns:\n",
    "        valid_dates = df_qmel_clean['ERDAT'].dropna()\n",
    "        if not valid_dates.empty:\n",
    "            report['date_range'] = {\n",
    "                'earliest': valid_dates.min(),\n",
    "                'latest': valid_dates.max(),\n",
    "                'span_days': (valid_dates.max() - valid_dates.min()).days\n",
    "            }\n",
    "    \n",
    "    # Notification types\n",
    "    if 'QMART' in df_qmel_clean.columns:\n",
    "        report['notification_types'] = df_qmel_clean['QMART'].value_counts().head(10).to_dict()\n",
    "    \n",
    "    # Priority distribution\n",
    "    if 'PRIOK' in df_qmel_clean.columns:\n",
    "        report['priority_distribution'] = df_qmel_clean['PRIOK'].value_counts().to_dict()\n",
    "    \n",
    "    # Data completeness\n",
    "    total_cells = df_qmel_clean.shape[0] * df_qmel_clean.shape[1]\n",
    "    missing_cells = df_qmel_clean.isnull().sum().sum()\n",
    "    report['data_completeness'] = {\n",
    "        'completeness_percentage': round((1 - missing_cells/total_cells) * 100, 2),\n",
    "        'missing_cells': missing_cells,\n",
    "        'total_cells': total_cells\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if df_qmel exists and clean it\n",
    "    if 'df_qmel' in globals() and not df_qmel.empty:\n",
    "        print(\"🚀 Starting QMEL Data Cleaning...\")\n",
    "        \n",
    "        # Clean the data\n",
    "        df_qmel_clean, stats = clean_qmel_data(df_qmel, verbose=True)\n",
    "        \n",
    "        # Generate quality report\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📋 QMEL DATA QUALITY REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        quality_report = get_qmel_quality_report(df_qmel_clean)\n",
    "        \n",
    "        print(f\"📊 Total Notifications: {quality_report.get('total_notifications', 'N/A'):,}\")\n",
    "        \n",
    "        if 'date_range' in quality_report and quality_report['date_range']:\n",
    "            dr = quality_report['date_range']\n",
    "            print(f\"📅 Date Range: {dr['earliest'].strftime('%Y-%m-%d')} to {dr['latest'].strftime('%Y-%m-%d')}\")\n",
    "            print(f\"   Span: {dr['span_days']:,} days\")\n",
    "        \n",
    "        if 'notification_types' in quality_report and quality_report['notification_types']:\n",
    "            print(f\"🏷️  Top Notification Types:\")\n",
    "            for ntype, count in list(quality_report['notification_types'].items())[:5]:\n",
    "                print(f\"   {ntype}: {count:,}\")\n",
    "        \n",
    "        if 'data_completeness' in quality_report:\n",
    "            dc = quality_report['data_completeness']\n",
    "            print(f\"✅ Data Completeness: {dc['completeness_percentage']}%\")\n",
    "        \n",
    "        print(f\"\\n🎯 Cleaned DataFrame 'df_qmel_clean' is ready for analysis!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ df_qmel not found or is empty. Please load the data first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96c956df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 SAP Quality Management Data Integration\n",
      "============================================================\n",
      "\n",
      "To use this script with your data:\n",
      "\n",
      "1. Load your SAP data:\n",
      "   df_aufk = pd.read_csv('your_aufk_file.csv')\n",
      "   df_afko = pd.read_csv('your_afko_file.csv')\n",
      "   # ... load other files\n",
      "\n",
      "2. Run the integration:\n",
      "   comprehensive_df, summary = create_comprehensive_view_corrected(\n",
      "       df_aufk=df_aufk,\n",
      "       df_afko=df_afko,\n",
      "       # ... other dataframes\n",
      "   )\n",
      "\n",
      "3. Analyze results:\n",
      "   analyze_results(comprehensive_df, summary)\n",
      "\n",
      "4. Export results:\n",
      "   comprehensive_df.to_csv('sap_comprehensive_view.csv', index=False)\n",
      "   comprehensive_df.to_excel('sap_comprehensive_view.xlsx', index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_plant_data():\n",
    "    \"\"\"\n",
    "    Create the plant description dataframe from your data\n",
    "    \"\"\"\n",
    "    plant_data = {\n",
    "        'Plant Code': ['A110', 'A111', 'A112', 'A113', 'A114', 'A210', 'A211', \n",
    "                      'A310', 'A410', 'A510', 'A610', 'A710', 'A810'],\n",
    "        'Name': ['Plant1', 'Plant2', 'Plant3', 'Plant4', 'Plant5', 'Plant6', \n",
    "                'Plant7', 'Plant8', 'Plant9', 'Plant10', 'Plant11', 'Plant12', 'Plant13']\n",
    "    }\n",
    "    \n",
    "    df_plant_description = pd.DataFrame(plant_data)\n",
    "    \n",
    "    # Also create SAP-standard column names as aliases\n",
    "    df_plant_description['WERKS'] = df_plant_description['Plant Code']\n",
    "    df_plant_description['NAME1'] = df_plant_description['Name']\n",
    "    \n",
    "    return df_plant_description\n",
    "\n",
    "def create_comprehensive_view_corrected(df_aufk, df_afko=None, df_afpo=None, df_aufm=None, \n",
    "                                      df_qmel=None, df_qmfe=None, df_qmur=None, df_qmih=None, \n",
    "                                      df_qpcd=None, df_qpct=None, df_qpgt=None, df_crhd_v1=None, \n",
    "                                      df_jest=None, df_plant_description=None):\n",
    "    \"\"\"\n",
    "    Creates a comprehensive view with corrected plant data handling\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔄 Building SAP Comprehensive View (CORRECTED)...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Validate input data\n",
    "    if df_aufk is None or df_aufk.empty:\n",
    "        raise ValueError(\"Order Master (AUFK) data is required but empty!\")\n",
    "    \n",
    "    print(f\"📋 Starting with AUFK: {len(df_aufk):,} orders\")\n",
    "    print(f\"   AUFK columns: {df_aufk.columns.tolist()}\")\n",
    "    \n",
    "    # Step 1: Build Production Order Base\n",
    "    print(\"\\n📋 Step 1: Building Production Order Base...\")\n",
    "    base_df = df_aufk.copy()\n",
    "    \n",
    "    # Join with AFKO if available\n",
    "    if df_afko is not None and not df_afko.empty:\n",
    "        print(f\"   AFKO columns: {df_afko.columns.tolist()}\")\n",
    "        \n",
    "        # Find common columns for joining\n",
    "        common_cols = list(set(df_aufk.columns) & set(df_afko.columns))\n",
    "        \n",
    "        if 'MANDT' in common_cols and 'AUFNR' in common_cols:\n",
    "            join_keys = ['MANDT', 'AUFNR']\n",
    "        elif 'AUFNR' in common_cols:\n",
    "            join_keys = ['AUFNR']\n",
    "        else:\n",
    "            print(f\"   ⚠️  No suitable join keys found between AUFK and AFKO\")\n",
    "            join_keys = None\n",
    "        \n",
    "        if join_keys:\n",
    "            base_df = base_df.merge(df_afko, on=join_keys, how='left', suffixes=('', '_AFKO'))\n",
    "            print(f\"   ✓ Orders after AUFK + AFKO join: {len(base_df):,}\")\n",
    "    \n",
    "    print(f\"   Current base_df columns: {base_df.columns.tolist()}\")\n",
    "    \n",
    "    # Step 2: Add Plant Information (CORRECTED)\n",
    "    print(\"\\n🏭 Step 2: Adding Plant Information...\")\n",
    "    \n",
    "    if df_plant_description is None:\n",
    "        # Create plant description from your data\n",
    "        df_plant_description = prepare_plant_data()\n",
    "        print(\"   ✓ Created plant description from provided data\")\n",
    "    \n",
    "    print(f\"   Plant description columns: {df_plant_description.columns.tolist()}\")\n",
    "    \n",
    "    # Find plant-related columns in base_df\n",
    "    possible_plant_cols = []\n",
    "    for col in base_df.columns:\n",
    "        col_upper = col.upper()\n",
    "        if any(keyword in col_upper for keyword in ['WERK', 'PLANT', 'FACILITY']):\n",
    "            possible_plant_cols.append(col)\n",
    "    \n",
    "    print(f\"   Possible plant columns in base_df: {possible_plant_cols}\")\n",
    "    \n",
    "    # Try different joining strategies\n",
    "    plant_join_successful = False\n",
    "    \n",
    "    # Strategy 1: Standard SAP WERKS column\n",
    "    if 'WERKS' in base_df.columns:\n",
    "        try:\n",
    "            if 'WERKS' in df_plant_description.columns:\n",
    "                join_key = 'WERKS'\n",
    "            elif 'Plant Code' in df_plant_description.columns:\n",
    "                join_key = 'Plant Code'\n",
    "                # Rename for consistency\n",
    "                df_plant_description['WERKS'] = df_plant_description['Plant Code']\n",
    "            \n",
    "            base_df = base_df.merge(\n",
    "                df_plant_description[['WERKS', 'Name'] if 'Name' in df_plant_description.columns else ['WERKS']],\n",
    "                on='WERKS',\n",
    "                how='left',\n",
    "                suffixes=('', '_PLANT')\n",
    "            )\n",
    "            print(f\"   ✓ Plant join successful on WERKS: {len(base_df):,}\")\n",
    "            plant_join_successful = True\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ WERKS join failed: {e}\")\n",
    "    \n",
    "    # Strategy 2: Try Plant Code directly\n",
    "    if not plant_join_successful and 'Plant Code' in base_df.columns:\n",
    "        try:\n",
    "            base_df = base_df.merge(\n",
    "                df_plant_description,\n",
    "                on='Plant Code',\n",
    "                how='left',\n",
    "                suffixes=('', '_PLANT')\n",
    "            )\n",
    "            print(f\"   ✓ Plant join successful on Plant Code: {len(base_df):,}\")\n",
    "            plant_join_successful = True\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Plant Code join failed: {e}\")\n",
    "    \n",
    "    # Strategy 3: Try any other plant-related columns\n",
    "    if not plant_join_successful:\n",
    "        for col in possible_plant_cols:\n",
    "            try:\n",
    "                # Try joining with Plant Code\n",
    "                base_df = base_df.merge(\n",
    "                    df_plant_description,\n",
    "                    left_on=col,\n",
    "                    right_on='Plant Code',\n",
    "                    how='left',\n",
    "                    suffixes=('', '_PLANT')\n",
    "                )\n",
    "                print(f\"   ✓ Plant join successful on {col} -> Plant Code: {len(base_df):,}\")\n",
    "                plant_join_successful = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ {col} -> Plant Code join failed: {e}\")\n",
    "    \n",
    "    if not plant_join_successful:\n",
    "        print(f\"   ⚠️  No plant join successful. Available columns in base_df:\")\n",
    "        print(f\"      {sorted(base_df.columns.tolist())}\")\n",
    "    \n",
    "    # Step 3: Add Order Items (AFPO)\n",
    "    print(\"\\n📦 Step 3: Adding Order Items...\")\n",
    "    \n",
    "    if df_afpo is not None and not df_afpo.empty:\n",
    "        try:\n",
    "            print(f\"   AFPO columns: {df_afpo.columns.tolist()}\")\n",
    "            \n",
    "            # Determine grouping columns\n",
    "            if 'MANDT' in df_afpo.columns and 'AUFNR' in df_afpo.columns:\n",
    "                group_cols = ['MANDT', 'AUFNR']\n",
    "            elif 'AUFNR' in df_afpo.columns:\n",
    "                group_cols = ['AUFNR']\n",
    "            else:\n",
    "                print(f\"   ⚠️  No AUFNR column found in AFPO\")\n",
    "                group_cols = None\n",
    "            \n",
    "            if group_cols:\n",
    "                # Create aggregation dictionary based on available columns\n",
    "                agg_dict = {}\n",
    "                \n",
    "                if 'MATNR' in df_afpo.columns:\n",
    "                    agg_dict['MATNR'] = lambda x: ', '.join(x.unique()[:5])\n",
    "                if 'POSNR' in df_afpo.columns:\n",
    "                    agg_dict['POSNR'] = 'count'\n",
    "                elif 'AUFNR' in df_afpo.columns:\n",
    "                    agg_dict['AUFNR_count'] = 'count'\n",
    "                if 'CHARG' in df_afpo.columns:\n",
    "                    agg_dict['CHARG'] = lambda x: ', '.join(x.dropna().unique()[:3])\n",
    "                \n",
    "                if agg_dict:\n",
    "                    material_summary = df_afpo.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "                    \n",
    "                    # Rename columns\n",
    "                    rename_dict = {}\n",
    "                    if 'MATNR' in agg_dict:\n",
    "                        rename_dict['MATNR'] = 'ORDER_MATERIALS'\n",
    "                    if 'POSNR' in agg_dict:\n",
    "                        rename_dict['POSNR'] = 'ORDER_ITEM_COUNT'\n",
    "                    if 'AUFNR_count' in agg_dict:\n",
    "                        rename_dict['AUFNR_count'] = 'ORDER_ITEM_COUNT'\n",
    "                    if 'CHARG' in agg_dict:\n",
    "                        rename_dict['CHARG'] = 'ORDER_BATCHES'\n",
    "                    \n",
    "                    material_summary = material_summary.rename(columns=rename_dict)\n",
    "                    \n",
    "                    # Join back to base\n",
    "                    base_df = base_df.merge(material_summary, on=group_cols, how='left')\n",
    "                    print(f\"   ✓ Order items added: {len(base_df):,}\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️  No suitable aggregation columns found in AFPO\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Order items processing failed: {e}\")\n",
    "    \n",
    "    # Step 4: Add Quality Notifications (QMEL)\n",
    "    print(\"\\n🔍 Step 4: Adding Quality Notifications...\")\n",
    "    \n",
    "    if df_qmel is not None and not df_qmel.empty:\n",
    "        try:\n",
    "            print(f\"   QMEL columns: {df_qmel.columns.tolist()}\")\n",
    "            \n",
    "            # Determine grouping columns\n",
    "            if 'MANDT' in df_qmel.columns and 'AUFNR' in df_qmel.columns:\n",
    "                group_cols = ['MANDT', 'AUFNR']\n",
    "            elif 'AUFNR' in df_qmel.columns:\n",
    "                group_cols = ['AUFNR']\n",
    "            else:\n",
    "                print(f\"   ⚠️  No AUFNR column found in QMEL\")\n",
    "                group_cols = None\n",
    "            \n",
    "            if group_cols:\n",
    "                # Create aggregation dictionary\n",
    "                agg_dict = {}\n",
    "                \n",
    "                if 'QMNUM' in df_qmel.columns:\n",
    "                    agg_dict['QMNUM'] = 'count'\n",
    "                if 'QMART' in df_qmel.columns:\n",
    "                    agg_dict['QMART'] = lambda x: ', '.join(x.unique())\n",
    "                if 'KURZTEXT' in df_qmel.columns:\n",
    "                    agg_dict['KURZTEXT'] = lambda x: ' | '.join(x.dropna().unique()[:3])\n",
    "                \n",
    "                if agg_dict:\n",
    "                    quality_summary = df_qmel.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "                    \n",
    "                    # Rename columns\n",
    "                    rename_dict = {}\n",
    "                    if 'QMNUM' in agg_dict:\n",
    "                        rename_dict['QMNUM'] = 'QUALITY_NOTIF_COUNT'\n",
    "                    if 'QMART' in agg_dict:\n",
    "                        rename_dict['QMART'] = 'QUALITY_NOTIF_TYPES'\n",
    "                    if 'KURZTEXT' in agg_dict:\n",
    "                        rename_dict['KURZTEXT'] = 'QUALITY_ISSUES_DESC'\n",
    "                    \n",
    "                    quality_summary = quality_summary.rename(columns=rename_dict)\n",
    "                    \n",
    "                    # Join back to base\n",
    "                    base_df = base_df.merge(quality_summary, on=group_cols, how='left')\n",
    "                    print(f\"   ✓ Quality notifications added: {len(base_df):,}\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️  No suitable aggregation columns found in QMEL\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Quality notifications processing failed: {e}\")\n",
    "    \n",
    "    # Step 5: Add other tables (simplified for now)\n",
    "    print(\"\\n📊 Step 5: Processing other tables...\")\n",
    "    \n",
    "    # Process other dataframes if provided\n",
    "    other_tables = {\n",
    "        'AUFM (Goods Movements)': df_aufm,\n",
    "        'QMFE (Quality Defects)': df_qmfe,\n",
    "        'QMUR (Root Causes)': df_qmur,\n",
    "        'QMIH (Maintenance)': df_qmih,\n",
    "        'CRHD_V1 (Work Centers)': df_crhd_v1\n",
    "    }\n",
    "    \n",
    "    for table_name, df in other_tables.items():\n",
    "        if df is not None and not df.empty:\n",
    "            print(f\"   📋 {table_name}: {len(df):,} rows, columns: {df.columns.tolist()}\")\n",
    "        else:\n",
    "            print(f\"   📋 {table_name}: Not provided or empty\")\n",
    "    \n",
    "    # Step 6: Create derived fields\n",
    "    print(\"\\n🛠️ Step 6: Creating Derived Fields...\")\n",
    "    \n",
    "    # Fill NaN values for numeric columns\n",
    "    numeric_cols = ['QUALITY_NOTIF_COUNT', 'ORDER_ITEM_COUNT']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in base_df.columns:\n",
    "            base_df[col] = base_df[col].fillna(0).astype(int)\n",
    "    \n",
    "    # Create quality indicators\n",
    "    if 'QUALITY_NOTIF_COUNT' in base_df.columns:\n",
    "        base_df['HAS_QUALITY_ISSUES'] = (base_df['QUALITY_NOTIF_COUNT'] > 0)\n",
    "        \n",
    "        # Simple quality score (0-100, higher is better)\n",
    "        max_issues = max(base_df['QUALITY_NOTIF_COUNT'].max(), 1)\n",
    "        base_df['QUALITY_SCORE'] = 100 - (base_df['QUALITY_NOTIF_COUNT'] / max_issues * 100)\n",
    "        base_df['QUALITY_SCORE'] = base_df['QUALITY_SCORE'].clip(0, 100).round(1)\n",
    "        \n",
    "        # Quality category\n",
    "        base_df['QUALITY_CATEGORY'] = pd.cut(\n",
    "            base_df['QUALITY_SCORE'],\n",
    "            bins=[0, 60, 80, 100],\n",
    "            labels=['Poor', 'Good', 'Excellent'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        \n",
    "        print(f\"   ✓ Quality indicators created\")\n",
    "    \n",
    "    print(f\"   ✓ Final comprehensive dataset: {len(base_df):,} records\")\n",
    "    print(f\"   ✓ Final columns ({len(base_df.columns)}): {base_df.columns.tolist()}\")\n",
    "    \n",
    "    # Generate Summary Statistics\n",
    "    print(\"\\n📊 Step 7: Generating Summary Statistics...\")\n",
    "    \n",
    "    summary_stats = {\n",
    "        'total_orders': len(base_df),\n",
    "        'total_columns': len(base_df.columns),\n",
    "        'column_names': base_df.columns.tolist()\n",
    "    }\n",
    "    \n",
    "    # Plant analysis\n",
    "    plant_col = None\n",
    "    for col in ['WERKS', 'Plant Code', 'Plant_Code']:\n",
    "        if col in base_df.columns:\n",
    "            plant_col = col\n",
    "            break\n",
    "    \n",
    "    if plant_col:\n",
    "        summary_stats['total_plants'] = base_df[plant_col].nunique()\n",
    "        summary_stats['plant_distribution'] = base_df[plant_col].value_counts().head(10).to_dict()\n",
    "    \n",
    "    # Quality analysis\n",
    "    if 'HAS_QUALITY_ISSUES' in base_df.columns:\n",
    "        summary_stats['orders_with_quality_issues'] = base_df['HAS_QUALITY_ISSUES'].sum()\n",
    "        summary_stats['quality_issue_percentage'] = (summary_stats['orders_with_quality_issues'] / len(base_df)) * 100\n",
    "    \n",
    "    if 'QUALITY_SCORE' in base_df.columns:\n",
    "        summary_stats['avg_quality_score'] = base_df['QUALITY_SCORE'].mean()\n",
    "        summary_stats['min_quality_score'] = base_df['QUALITY_SCORE'].min()\n",
    "        summary_stats['max_quality_score'] = base_df['QUALITY_SCORE'].max()\n",
    "    \n",
    "    if 'QUALITY_CATEGORY' in base_df.columns:\n",
    "        summary_stats['quality_distribution'] = base_df['QUALITY_CATEGORY'].value_counts().to_dict()\n",
    "    \n",
    "    print(\"\\n✅ Comprehensive View Created Successfully!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📈 Summary Statistics:\")\n",
    "    print(f\"   • Total Orders: {summary_stats['total_orders']:,}\")\n",
    "    print(f\"   • Total Columns: {summary_stats['total_columns']}\")\n",
    "    \n",
    "    if 'total_plants' in summary_stats:\n",
    "        print(f\"   • Total Plants: {summary_stats['total_plants']}\")\n",
    "    \n",
    "    if 'orders_with_quality_issues' in summary_stats:\n",
    "        print(f\"   • Orders with Quality Issues: {summary_stats['orders_with_quality_issues']:,} ({summary_stats['quality_issue_percentage']:.1f}%)\")\n",
    "    \n",
    "    if 'avg_quality_score' in summary_stats:\n",
    "        print(f\"   • Average Quality Score: {summary_stats['avg_quality_score']:.1f}/100\")\n",
    "    \n",
    "    return base_df, summary_stats\n",
    "\n",
    "def analyze_results(comprehensive_df, summary_stats):\n",
    "    \"\"\"\n",
    "    Analyze the comprehensive view results\n",
    "    \"\"\"\n",
    "    print(\"\\n🔍 DETAILED ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n📊 Dataset Overview:\")\n",
    "    print(f\"   • Shape: {comprehensive_df.shape}\")\n",
    "    print(f\"   • Memory usage: {comprehensive_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Column analysis\n",
    "    print(f\"\\n📋 Column Analysis:\")\n",
    "    for col in comprehensive_df.columns:\n",
    "        non_null = comprehensive_df[col].notna().sum()\n",
    "        null_pct = (len(comprehensive_df) - non_null) / len(comprehensive_df) * 100\n",
    "        print(f\"   • {col}: {non_null:,} non-null ({100-null_pct:.1f}%)\")\n",
    "    \n",
    "    # Plant analysis\n",
    "    plant_cols = [col for col in comprehensive_df.columns if any(keyword in col.upper() for keyword in ['WERK', 'PLANT'])]\n",
    "    if plant_cols:\n",
    "        print(f\"\\n🏭 Plant Analysis:\")\n",
    "        for col in plant_cols:\n",
    "            if comprehensive_df[col].dtype == 'object':\n",
    "                print(f\"   • {col}: {comprehensive_df[col].value_counts().head().to_dict()}\")\n",
    "    \n",
    "    # Quality analysis\n",
    "    if 'QUALITY_CATEGORY' in comprehensive_df.columns:\n",
    "        print(f\"\\n🔍 Quality Analysis:\")\n",
    "        quality_dist = comprehensive_df['QUALITY_CATEGORY'].value_counts()\n",
    "        for category, count in quality_dist.items():\n",
    "            pct = count/len(comprehensive_df)*100\n",
    "            print(f\"   • {category}: {count:,} orders ({pct:.1f}%)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function demonstrating usage\n",
    "    \"\"\"\n",
    "    print(\"🚀 SAP Quality Management Data Integration\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nTo use this script with your data:\")\n",
    "    print(\"\\n1. Load your SAP data:\")\n",
    "    print(\"   df_aufk = pd.read_csv('your_aufk_file.csv')\")\n",
    "    print(\"   df_afko = pd.read_csv('your_afko_file.csv')\")\n",
    "    print(\"   # ... load other files\")\n",
    "    \n",
    "    print(\"\\n2. Run the integration:\")\n",
    "    print(\"   comprehensive_df, summary = create_comprehensive_view_corrected(\")\n",
    "    print(\"       df_aufk=df_aufk,\")\n",
    "    print(\"       df_afko=df_afko,\")\n",
    "    print(\"       # ... other dataframes\")\n",
    "    print(\"   )\")\n",
    "    \n",
    "    print(\"\\n3. Analyze results:\")\n",
    "    print(\"   analyze_results(comprehensive_df, summary)\")\n",
    "    \n",
    "    print(\"\\n4. Export results:\")\n",
    "    print(\"   comprehensive_df.to_csv('sap_comprehensive_view.csv', index=False)\")\n",
    "    print(\"   comprehensive_df.to_excel('sap_comprehensive_view.xlsx', index=False)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bafbc355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Building SAP Comprehensive View...\n",
      "============================================================\n",
      "📋 Step 1: Building Production Order Base...\n",
      "   ✓ Orders after AUFK + AFKO join: 3\n",
      "   ✓ Orders after Work Center join: 3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'WERKS'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_49332\\1035100022.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m comprehensive_df, summary = create_comprehensive_view(\n\u001b[32m      2\u001b[39m        df_aufk, df_afko, df_afpo, df_aufm, df_qmel, df_qmfe,\n\u001b[32m      3\u001b[39m        df_qmur, df_qmih, df_qpcd, df_qpct, df_qpgt,\n\u001b[32m      4\u001b[39m        df_crhd_v1, df_jest, df_plant_description)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_49332\\4193037451.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df_aufk, df_afko, df_afpo, df_aufm, df_qmel, df_qmfe, df_qmur, df_qmih, df_qpcd, df_qpct, df_qpgt, df_crhd_v1, df_jest, df_plant_description)\u001b[39m\n\u001b[32m     36\u001b[39m         print(f\"   ✓ Orders after Work Center join: {len(base_df):,}\")\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Add Plant Information\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'WERKS'\u001b[39m \u001b[38;5;28;01min\u001b[39;00m base_df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         base_df = base_df.merge(\n\u001b[32m     41\u001b[39m             df_plant_description,\n\u001b[32m     42\u001b[39m             on=[\u001b[33m'MANDT'\u001b[39m, \u001b[33m'WERKS'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'MANDT'\u001b[39m \u001b[38;5;28;01min\u001b[39;00m df_plant_description.columns \u001b[38;5;28;01melse\u001b[39;00m [\u001b[33m'WERKS'\u001b[39m],\n\u001b[32m     43\u001b[39m             how=\u001b[33m'left'\u001b[39m,\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10828\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10829\u001b[39m     ) -> DataFrame:\n\u001b[32m  10830\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10831\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10832\u001b[39m         return merge(\n\u001b[32m  10833\u001b[39m             self,\n\u001b[32m  10834\u001b[39m             right,\n\u001b[32m  10835\u001b[39m             how=how,\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1293\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1296\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1298\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1299\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1300\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'WERKS'"
     ]
    }
   ],
   "source": [
    "comprehensive_df, summary = create_comprehensive_view(\n",
    "       df_aufk, df_afko, df_afpo, df_aufm, df_qmel, df_qmfe,\n",
    "       df_qmur, df_qmih, df_qpcd, df_qpct, df_qpgt,\n",
    "       df_crhd_v1, df_jest, df_plant_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb9ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
